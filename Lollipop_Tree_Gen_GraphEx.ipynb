{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and Importing Relevant Packages"
      ],
      "metadata": {
        "id": "l-h8Ue0HuhvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFDQhSgewSqQ",
        "outputId": "42227122-9a7e-48de-c3ed-b3c397d38564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import torch_geometric as torch_geometric\n",
        "import math\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "#from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import GCNConv,GINConv\n",
        "from torch.distributions import Bernoulli,Categorical\n",
        "import matplotlib.cm as cmx"
      ],
      "metadata": {
        "id": "JBoBZYpLwexz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to Generate the Dataset"
      ],
      "metadata": {
        "id": "7fdWtCLqusAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=[]"
      ],
      "metadata": {
        "id": "1RCeLw_13PVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we generate lollipop graphs and label them as 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EJXvrdxrzDNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_node_features=1\n",
        "count1=0\n",
        "totalnode=0\n",
        "for numdata in range(100):\n",
        "  range1=random.randint(2,5)\n",
        "  range2=random.randint(7,15)\n",
        "  m=[i for i in range(range1)]\n",
        "  n=[i for i in range(range1,range2)]\n",
        "  Cycle = nx.lollipop_graph(m,n)\n",
        "  num_nodes=nx.number_of_nodes(Cycle)\n",
        "  totalnode+=num_nodes\n",
        "\n",
        "  y=np.ones(num_nodes)\n",
        "\n",
        "  #Cycle.add_nodes_from([i in range(0,100)])\n",
        "  #p=math.ceil(random.uniform(5,8))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  data=pyg_utils.from_networkx(Cycle)\n",
        "  count1+=np.count_nonzero(y)\n",
        "\n",
        "  #print(y)\n",
        "\n",
        "  data.y=1\n",
        "  x=torch.ones(num_nodes,1)\n",
        "  x=x.float()\n",
        "\n",
        "  #print(deg.shape)\n",
        "\n",
        "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
        "  data.x=x\n",
        "  dataset.append(data)\n",
        "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "Oxk4yoXkxDNx",
        "outputId": "33946f65-c354-4c55-8e9a-5c4d94a83fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we generate trees and label them as 0"
      ],
      "metadata": {
        "id": "zU4LrWOywTW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_node_features=1\n",
        "count1=0\n",
        "totalnode=0\n",
        "for numdata in range(100):\n",
        "  num_nodes=random.randint(3,10)\n",
        "  totalnode+=num_nodes\n",
        "\n",
        "  y=np.ones(num_nodes)\n",
        "\n",
        "  #Cycle.add_nodes_from([i in range(0,100)])\n",
        "  p=math.ceil(random.uniform(2,4))\n",
        "  Cycle = nx.full_rary_tree(p,num_nodes)\n",
        "  #Cycle=nx.wheel_graph(num_nodes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  data=pyg_utils.from_networkx(Cycle)\n",
        "  count1+=np.count_nonzero(y)\n",
        "\n",
        "  #print(y)\n",
        "\n",
        "  data.y=0\n",
        "  x=torch.ones(num_nodes,1)\n",
        "  x=x.float()\n",
        "\n",
        "  #print(deg.shape)\n",
        "\n",
        "  #x = torch.randint(low=5,high=10,size=(num_nodes, num_node_features), dtype=torch.float32)\n",
        "  data.x=x\n",
        "  dataset.append(data)\n",
        "nx.draw_networkx(Cycle, node_size=150, node_color='red',with_labels=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "rpQOlFXE3lUv",
        "outputId": "17d566bb-35a0-41cf-af81-8622affd5f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to Load/Save Dataset"
      ],
      "metadata": {
        "id": "ZULdzXCYu3ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example list of tensors (replace this with your actual dataset)\n",
        "#my_dataset = [torch.randn((3, 64, 64)) for _ in range(100)]\n",
        "\n",
        "# Define the path to the local folder on Colab where you want to save the dataset\n",
        "save_path_dataset = '/content/my_dataset1.pt'\n",
        "\n",
        "# Save the dataset (list of tensors)\n",
        "torch.save(dataset, save_path_dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "gcxc7XDxbkhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for loading saved dataset\n",
        "\n",
        "\n",
        "# Define the path to the saved dataset on your local machine\n",
        "\n",
        "\n",
        "# Define the path to the saved dataset on your local machine\n",
        "load_path_dataset = '/content/loltreedataset.pt'\n",
        "\n",
        "# Load the saved dataset (list of tensors)\n",
        "loaded_dataset = torch.load(load_path_dataset)\n",
        "\n",
        "\n",
        "# Load the saved dataset (list of tensors)\n",
        "loaded_dataset = torch.load(load_path_dataset)\n",
        "dataset=loaded_dataset\n"
      ],
      "metadata": {
        "id": "o72tUiLlc8-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "BvxIidqS3n-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train-test"
      ],
      "metadata": {
        "id": "eLJNp1wAN3i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(12345)\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWp70Bo9MauD",
        "outputId": "98cd7100-2a34-4ae9-b376-5da13630a853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data.x)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvhePkMnOgGs",
        "outputId": "7679e116-877f-4983-e574-94e11d2ea72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 22\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definition and Training of the Classifier Model"
      ],
      "metadata": {
        "id": "mcH4DTbWvVer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, inputdim,hidden_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(inputdim, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "        self.leaky_relu = torch.nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.bn(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x\n",
        "\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.linear = Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self,inputdim, hidden_channels, num_classes):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.encoder = GCNEncoder(inputdim,hidden_channels)\n",
        "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Get the embeddings from the encoder\n",
        "        embeddings = self.encoder(x, edge_index, batch)\n",
        "\n",
        "        # Get the logits from the classifier\n",
        "        logits = self.classifier(embeddings)\n",
        "\n",
        "        return embeddings, logits\n",
        "num_features=1\n",
        "inputdim=num_features\n",
        "model=CombinedModel(inputdim, hidden_channels=64,num_classes=2)\n"
      ],
      "metadata": {
        "id": "W1_TkOM5qbia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Add a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "            embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "            #print(out)\n",
        "            loss = criterion(out, data.y)  # Compute the loss.\n",
        "            loss.backward()  # Derive gradients.\n",
        "            optimizer.step()  # Update parameters based on gradients.\n",
        "            optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "        # Update the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Print the current learning rate every epoch (optional)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
        "        # train_acc = test(train_loader)\n",
        "        # test_acc = test(test_loader)\n",
        "        # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 300\n",
        "\n",
        "# Call the training loop\n",
        "train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ3Vf634cVfa",
        "outputId": "50149162-c43b-4bcf-ae0e-5deddc8a4633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Learning Rate: 0.01 tensor(0.1770, grad_fn=<NllLossBackward0>)\n",
            "Epoch 2/300, Learning Rate: 0.01 tensor(0.7995, grad_fn=<NllLossBackward0>)\n",
            "Epoch 3/300, Learning Rate: 0.01 tensor(0.2311, grad_fn=<NllLossBackward0>)\n",
            "Epoch 4/300, Learning Rate: 0.01 tensor(0.1156, grad_fn=<NllLossBackward0>)\n",
            "Epoch 5/300, Learning Rate: 0.01 tensor(0.1981, grad_fn=<NllLossBackward0>)\n",
            "Epoch 6/300, Learning Rate: 0.01 tensor(0.1844, grad_fn=<NllLossBackward0>)\n",
            "Epoch 7/300, Learning Rate: 0.01 tensor(0.0411, grad_fn=<NllLossBackward0>)\n",
            "Epoch 8/300, Learning Rate: 0.01 tensor(0.1450, grad_fn=<NllLossBackward0>)\n",
            "Epoch 9/300, Learning Rate: 0.01 tensor(0.1347, grad_fn=<NllLossBackward0>)\n",
            "Epoch 10/300, Learning Rate: 0.01 tensor(0.4096, grad_fn=<NllLossBackward0>)\n",
            "Epoch 11/300, Learning Rate: 0.01 tensor(0.1092, grad_fn=<NllLossBackward0>)\n",
            "Epoch 12/300, Learning Rate: 0.01 tensor(0.1643, grad_fn=<NllLossBackward0>)\n",
            "Epoch 13/300, Learning Rate: 0.01 tensor(0.3475, grad_fn=<NllLossBackward0>)\n",
            "Epoch 14/300, Learning Rate: 0.01 tensor(0.3142, grad_fn=<NllLossBackward0>)\n",
            "Epoch 15/300, Learning Rate: 0.01 tensor(0.0169, grad_fn=<NllLossBackward0>)\n",
            "Epoch 16/300, Learning Rate: 0.01 tensor(0.1001, grad_fn=<NllLossBackward0>)\n",
            "Epoch 17/300, Learning Rate: 0.01 tensor(0.1232, grad_fn=<NllLossBackward0>)\n",
            "Epoch 18/300, Learning Rate: 0.01 tensor(0.1922, grad_fn=<NllLossBackward0>)\n",
            "Epoch 19/300, Learning Rate: 0.01 tensor(0.0984, grad_fn=<NllLossBackward0>)\n",
            "Epoch 20/300, Learning Rate: 0.005 tensor(0.0924, grad_fn=<NllLossBackward0>)\n",
            "Epoch 21/300, Learning Rate: 0.005 tensor(0.0130, grad_fn=<NllLossBackward0>)\n",
            "Epoch 22/300, Learning Rate: 0.005 tensor(0.2009, grad_fn=<NllLossBackward0>)\n",
            "Epoch 23/300, Learning Rate: 0.005 tensor(0.0468, grad_fn=<NllLossBackward0>)\n",
            "Epoch 24/300, Learning Rate: 0.005 tensor(0.2073, grad_fn=<NllLossBackward0>)\n",
            "Epoch 25/300, Learning Rate: 0.005 tensor(0.1445, grad_fn=<NllLossBackward0>)\n",
            "Epoch 26/300, Learning Rate: 0.005 tensor(0.1281, grad_fn=<NllLossBackward0>)\n",
            "Epoch 27/300, Learning Rate: 0.005 tensor(0.1309, grad_fn=<NllLossBackward0>)\n",
            "Epoch 28/300, Learning Rate: 0.005 tensor(0.0861, grad_fn=<NllLossBackward0>)\n",
            "Epoch 29/300, Learning Rate: 0.005 tensor(0.1047, grad_fn=<NllLossBackward0>)\n",
            "Epoch 30/300, Learning Rate: 0.005 tensor(0.1037, grad_fn=<NllLossBackward0>)\n",
            "Epoch 31/300, Learning Rate: 0.005 tensor(0.0670, grad_fn=<NllLossBackward0>)\n",
            "Epoch 32/300, Learning Rate: 0.005 tensor(0.0068, grad_fn=<NllLossBackward0>)\n",
            "Epoch 33/300, Learning Rate: 0.005 tensor(0.0603, grad_fn=<NllLossBackward0>)\n",
            "Epoch 34/300, Learning Rate: 0.005 tensor(0.1142, grad_fn=<NllLossBackward0>)\n",
            "Epoch 35/300, Learning Rate: 0.005 tensor(0.1708, grad_fn=<NllLossBackward0>)\n",
            "Epoch 36/300, Learning Rate: 0.005 tensor(0.0047, grad_fn=<NllLossBackward0>)\n",
            "Epoch 37/300, Learning Rate: 0.005 tensor(0.0110, grad_fn=<NllLossBackward0>)\n",
            "Epoch 38/300, Learning Rate: 0.005 tensor(0.0206, grad_fn=<NllLossBackward0>)\n",
            "Epoch 39/300, Learning Rate: 0.005 tensor(0.0429, grad_fn=<NllLossBackward0>)\n",
            "Epoch 40/300, Learning Rate: 0.0025 tensor(0.0579, grad_fn=<NllLossBackward0>)\n",
            "Epoch 41/300, Learning Rate: 0.0025 tensor(0.0340, grad_fn=<NllLossBackward0>)\n",
            "Epoch 42/300, Learning Rate: 0.0025 tensor(0.0388, grad_fn=<NllLossBackward0>)\n",
            "Epoch 43/300, Learning Rate: 0.0025 tensor(0.0225, grad_fn=<NllLossBackward0>)\n",
            "Epoch 44/300, Learning Rate: 0.0025 tensor(0.0595, grad_fn=<NllLossBackward0>)\n",
            "Epoch 45/300, Learning Rate: 0.0025 tensor(0.0783, grad_fn=<NllLossBackward0>)\n",
            "Epoch 46/300, Learning Rate: 0.0025 tensor(0.0414, grad_fn=<NllLossBackward0>)\n",
            "Epoch 47/300, Learning Rate: 0.0025 tensor(0.1689, grad_fn=<NllLossBackward0>)\n",
            "Epoch 48/300, Learning Rate: 0.0025 tensor(0.2452, grad_fn=<NllLossBackward0>)\n",
            "Epoch 49/300, Learning Rate: 0.0025 tensor(0.0027, grad_fn=<NllLossBackward0>)\n",
            "Epoch 50/300, Learning Rate: 0.0025 tensor(0.0685, grad_fn=<NllLossBackward0>)\n",
            "Epoch 51/300, Learning Rate: 0.0025 tensor(0.2354, grad_fn=<NllLossBackward0>)\n",
            "Epoch 52/300, Learning Rate: 0.0025 tensor(0.0227, grad_fn=<NllLossBackward0>)\n",
            "Epoch 53/300, Learning Rate: 0.0025 tensor(0.0173, grad_fn=<NllLossBackward0>)\n",
            "Epoch 54/300, Learning Rate: 0.0025 tensor(0.2507, grad_fn=<NllLossBackward0>)\n",
            "Epoch 55/300, Learning Rate: 0.0025 tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
            "Epoch 56/300, Learning Rate: 0.0025 tensor(0.1419, grad_fn=<NllLossBackward0>)\n",
            "Epoch 57/300, Learning Rate: 0.0025 tensor(0.0989, grad_fn=<NllLossBackward0>)\n",
            "Epoch 58/300, Learning Rate: 0.0025 tensor(0.0311, grad_fn=<NllLossBackward0>)\n",
            "Epoch 59/300, Learning Rate: 0.0025 tensor(0.1106, grad_fn=<NllLossBackward0>)\n",
            "Epoch 60/300, Learning Rate: 0.00125 tensor(0.0067, grad_fn=<NllLossBackward0>)\n",
            "Epoch 61/300, Learning Rate: 0.00125 tensor(0.1613, grad_fn=<NllLossBackward0>)\n",
            "Epoch 62/300, Learning Rate: 0.00125 tensor(0.0535, grad_fn=<NllLossBackward0>)\n",
            "Epoch 63/300, Learning Rate: 0.00125 tensor(0.0741, grad_fn=<NllLossBackward0>)\n",
            "Epoch 64/300, Learning Rate: 0.00125 tensor(0.0719, grad_fn=<NllLossBackward0>)\n",
            "Epoch 65/300, Learning Rate: 0.00125 tensor(0.0474, grad_fn=<NllLossBackward0>)\n",
            "Epoch 66/300, Learning Rate: 0.00125 tensor(0.0818, grad_fn=<NllLossBackward0>)\n",
            "Epoch 67/300, Learning Rate: 0.00125 tensor(0.0027, grad_fn=<NllLossBackward0>)\n",
            "Epoch 68/300, Learning Rate: 0.00125 tensor(0.3417, grad_fn=<NllLossBackward0>)\n",
            "Epoch 69/300, Learning Rate: 0.00125 tensor(0.1127, grad_fn=<NllLossBackward0>)\n",
            "Epoch 70/300, Learning Rate: 0.00125 tensor(0.0398, grad_fn=<NllLossBackward0>)\n",
            "Epoch 71/300, Learning Rate: 0.00125 tensor(0.0359, grad_fn=<NllLossBackward0>)\n",
            "Epoch 72/300, Learning Rate: 0.00125 tensor(0.0233, grad_fn=<NllLossBackward0>)\n",
            "Epoch 73/300, Learning Rate: 0.00125 tensor(0.1268, grad_fn=<NllLossBackward0>)\n",
            "Epoch 74/300, Learning Rate: 0.00125 tensor(0.0332, grad_fn=<NllLossBackward0>)\n",
            "Epoch 75/300, Learning Rate: 0.00125 tensor(0.0262, grad_fn=<NllLossBackward0>)\n",
            "Epoch 76/300, Learning Rate: 0.00125 tensor(0.0388, grad_fn=<NllLossBackward0>)\n",
            "Epoch 77/300, Learning Rate: 0.00125 tensor(0.0271, grad_fn=<NllLossBackward0>)\n",
            "Epoch 78/300, Learning Rate: 0.00125 tensor(0.0139, grad_fn=<NllLossBackward0>)\n",
            "Epoch 79/300, Learning Rate: 0.00125 tensor(0.0337, grad_fn=<NllLossBackward0>)\n",
            "Epoch 80/300, Learning Rate: 0.000625 tensor(0.0107, grad_fn=<NllLossBackward0>)\n",
            "Epoch 81/300, Learning Rate: 0.000625 tensor(0.0847, grad_fn=<NllLossBackward0>)\n",
            "Epoch 82/300, Learning Rate: 0.000625 tensor(0.0490, grad_fn=<NllLossBackward0>)\n",
            "Epoch 83/300, Learning Rate: 0.000625 tensor(0.0791, grad_fn=<NllLossBackward0>)\n",
            "Epoch 84/300, Learning Rate: 0.000625 tensor(0.0025, grad_fn=<NllLossBackward0>)\n",
            "Epoch 85/300, Learning Rate: 0.000625 tensor(0.0223, grad_fn=<NllLossBackward0>)\n",
            "Epoch 86/300, Learning Rate: 0.000625 tensor(0.0215, grad_fn=<NllLossBackward0>)\n",
            "Epoch 87/300, Learning Rate: 0.000625 tensor(0.0127, grad_fn=<NllLossBackward0>)\n",
            "Epoch 88/300, Learning Rate: 0.000625 tensor(0.0947, grad_fn=<NllLossBackward0>)\n",
            "Epoch 89/300, Learning Rate: 0.000625 tensor(0.0477, grad_fn=<NllLossBackward0>)\n",
            "Epoch 90/300, Learning Rate: 0.000625 tensor(0.0198, grad_fn=<NllLossBackward0>)\n",
            "Epoch 91/300, Learning Rate: 0.000625 tensor(0.1835, grad_fn=<NllLossBackward0>)\n",
            "Epoch 92/300, Learning Rate: 0.000625 tensor(0.2809, grad_fn=<NllLossBackward0>)\n",
            "Epoch 93/300, Learning Rate: 0.000625 tensor(0.0758, grad_fn=<NllLossBackward0>)\n",
            "Epoch 94/300, Learning Rate: 0.000625 tensor(0.0556, grad_fn=<NllLossBackward0>)\n",
            "Epoch 95/300, Learning Rate: 0.000625 tensor(0.0273, grad_fn=<NllLossBackward0>)\n",
            "Epoch 96/300, Learning Rate: 0.000625 tensor(0.0387, grad_fn=<NllLossBackward0>)\n",
            "Epoch 97/300, Learning Rate: 0.000625 tensor(0.0209, grad_fn=<NllLossBackward0>)\n",
            "Epoch 98/300, Learning Rate: 0.000625 tensor(0.0018, grad_fn=<NllLossBackward0>)\n",
            "Epoch 99/300, Learning Rate: 0.000625 tensor(0.1144, grad_fn=<NllLossBackward0>)\n",
            "Epoch 100/300, Learning Rate: 0.0003125 tensor(0.0190, grad_fn=<NllLossBackward0>)\n",
            "Epoch 101/300, Learning Rate: 0.0003125 tensor(0.0116, grad_fn=<NllLossBackward0>)\n",
            "Epoch 102/300, Learning Rate: 0.0003125 tensor(0.0226, grad_fn=<NllLossBackward0>)\n",
            "Epoch 103/300, Learning Rate: 0.0003125 tensor(0.0648, grad_fn=<NllLossBackward0>)\n",
            "Epoch 104/300, Learning Rate: 0.0003125 tensor(0.0287, grad_fn=<NllLossBackward0>)\n",
            "Epoch 105/300, Learning Rate: 0.0003125 tensor(0.0098, grad_fn=<NllLossBackward0>)\n",
            "Epoch 106/300, Learning Rate: 0.0003125 tensor(0.0541, grad_fn=<NllLossBackward0>)\n",
            "Epoch 107/300, Learning Rate: 0.0003125 tensor(0.0112, grad_fn=<NllLossBackward0>)\n",
            "Epoch 108/300, Learning Rate: 0.0003125 tensor(0.0112, grad_fn=<NllLossBackward0>)\n",
            "Epoch 109/300, Learning Rate: 0.0003125 tensor(0.0282, grad_fn=<NllLossBackward0>)\n",
            "Epoch 110/300, Learning Rate: 0.0003125 tensor(0.0530, grad_fn=<NllLossBackward0>)\n",
            "Epoch 111/300, Learning Rate: 0.0003125 tensor(0.0154, grad_fn=<NllLossBackward0>)\n",
            "Epoch 112/300, Learning Rate: 0.0003125 tensor(0.0385, grad_fn=<NllLossBackward0>)\n",
            "Epoch 113/300, Learning Rate: 0.0003125 tensor(0.0632, grad_fn=<NllLossBackward0>)\n",
            "Epoch 114/300, Learning Rate: 0.0003125 tensor(0.1558, grad_fn=<NllLossBackward0>)\n",
            "Epoch 115/300, Learning Rate: 0.0003125 tensor(0.0300, grad_fn=<NllLossBackward0>)\n",
            "Epoch 116/300, Learning Rate: 0.0003125 tensor(0.1766, grad_fn=<NllLossBackward0>)\n",
            "Epoch 117/300, Learning Rate: 0.0003125 tensor(0.0399, grad_fn=<NllLossBackward0>)\n",
            "Epoch 118/300, Learning Rate: 0.0003125 tensor(0.5516, grad_fn=<NllLossBackward0>)\n",
            "Epoch 119/300, Learning Rate: 0.0003125 tensor(0.0476, grad_fn=<NllLossBackward0>)\n",
            "Epoch 120/300, Learning Rate: 0.00015625 tensor(0.0223, grad_fn=<NllLossBackward0>)\n",
            "Epoch 121/300, Learning Rate: 0.00015625 tensor(0.0597, grad_fn=<NllLossBackward0>)\n",
            "Epoch 122/300, Learning Rate: 0.00015625 tensor(0.0044, grad_fn=<NllLossBackward0>)\n",
            "Epoch 123/300, Learning Rate: 0.00015625 tensor(0.0287, grad_fn=<NllLossBackward0>)\n",
            "Epoch 124/300, Learning Rate: 0.00015625 tensor(0.0490, grad_fn=<NllLossBackward0>)\n",
            "Epoch 125/300, Learning Rate: 0.00015625 tensor(0.0187, grad_fn=<NllLossBackward0>)\n",
            "Epoch 126/300, Learning Rate: 0.00015625 tensor(0.1276, grad_fn=<NllLossBackward0>)\n",
            "Epoch 127/300, Learning Rate: 0.00015625 tensor(0.0513, grad_fn=<NllLossBackward0>)\n",
            "Epoch 128/300, Learning Rate: 0.00015625 tensor(0.0182, grad_fn=<NllLossBackward0>)\n",
            "Epoch 129/300, Learning Rate: 0.00015625 tensor(0.0259, grad_fn=<NllLossBackward0>)\n",
            "Epoch 130/300, Learning Rate: 0.00015625 tensor(0.1416, grad_fn=<NllLossBackward0>)\n",
            "Epoch 131/300, Learning Rate: 0.00015625 tensor(0.0132, grad_fn=<NllLossBackward0>)\n",
            "Epoch 132/300, Learning Rate: 0.00015625 tensor(0.0003, grad_fn=<NllLossBackward0>)\n",
            "Epoch 133/300, Learning Rate: 0.00015625 tensor(0.0059, grad_fn=<NllLossBackward0>)\n",
            "Epoch 134/300, Learning Rate: 0.00015625 tensor(0.0114, grad_fn=<NllLossBackward0>)\n",
            "Epoch 135/300, Learning Rate: 0.00015625 tensor(0.0289, grad_fn=<NllLossBackward0>)\n",
            "Epoch 136/300, Learning Rate: 0.00015625 tensor(0.0275, grad_fn=<NllLossBackward0>)\n",
            "Epoch 137/300, Learning Rate: 0.00015625 tensor(0.7752, grad_fn=<NllLossBackward0>)\n",
            "Epoch 138/300, Learning Rate: 0.00015625 tensor(0.0110, grad_fn=<NllLossBackward0>)\n",
            "Epoch 139/300, Learning Rate: 0.00015625 tensor(0.0277, grad_fn=<NllLossBackward0>)\n",
            "Epoch 140/300, Learning Rate: 7.8125e-05 tensor(0.0571, grad_fn=<NllLossBackward0>)\n",
            "Epoch 141/300, Learning Rate: 7.8125e-05 tensor(0.3133, grad_fn=<NllLossBackward0>)\n",
            "Epoch 142/300, Learning Rate: 7.8125e-05 tensor(0.0008, grad_fn=<NllLossBackward0>)\n",
            "Epoch 143/300, Learning Rate: 7.8125e-05 tensor(0.0352, grad_fn=<NllLossBackward0>)\n",
            "Epoch 144/300, Learning Rate: 7.8125e-05 tensor(0.0040, grad_fn=<NllLossBackward0>)\n",
            "Epoch 145/300, Learning Rate: 7.8125e-05 tensor(0.0707, grad_fn=<NllLossBackward0>)\n",
            "Epoch 146/300, Learning Rate: 7.8125e-05 tensor(0.0074, grad_fn=<NllLossBackward0>)\n",
            "Epoch 147/300, Learning Rate: 7.8125e-05 tensor(0.1384, grad_fn=<NllLossBackward0>)\n",
            "Epoch 148/300, Learning Rate: 7.8125e-05 tensor(0.0187, grad_fn=<NllLossBackward0>)\n",
            "Epoch 149/300, Learning Rate: 7.8125e-05 tensor(0.0179, grad_fn=<NllLossBackward0>)\n",
            "Epoch 150/300, Learning Rate: 7.8125e-05 tensor(0.0026, grad_fn=<NllLossBackward0>)\n",
            "Epoch 151/300, Learning Rate: 7.8125e-05 tensor(0.0344, grad_fn=<NllLossBackward0>)\n",
            "Epoch 152/300, Learning Rate: 7.8125e-05 tensor(0.0104, grad_fn=<NllLossBackward0>)\n",
            "Epoch 153/300, Learning Rate: 7.8125e-05 tensor(0.0008, grad_fn=<NllLossBackward0>)\n",
            "Epoch 154/300, Learning Rate: 7.8125e-05 tensor(0.0412, grad_fn=<NllLossBackward0>)\n",
            "Epoch 155/300, Learning Rate: 7.8125e-05 tensor(0.2451, grad_fn=<NllLossBackward0>)\n",
            "Epoch 156/300, Learning Rate: 7.8125e-05 tensor(0.1460, grad_fn=<NllLossBackward0>)\n",
            "Epoch 157/300, Learning Rate: 7.8125e-05 tensor(0.0229, grad_fn=<NllLossBackward0>)\n",
            "Epoch 158/300, Learning Rate: 7.8125e-05 tensor(0.0024, grad_fn=<NllLossBackward0>)\n",
            "Epoch 159/300, Learning Rate: 7.8125e-05 tensor(0.0555, grad_fn=<NllLossBackward0>)\n",
            "Epoch 160/300, Learning Rate: 3.90625e-05 tensor(0.0123, grad_fn=<NllLossBackward0>)\n",
            "Epoch 161/300, Learning Rate: 3.90625e-05 tensor(0.1081, grad_fn=<NllLossBackward0>)\n",
            "Epoch 162/300, Learning Rate: 3.90625e-05 tensor(0.0050, grad_fn=<NllLossBackward0>)\n",
            "Epoch 163/300, Learning Rate: 3.90625e-05 tensor(0.0017, grad_fn=<NllLossBackward0>)\n",
            "Epoch 164/300, Learning Rate: 3.90625e-05 tensor(0.0405, grad_fn=<NllLossBackward0>)\n",
            "Epoch 165/300, Learning Rate: 3.90625e-05 tensor(0.0021, grad_fn=<NllLossBackward0>)\n",
            "Epoch 166/300, Learning Rate: 3.90625e-05 tensor(0.0406, grad_fn=<NllLossBackward0>)\n",
            "Epoch 167/300, Learning Rate: 3.90625e-05 tensor(1.1511, grad_fn=<NllLossBackward0>)\n",
            "Epoch 168/300, Learning Rate: 3.90625e-05 tensor(0.0621, grad_fn=<NllLossBackward0>)\n",
            "Epoch 169/300, Learning Rate: 3.90625e-05 tensor(0.1210, grad_fn=<NllLossBackward0>)\n",
            "Epoch 170/300, Learning Rate: 3.90625e-05 tensor(0.0030, grad_fn=<NllLossBackward0>)\n",
            "Epoch 171/300, Learning Rate: 3.90625e-05 tensor(0.0132, grad_fn=<NllLossBackward0>)\n",
            "Epoch 172/300, Learning Rate: 3.90625e-05 tensor(0.0256, grad_fn=<NllLossBackward0>)\n",
            "Epoch 173/300, Learning Rate: 3.90625e-05 tensor(0.0189, grad_fn=<NllLossBackward0>)\n",
            "Epoch 174/300, Learning Rate: 3.90625e-05 tensor(0.0232, grad_fn=<NllLossBackward0>)\n",
            "Epoch 175/300, Learning Rate: 3.90625e-05 tensor(0.0303, grad_fn=<NllLossBackward0>)\n",
            "Epoch 176/300, Learning Rate: 3.90625e-05 tensor(0.0053, grad_fn=<NllLossBackward0>)\n",
            "Epoch 177/300, Learning Rate: 3.90625e-05 tensor(0.0627, grad_fn=<NllLossBackward0>)\n",
            "Epoch 178/300, Learning Rate: 3.90625e-05 tensor(0.0707, grad_fn=<NllLossBackward0>)\n",
            "Epoch 179/300, Learning Rate: 3.90625e-05 tensor(0.0358, grad_fn=<NllLossBackward0>)\n",
            "Epoch 180/300, Learning Rate: 1.953125e-05 tensor(0.0353, grad_fn=<NllLossBackward0>)\n",
            "Epoch 181/300, Learning Rate: 1.953125e-05 tensor(0.0508, grad_fn=<NllLossBackward0>)\n",
            "Epoch 182/300, Learning Rate: 1.953125e-05 tensor(0.0286, grad_fn=<NllLossBackward0>)\n",
            "Epoch 183/300, Learning Rate: 1.953125e-05 tensor(0.3211, grad_fn=<NllLossBackward0>)\n",
            "Epoch 184/300, Learning Rate: 1.953125e-05 tensor(0.0144, grad_fn=<NllLossBackward0>)\n",
            "Epoch 185/300, Learning Rate: 1.953125e-05 tensor(0.0224, grad_fn=<NllLossBackward0>)\n",
            "Epoch 186/300, Learning Rate: 1.953125e-05 tensor(0.4579, grad_fn=<NllLossBackward0>)\n",
            "Epoch 187/300, Learning Rate: 1.953125e-05 tensor(0.0760, grad_fn=<NllLossBackward0>)\n",
            "Epoch 188/300, Learning Rate: 1.953125e-05 tensor(0.0194, grad_fn=<NllLossBackward0>)\n",
            "Epoch 189/300, Learning Rate: 1.953125e-05 tensor(0.0117, grad_fn=<NllLossBackward0>)\n",
            "Epoch 190/300, Learning Rate: 1.953125e-05 tensor(0.0123, grad_fn=<NllLossBackward0>)\n",
            "Epoch 191/300, Learning Rate: 1.953125e-05 tensor(0.0850, grad_fn=<NllLossBackward0>)\n",
            "Epoch 192/300, Learning Rate: 1.953125e-05 tensor(0.0230, grad_fn=<NllLossBackward0>)\n",
            "Epoch 193/300, Learning Rate: 1.953125e-05 tensor(0.0115, grad_fn=<NllLossBackward0>)\n",
            "Epoch 194/300, Learning Rate: 1.953125e-05 tensor(0.0200, grad_fn=<NllLossBackward0>)\n",
            "Epoch 195/300, Learning Rate: 1.953125e-05 tensor(0.0098, grad_fn=<NllLossBackward0>)\n",
            "Epoch 196/300, Learning Rate: 1.953125e-05 tensor(0.1524, grad_fn=<NllLossBackward0>)\n",
            "Epoch 197/300, Learning Rate: 1.953125e-05 tensor(0.0017, grad_fn=<NllLossBackward0>)\n",
            "Epoch 198/300, Learning Rate: 1.953125e-05 tensor(0.0034, grad_fn=<NllLossBackward0>)\n",
            "Epoch 199/300, Learning Rate: 1.953125e-05 tensor(0.0504, grad_fn=<NllLossBackward0>)\n",
            "Epoch 200/300, Learning Rate: 9.765625e-06 tensor(0.0289, grad_fn=<NllLossBackward0>)\n",
            "Epoch 201/300, Learning Rate: 9.765625e-06 tensor(0.0683, grad_fn=<NllLossBackward0>)\n",
            "Epoch 202/300, Learning Rate: 9.765625e-06 tensor(0.0359, grad_fn=<NllLossBackward0>)\n",
            "Epoch 203/300, Learning Rate: 9.765625e-06 tensor(0.0229, grad_fn=<NllLossBackward0>)\n",
            "Epoch 204/300, Learning Rate: 9.765625e-06 tensor(0.0101, grad_fn=<NllLossBackward0>)\n",
            "Epoch 205/300, Learning Rate: 9.765625e-06 tensor(0.0249, grad_fn=<NllLossBackward0>)\n",
            "Epoch 206/300, Learning Rate: 9.765625e-06 tensor(0.0148, grad_fn=<NllLossBackward0>)\n",
            "Epoch 207/300, Learning Rate: 9.765625e-06 tensor(0.0194, grad_fn=<NllLossBackward0>)\n",
            "Epoch 208/300, Learning Rate: 9.765625e-06 tensor(0.0132, grad_fn=<NllLossBackward0>)\n",
            "Epoch 209/300, Learning Rate: 9.765625e-06 tensor(0.1669, grad_fn=<NllLossBackward0>)\n",
            "Epoch 210/300, Learning Rate: 9.765625e-06 tensor(0.0183, grad_fn=<NllLossBackward0>)\n",
            "Epoch 211/300, Learning Rate: 9.765625e-06 tensor(0.0120, grad_fn=<NllLossBackward0>)\n",
            "Epoch 212/300, Learning Rate: 9.765625e-06 tensor(0.0287, grad_fn=<NllLossBackward0>)\n",
            "Epoch 213/300, Learning Rate: 9.765625e-06 tensor(0.0174, grad_fn=<NllLossBackward0>)\n",
            "Epoch 214/300, Learning Rate: 9.765625e-06 tensor(0.0159, grad_fn=<NllLossBackward0>)\n",
            "Epoch 215/300, Learning Rate: 9.765625e-06 tensor(0.0092, grad_fn=<NllLossBackward0>)\n",
            "Epoch 216/300, Learning Rate: 9.765625e-06 tensor(0.0091, grad_fn=<NllLossBackward0>)\n",
            "Epoch 217/300, Learning Rate: 9.765625e-06 tensor(0.0457, grad_fn=<NllLossBackward0>)\n",
            "Epoch 218/300, Learning Rate: 9.765625e-06 tensor(0.0432, grad_fn=<NllLossBackward0>)\n",
            "Epoch 219/300, Learning Rate: 9.765625e-06 tensor(0.0117, grad_fn=<NllLossBackward0>)\n",
            "Epoch 220/300, Learning Rate: 4.8828125e-06 tensor(0.0991, grad_fn=<NllLossBackward0>)\n",
            "Epoch 221/300, Learning Rate: 4.8828125e-06 tensor(0.1179, grad_fn=<NllLossBackward0>)\n",
            "Epoch 222/300, Learning Rate: 4.8828125e-06 tensor(0.0345, grad_fn=<NllLossBackward0>)\n",
            "Epoch 223/300, Learning Rate: 4.8828125e-06 tensor(0.1071, grad_fn=<NllLossBackward0>)\n",
            "Epoch 224/300, Learning Rate: 4.8828125e-06 tensor(0.0119, grad_fn=<NllLossBackward0>)\n",
            "Epoch 225/300, Learning Rate: 4.8828125e-06 tensor(0.0621, grad_fn=<NllLossBackward0>)\n",
            "Epoch 226/300, Learning Rate: 4.8828125e-06 tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
            "Epoch 227/300, Learning Rate: 4.8828125e-06 tensor(0.0214, grad_fn=<NllLossBackward0>)\n",
            "Epoch 228/300, Learning Rate: 4.8828125e-06 tensor(0.1865, grad_fn=<NllLossBackward0>)\n",
            "Epoch 229/300, Learning Rate: 4.8828125e-06 tensor(0.0109, grad_fn=<NllLossBackward0>)\n",
            "Epoch 230/300, Learning Rate: 4.8828125e-06 tensor(0.0070, grad_fn=<NllLossBackward0>)\n",
            "Epoch 231/300, Learning Rate: 4.8828125e-06 tensor(0.0228, grad_fn=<NllLossBackward0>)\n",
            "Epoch 232/300, Learning Rate: 4.8828125e-06 tensor(0.0053, grad_fn=<NllLossBackward0>)\n",
            "Epoch 233/300, Learning Rate: 4.8828125e-06 tensor(0.0148, grad_fn=<NllLossBackward0>)\n",
            "Epoch 234/300, Learning Rate: 4.8828125e-06 tensor(0.3849, grad_fn=<NllLossBackward0>)\n",
            "Epoch 235/300, Learning Rate: 4.8828125e-06 tensor(0.0570, grad_fn=<NllLossBackward0>)\n",
            "Epoch 236/300, Learning Rate: 4.8828125e-06 tensor(0.0190, grad_fn=<NllLossBackward0>)\n",
            "Epoch 237/300, Learning Rate: 4.8828125e-06 tensor(0.0236, grad_fn=<NllLossBackward0>)\n",
            "Epoch 238/300, Learning Rate: 4.8828125e-06 tensor(0.0158, grad_fn=<NllLossBackward0>)\n",
            "Epoch 239/300, Learning Rate: 4.8828125e-06 tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
            "Epoch 240/300, Learning Rate: 2.44140625e-06 tensor(0.0135, grad_fn=<NllLossBackward0>)\n",
            "Epoch 241/300, Learning Rate: 2.44140625e-06 tensor(0.0125, grad_fn=<NllLossBackward0>)\n",
            "Epoch 242/300, Learning Rate: 2.44140625e-06 tensor(0.0067, grad_fn=<NllLossBackward0>)\n",
            "Epoch 243/300, Learning Rate: 2.44140625e-06 tensor(0.0085, grad_fn=<NllLossBackward0>)\n",
            "Epoch 244/300, Learning Rate: 2.44140625e-06 tensor(0.0308, grad_fn=<NllLossBackward0>)\n",
            "Epoch 245/300, Learning Rate: 2.44140625e-06 tensor(0.0211, grad_fn=<NllLossBackward0>)\n",
            "Epoch 246/300, Learning Rate: 2.44140625e-06 tensor(0.0141, grad_fn=<NllLossBackward0>)\n",
            "Epoch 247/300, Learning Rate: 2.44140625e-06 tensor(0.0074, grad_fn=<NllLossBackward0>)\n",
            "Epoch 248/300, Learning Rate: 2.44140625e-06 tensor(0.0014, grad_fn=<NllLossBackward0>)\n",
            "Epoch 249/300, Learning Rate: 2.44140625e-06 tensor(0.0174, grad_fn=<NllLossBackward0>)\n",
            "Epoch 250/300, Learning Rate: 2.44140625e-06 tensor(0.0083, grad_fn=<NllLossBackward0>)\n",
            "Epoch 251/300, Learning Rate: 2.44140625e-06 tensor(0.0496, grad_fn=<NllLossBackward0>)\n",
            "Epoch 252/300, Learning Rate: 2.44140625e-06 tensor(0.0234, grad_fn=<NllLossBackward0>)\n",
            "Epoch 253/300, Learning Rate: 2.44140625e-06 tensor(0.1312, grad_fn=<NllLossBackward0>)\n",
            "Epoch 254/300, Learning Rate: 2.44140625e-06 tensor(0.0532, grad_fn=<NllLossBackward0>)\n",
            "Epoch 255/300, Learning Rate: 2.44140625e-06 tensor(0.0859, grad_fn=<NllLossBackward0>)\n",
            "Epoch 256/300, Learning Rate: 2.44140625e-06 tensor(0.0324, grad_fn=<NllLossBackward0>)\n",
            "Epoch 257/300, Learning Rate: 2.44140625e-06 tensor(0.0219, grad_fn=<NllLossBackward0>)\n",
            "Epoch 258/300, Learning Rate: 2.44140625e-06 tensor(0.0340, grad_fn=<NllLossBackward0>)\n",
            "Epoch 259/300, Learning Rate: 2.44140625e-06 tensor(0.2063, grad_fn=<NllLossBackward0>)\n",
            "Epoch 260/300, Learning Rate: 1.220703125e-06 tensor(0.0172, grad_fn=<NllLossBackward0>)\n",
            "Epoch 261/300, Learning Rate: 1.220703125e-06 tensor(0.0056, grad_fn=<NllLossBackward0>)\n",
            "Epoch 262/300, Learning Rate: 1.220703125e-06 tensor(0.0392, grad_fn=<NllLossBackward0>)\n",
            "Epoch 263/300, Learning Rate: 1.220703125e-06 tensor(0.0622, grad_fn=<NllLossBackward0>)\n",
            "Epoch 264/300, Learning Rate: 1.220703125e-06 tensor(0.0242, grad_fn=<NllLossBackward0>)\n",
            "Epoch 265/300, Learning Rate: 1.220703125e-06 tensor(0.0070, grad_fn=<NllLossBackward0>)\n",
            "Epoch 266/300, Learning Rate: 1.220703125e-06 tensor(0.0048, grad_fn=<NllLossBackward0>)\n",
            "Epoch 267/300, Learning Rate: 1.220703125e-06 tensor(0.0635, grad_fn=<NllLossBackward0>)\n",
            "Epoch 268/300, Learning Rate: 1.220703125e-06 tensor(0.0522, grad_fn=<NllLossBackward0>)\n",
            "Epoch 269/300, Learning Rate: 1.220703125e-06 tensor(0.0107, grad_fn=<NllLossBackward0>)\n",
            "Epoch 270/300, Learning Rate: 1.220703125e-06 tensor(0.0361, grad_fn=<NllLossBackward0>)\n",
            "Epoch 271/300, Learning Rate: 1.220703125e-06 tensor(0.0752, grad_fn=<NllLossBackward0>)\n",
            "Epoch 272/300, Learning Rate: 1.220703125e-06 tensor(0.0131, grad_fn=<NllLossBackward0>)\n",
            "Epoch 273/300, Learning Rate: 1.220703125e-06 tensor(0.0561, grad_fn=<NllLossBackward0>)\n",
            "Epoch 274/300, Learning Rate: 1.220703125e-06 tensor(0.0357, grad_fn=<NllLossBackward0>)\n",
            "Epoch 275/300, Learning Rate: 1.220703125e-06 tensor(0.0199, grad_fn=<NllLossBackward0>)\n",
            "Epoch 276/300, Learning Rate: 1.220703125e-06 tensor(0.0095, grad_fn=<NllLossBackward0>)\n",
            "Epoch 277/300, Learning Rate: 1.220703125e-06 tensor(0.1332, grad_fn=<NllLossBackward0>)\n",
            "Epoch 278/300, Learning Rate: 1.220703125e-06 tensor(0.0093, grad_fn=<NllLossBackward0>)\n",
            "Epoch 279/300, Learning Rate: 1.220703125e-06 tensor(0.0312, grad_fn=<NllLossBackward0>)\n",
            "Epoch 280/300, Learning Rate: 6.103515625e-07 tensor(0.0282, grad_fn=<NllLossBackward0>)\n",
            "Epoch 281/300, Learning Rate: 6.103515625e-07 tensor(0.0109, grad_fn=<NllLossBackward0>)\n",
            "Epoch 282/300, Learning Rate: 6.103515625e-07 tensor(0.0526, grad_fn=<NllLossBackward0>)\n",
            "Epoch 283/300, Learning Rate: 6.103515625e-07 tensor(0.0213, grad_fn=<NllLossBackward0>)\n",
            "Epoch 284/300, Learning Rate: 6.103515625e-07 tensor(0.0192, grad_fn=<NllLossBackward0>)\n",
            "Epoch 285/300, Learning Rate: 6.103515625e-07 tensor(0.0338, grad_fn=<NllLossBackward0>)\n",
            "Epoch 286/300, Learning Rate: 6.103515625e-07 tensor(0.0109, grad_fn=<NllLossBackward0>)\n",
            "Epoch 287/300, Learning Rate: 6.103515625e-07 tensor(0.0254, grad_fn=<NllLossBackward0>)\n",
            "Epoch 288/300, Learning Rate: 6.103515625e-07 tensor(0.0059, grad_fn=<NllLossBackward0>)\n",
            "Epoch 289/300, Learning Rate: 6.103515625e-07 tensor(0.3527, grad_fn=<NllLossBackward0>)\n",
            "Epoch 290/300, Learning Rate: 6.103515625e-07 tensor(0.0185, grad_fn=<NllLossBackward0>)\n",
            "Epoch 291/300, Learning Rate: 6.103515625e-07 tensor(0.0156, grad_fn=<NllLossBackward0>)\n",
            "Epoch 292/300, Learning Rate: 6.103515625e-07 tensor(0.0459, grad_fn=<NllLossBackward0>)\n",
            "Epoch 293/300, Learning Rate: 6.103515625e-07 tensor(0.0435, grad_fn=<NllLossBackward0>)\n",
            "Epoch 294/300, Learning Rate: 6.103515625e-07 tensor(0.0110, grad_fn=<NllLossBackward0>)\n",
            "Epoch 295/300, Learning Rate: 6.103515625e-07 tensor(0.2294, grad_fn=<NllLossBackward0>)\n",
            "Epoch 296/300, Learning Rate: 6.103515625e-07 tensor(0.0281, grad_fn=<NllLossBackward0>)\n",
            "Epoch 297/300, Learning Rate: 6.103515625e-07 tensor(0.0387, grad_fn=<NllLossBackward0>)\n",
            "Epoch 298/300, Learning Rate: 6.103515625e-07 tensor(0.0235, grad_fn=<NllLossBackward0>)\n",
            "Epoch 299/300, Learning Rate: 6.103515625e-07 tensor(0.1682, grad_fn=<NllLossBackward0>)\n",
            "Epoch 300/300, Learning Rate: 3.0517578125e-07 tensor(0.0358, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the Confusion Matrix of the Classifier"
      ],
      "metadata": {
        "id": "75KU_SNPwCeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(model, dataset, class_dict):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
        "    and plot it with class names.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained GNN model\n",
        "    - dataset: List of data objects\n",
        "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Evaluate the model and get predictions and true labels\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataset:\n",
        "            _, out = model(data.x, data.edge_index, data.batch)\n",
        "            pred = out.argmax(dim=1)\n",
        "            #data.y=torch.tensor(data.y)\n",
        "            all_preds.append(pred.numpy().flatten())\n",
        "            all_labels.append(data.y.numpy().flatten())\n",
        "\n",
        "    print(all_labels)\n",
        "    print(all_preds)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Step 2: Compute the confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Step 3: Plot the confusion matrix\n",
        "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
        "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
        "\n",
        "# Example dataset (assuming it's a list of data objects)\n",
        "# dataset = [...]\n",
        "\n",
        "# Call the function with the model, dataset (as a list), and class dictionary\n",
        "#plot_confusion_matrix(model, dataset, class_dict)\n"
      ],
      "metadata": {
        "id": "Ub2ns9s8ScHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict={0:'Tree',1:'Lollipop'}\n",
        "plot_confusion_matrix(model,dataset,class_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "t0LchX_6SfLR",
        "outputId": "1acabce9-c05c-427e-8c2b-7ef7a954463e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([0]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([0]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([0]), array([0]), array([0]), array([1])]\n",
            "[array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([0]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([0]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([1]), array([0]), array([1]), array([1]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([0]), array([1]), array([1]), array([0]), array([0]), array([0]), array([1])]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = ImprovedGCNWithMLP(hidden_channels=64, mlp_hidden_channels=128)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "# # Add a learning rate scheduler\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "\n",
        "# def train():\n",
        "#     model.train()\n",
        "\n",
        "#     for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "\n",
        "\n",
        "#         #  print(data.batch)\n",
        "#         #  # Example troubleshooting steps\n",
        "#         #  print(\"Number of nodes:\", data.x.size(0))\n",
        "#         #  print(\"Node indices in edge_index:\", data.edge_index.max().item())\n",
        "#         #  print(\"Node features shape:\", data.x.shape)\n",
        "#         #  print(\"Batch indices:\", data.batch.max().item())\n",
        "\n",
        "#          # Add more debugging information as needed\n",
        "\n",
        "#          # Example troubleshooting steps\n",
        "\n",
        "\n",
        "# # Add more debugging information as needed\n",
        "\n",
        "#          out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "#          loss = criterion(out, data.y)  # Compute the loss.\n",
        "#          loss.backward()  # Derive gradients.\n",
        "#          optimizer.step()  # Update parameters based on gradients.\n",
        "#          optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)\n",
        "         #out=torch.cat(out,dim=0)\n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "# for epoch in range(1, 800):\n",
        "#     train()\n",
        "\n",
        "#     train_acc = test(train_loader)\n",
        "#     test_acc = test(test_loader)\n",
        "#     print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "bwDJYoIFRUkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the path to the local folder on Colab where you want to save the model\n",
        "save_path = '/content/loltree.pth'\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), save_path)\n"
      ],
      "metadata": {
        "id": "GsQ70qglLDqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Zip the folder containing the model\n",
        "!zip -r /content.zip /content\n",
        "\n",
        "# Download the zipped folder\n",
        "files.download('/content.zip')\n"
      ],
      "metadata": {
        "id": "HPLGLBSwL-1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "load_path = '/content/loltree.pth'\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model architecture\n",
        "\n",
        "# Load the saved model weights\n",
        "model.load_state_dict(torch.load(load_path))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "AEgdKYOLMmpQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "39848629-7350-4bf4-b4bc-54bdc023888e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bc0de7276082>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load the saved model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1=[]\n",
        "data2=[]\n",
        "latent_data1=[]\n",
        "latent_data2=[]\n",
        "for i in range(len(dataset)):\n",
        "    model.eval()\n",
        "    data=dataset[i]\n",
        "    embedding , out = model(data.x, data.edge_index, data.batch)\n",
        "    pred = out.argmax(dim=1)\n",
        "    if(pred==0):\n",
        "        data1.append(data)\n",
        "        latent_data1.append(embedding)\n",
        "    else:\n",
        "        data2.append(data)\n",
        "        latent_data2.append(embedding)\n",
        "\n",
        "print(len(data1))\n",
        "print(len(data2))\n",
        "\n",
        "print(data.batch)\n",
        "\n",
        "#latent_explanations=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1Fs7YzORZPh",
        "outputId": "18254969-4103-41a9-fcf7-3ff39cb9e2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "100\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjlist=[]"
      ],
      "metadata": {
        "id": "LLVq8OYB01Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to extract graphs labelled by classifier above the threshold confidence level"
      ],
      "metadata": {
        "id": "cHt50eSvjY4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOfX0kbgYtYi"
      },
      "outputs": [],
      "source": [
        "def Dpc(dataset, threshold):\n",
        "    data1 = []\n",
        "    data2 = []\n",
        "\n",
        "    m = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        model.eval()\n",
        "        data = dataset[i]\n",
        "        embedding, out = model(data.x, data.edge_index, data.batch)\n",
        "        probs = m(out)\n",
        "\n",
        "        if threshold < probs[0][0]:\n",
        "            data1.append(data)\n",
        "        if threshold < probs[0][1]:\n",
        "            data2.append(data)\n",
        "\n",
        "\n",
        "    return data1, data2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Representation Model"
      ],
      "metadata": {
        "id": "LrLPZ1fQyfue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "N=25\n",
        "def GraphRepModel(targetclass,N):\n",
        "  #N=25 # Number of nodes of the largest graph that the model can generate\n",
        "\n",
        "  X=np.zeros((N,2))# Node Type Matrix for nodes of 10 types\n",
        "  #targetclass=data2\n",
        "  print(\"length of target class\",len(targetclass))\n",
        "\n",
        "  #Learning the node representations\n",
        "  for i in tqdm(range(len(targetclass))):#len(data1))\n",
        "      data=targetclass[i]\n",
        "      x=data.x\n",
        "      x=np.array(x,dtype=int)\n",
        "\n",
        "      #print(x)\n",
        "      for j in range(len(x)):\n",
        "          X[j][x[j]]+=1\n",
        "      for k in range(j+1,N):\n",
        "          X[k][0]+=1\n",
        "\n",
        "\n",
        "  #Learning the edge representations\n",
        "  Adj=np.zeros((N,N))# Edge type count for only two types edge present/edge absent\n",
        "  for i in tqdm(range(len(targetclass))):\n",
        "      data=targetclass[i]\n",
        "      adj=data.edge_index\n",
        "      rowlen=len(adj[0][:])\n",
        "      #print(rowlen)\n",
        "      #print(adj[:][0])\n",
        "      #print(adj[:][1])\n",
        "      for j in range(rowlen):\n",
        "          k1=adj[0][j]\n",
        "          k2=adj[1][j]\n",
        "          Adj[k1][k2]+=1\n",
        "          #Adj[k2][k1]+=1\n",
        "\n",
        "  #Learning the parameters for the distribution of nodes\n",
        "  numgraphs=len(targetclass)\n",
        "  print(numgraphs)\n",
        "  X=X/numgraphs #converting X to the node distribution matrix\n",
        "\n",
        "  Adj=Adj/numgraphs\n",
        "  return X , Adj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Hbu0egX8Ur",
        "outputId": "60cdc606-a5f4-42ab-c6ca-70d0f9002a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of target class 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 5713.61it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 1772.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "[[0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.12 0.88]\n",
            " [0.2  0.8 ]\n",
            " [0.32 0.68]\n",
            " [0.43 0.57]\n",
            " [0.55 0.45]\n",
            " [0.62 0.38]\n",
            " [0.76 0.24]\n",
            " [0.88 0.12]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aligning the data degreewise"
      ],
      "metadata": {
        "id": "u1uxVncbxRaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import degree\n",
        "import torch\n",
        "\n",
        "def align_data_degreewise(data_list):\n",
        "    \"\"\"\n",
        "    Aligns the node features (`data.x`) and edges (`data.edge_index`) of a list\n",
        "    of PyTorch Geometric `Data` objects degreewise in descending order.\n",
        "\n",
        "    Args:\n",
        "        data_list (list): A list of PyTorch Geometric `Data` objects.\n",
        "\n",
        "    Returns:\n",
        "        list: A new list of `Data` objects with aligned node features and edges.\n",
        "    \"\"\"\n",
        "    aligned_data_list = []\n",
        "\n",
        "    for data in data_list:\n",
        "        # Compute the degree of each node\n",
        "        node_degrees = degree(data.edge_index[0], num_nodes=data.x.size(0))\n",
        "\n",
        "        # Get the descending order of nodes by degree\n",
        "        sorted_indices = node_degrees.argsort(descending=True)\n",
        "\n",
        "        # Reorder the node features\n",
        "        aligned_x = data.x[sorted_indices]\n",
        "\n",
        "        # Create a mapping from old node indices to new indices\n",
        "        index_mapping = torch.zeros_like(sorted_indices)\n",
        "        index_mapping[sorted_indices] = torch.arange(len(sorted_indices))\n",
        "\n",
        "        # Update edge_index using the index_mapping\n",
        "        aligned_edge_index = index_mapping[data.edge_index]\n",
        "\n",
        "        # Create a new Data object with aligned features and edges\n",
        "        aligned_data = data.clone()\n",
        "        aligned_data.x = aligned_x\n",
        "        aligned_data.edge_index = aligned_edge_index\n",
        "\n",
        "        aligned_data_list.append(aligned_data)\n",
        "\n",
        "    return aligned_data_list\n"
      ],
      "metadata": {
        "id": "ftMqa05rxQB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1=align_data_degreewise(data1)\n",
        "data2=align_data_degreewise(data2)"
      ],
      "metadata": {
        "id": "EjzpcfH9xYMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting the Generative Distributions of all Target Classes"
      ],
      "metadata": {
        "id": "eNH6QOG2gSZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RMSe2pjOVV8z",
        "outputId": "789ef868-2c5c-4134-8fd1-46e1293ec373"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6676b1d98beb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdj1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGraphRepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(Adj1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdj2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGraphRepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdj3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGraphRepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdj4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGraphRepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data1' is not defined"
          ]
        }
      ],
      "source": [
        "X1,Adj1=GraphRepModel(data1,N)\n",
        "#print(Adj1)\n",
        "X2,Adj2=GraphRepModel(data2,N)\n",
        "datalist=[data1,data2]\n",
        "AllX=[X1,X2]\n",
        "AllAdj=[Adj1,Adj2]\n",
        "\n",
        "#print(Adj2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions to Calculate Various Graph Statistics"
      ],
      "metadata": {
        "id": "ZNXI8aW8ltpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def degree_distribution(data, N):\n",
        "    # Find all unique nodes present in the edge index tensor\n",
        "    all_nodes = torch.unique(torch.cat((data.edge_index[0], data.edge_index[1])))\n",
        "\n",
        "    # Calculate the number of nodes\n",
        "    num_nodes = int(all_nodes.max()) + 1 if all_nodes.numel() > 0 else 0  # If there are no nodes, set num_nodes to 0\n",
        "\n",
        "    # Calculate the degree of each node\n",
        "    degrees = torch.bincount(data.edge_index[0], minlength=num_nodes)\n",
        "\n",
        "    # Convert degrees to degree distribution\n",
        "    degree_values = torch.bincount(degrees)\n",
        "\n",
        "    # Make sure the tensor is of length N\n",
        "    degree_distribution = torch.cat((degree_values, torch.zeros(max(N - len(degree_values), 0)).to(degree_values.device)))\n",
        "\n",
        "    return degree_distribution\n",
        "\n",
        "\n",
        "def clustering_coefficient(data):\n",
        "    # Ensure the graph is undirected\n",
        "    #data = to_undirected(data)\n",
        "\n",
        "    # Calculate the number of nodes\n",
        "    num_nodes = data.x.shape[0]\n",
        "\n",
        "    # Calculate the number of triangles each node is involved in\n",
        "    num_triangles = torch.zeros(num_nodes, dtype=torch.float)\n",
        "\n",
        "    # Calculate the number of connected triples each node is involved in\n",
        "    num_connected_triples = torch.zeros(num_nodes, dtype=torch.float)\n",
        "\n",
        "    # Iterate over each edge in the graph\n",
        "    for i, j in data.edge_index.t().tolist():\n",
        "        # Get the neighbors of node i and node j\n",
        "        neighbors_i = set(data.edge_index[1][data.edge_index[0] == i].tolist())\n",
        "        neighbors_j = set(data.edge_index[1][data.edge_index[0] == j].tolist())\n",
        "\n",
        "        # Calculate the number of common neighbors between i and j\n",
        "        common_neighbors = neighbors_i.intersection(neighbors_j)\n",
        "\n",
        "        # Update the number of triangles and connected triples for nodes i and j\n",
        "        num_triangles[i] += len(common_neighbors)\n",
        "        num_triangles[j] += len(common_neighbors)\n",
        "        num_connected_triples[i] += len(neighbors_i) - 1\n",
        "        num_connected_triples[j] += len(neighbors_j) - 1\n",
        "\n",
        "    # Print intermediate results for debugging\n",
        "    #print(\"Number of triangles per node:\", num_triangles)\n",
        "    #print(\"Number of connected triples per node:\", num_connected_triples)\n",
        "\n",
        "    # Calculate the local clustering coefficient for each node\n",
        "    local_clustering_coefficient = (num_triangles / 2) / num_connected_triples\n",
        "    local_clustering_coefficient[torch.isnan(local_clustering_coefficient)] = 0  # Set NaN values to 0\n",
        "\n",
        "    # Print intermediate results for debugging\n",
        "    #print(\"Local clustering coefficient per node:\", local_clustering_coefficient)\n",
        "\n",
        "    # Calculate the average clustering coefficient\n",
        "    avg_clustering_coefficient = local_clustering_coefficient.mean()\n",
        "\n",
        "    return avg_clustering_coefficient\n",
        "\n",
        "def spectrum_distribution(data, k):\n",
        "    # Convert PyTorch Geometric Data object to a dense adjacency matrix\n",
        "    num_nodes = data.num_nodes\n",
        "    adj_matrix = torch_geometric.utils.to_dense_adj(data.edge_index).squeeze(0)\n",
        "\n",
        "    # Calculate degree matrix\n",
        "    degree = torch.sum(adj_matrix, dim=1)\n",
        "    degree_matrix = torch.diag(degree)\n",
        "\n",
        "    # Calculate Laplacian matrix\n",
        "    laplacian_matrix = degree_matrix - adj_matrix\n",
        "\n",
        "    # Convert Laplacian to dense matrix and numpy array\n",
        "    laplacian_dense = laplacian_matrix.numpy()\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = np.linalg.eig(laplacian_dense)\n",
        "\n",
        "    # Sort eigenvalues in ascending order\n",
        "    eigenvalues = np.sort(eigenvalues)\n",
        "\n",
        "    # If k is greater than the number of eigenvalues, pad with zeros\n",
        "    if k > len(eigenvalues):\n",
        "        padded_eigenvalues = np.pad(eigenvalues, (0, k - len(eigenvalues)), 'constant')\n",
        "    else:\n",
        "        padded_eigenvalues = eigenvalues[:k]\n",
        "\n",
        "    return padded_eigenvalues\n",
        "\n",
        "# Example usage\n",
        "# Assuming 'data' is your PyTorch Geometric Data object and 'k' is the desired number of eigenvalues\n",
        "# eigenvalues = spectrum_distribution(data, k)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n-Ch2LsLloa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Calculate MMD using Gaussian Kernel"
      ],
      "metadata": {
        "id": "5Gyggs5foZ1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41a1152-15ad-4013-89e5-ef7cffea7570",
        "id": "bd4AnxNWz0dk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MMD between the two samples: 0.0001\n"
          ]
        }
      ],
      "source": [
        "def mmd_kernel(X, Y, kernel_func):\n",
        "    \"\"\"\n",
        "    Compute Maximum Mean Discrepancy (MMD) between two samples using a given kernel function.\n",
        "\n",
        "    Parameters:\n",
        "    - X: torch.Tensor, shape (n_samples, n_features)\n",
        "    - Y: torch.Tensor, shape (m_samples, n_features)\n",
        "    - kernel_func: function, kernel function to compute pairwise kernel values\n",
        "\n",
        "    Returns:\n",
        "    - mmd: float, Maximum Mean Discrepancy\n",
        "    \"\"\"\n",
        "    m, n = len(X), len(Y)\n",
        "\n",
        "    # Compute kernel matrices\n",
        "    K_xx = kernel_func(X, X)\n",
        "    K_yy = kernel_func(Y, Y)\n",
        "    K_xy = kernel_func(X, Y)\n",
        "\n",
        "    # Compute MMD statistic\n",
        "    mmd = 1.0 / (m * (m - 1)) * torch.sum(K_xx - torch.diag(torch.diagonal(K_xx))) + \\\n",
        "          1.0 / (n * (n - 1)) * torch.sum(K_yy - torch.diag(torch.diagonal(K_yy))) - \\\n",
        "          2.0 / (m * n) * torch.sum(K_xy)\n",
        "\n",
        "    return mmd.item()  # Convert the result to a Python float\n",
        "\n",
        "# Example of using a Gaussian (RBF) kernel function for PyTorch\n",
        "def gaussian_kernel(X, Y, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Gaussian (RBF) kernel function.\n",
        "\n",
        "    Parameters:\n",
        "    - X: torch.Tensor, shape (n_samples, n_features)\n",
        "    - Y: torch.Tensor, shape (m_samples, n_features)\n",
        "    - sigma: float, bandwidth parameter of the kernel\n",
        "\n",
        "    Returns:\n",
        "    - K: torch.Tensor, shape (n_samples, m_samples), kernel matrix\n",
        "    \"\"\"\n",
        "    pairwise_sq_dists = torch.sum(X**2, dim=1, keepdim=True) + torch.sum(Y**2, dim=1, keepdim=True).t() - 2 * torch.mm(X, Y.t())\n",
        "    K = torch.exp(-pairwise_sq_dists / (2.0 * sigma**2))\n",
        "    return K\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to sample a graph from the generative model"
      ],
      "metadata": {
        "id": "zEtYTtuvhy9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dgsedzUz0dl"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Categorical\n",
        "from torch_geometric.utils import subgraph\n",
        "from torch_geometric.utils import remove_isolated_nodes\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_networkx\n",
        "def graphsampler(N,X,Adj,show=False):\n",
        "  nodeset=[]\n",
        "  keepnodes=[]\n",
        "  newnodes=[]\n",
        "  count=0\n",
        "\n",
        "\n",
        "  for i in range(N):\n",
        "      probs=X[i][:]\n",
        "      probs=torch.from_numpy(probs)\n",
        "      m=Categorical(probs)\n",
        "      chosennode=m.sample()\n",
        "      nodeset.append(chosennode)\n",
        "      if(chosennode!=0):\n",
        "          keepnodes.append(i)\n",
        "          newnodes.append(chosennode)\n",
        "  # print(\"Whole set of nodes \")\n",
        "  # print(nodeset)\n",
        "  # print(\"Nodes to keep\")\n",
        "  # print(keepnodes)\n",
        "  # print(newnodes)\n",
        "  # #print(Adjacency)\n",
        "  keepnodescp=keepnodes.copy()\n",
        "  Adj1=torch.from_numpy(Adj)\n",
        "  Adjacency= torch.bernoulli(Adj1)\n",
        "  #Symmetrizing the adjacency matrix as it is a dataset of undirected graphs\n",
        "  for i in range(N):\n",
        "    for j in range(i+1):\n",
        "      Adjacency[i][j]=Adjacency[j][i]\n",
        "  #print(Adjacency.size())\n",
        "  # Removing edges generated for absent nodes\n",
        "  for i in range(len(nodeset)):\n",
        "    if(nodeset[i]==0):\n",
        "\n",
        "      Adjacency[i][:]=0\n",
        "      Adjacency[:][i]=0\n",
        "\n",
        "  #print(\"Length of keepnodes list\",len(keepnodes))\n",
        "\n",
        "  templen =  len(keepnodes)\n",
        "  for j in range(templen):\n",
        "    testval=0\n",
        "  # print(\"value of loop counter j\",j)\n",
        "    place=keepnodes[j]\n",
        "    #print(\"corresponding keepnode value\",place)\n",
        "    for k in range(10):\n",
        "      if(Adjacency[place][k]!=0):\n",
        "        testval=1\n",
        "        break\n",
        "    if(testval==0):\n",
        "      keepnodescp.remove(place)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # To test how potent the explainer is we need to feed samples generated from this to the GNN to classify\n",
        "  #First we need to convert the data into the tensor proper for fitting into the GNN\n",
        "  edge_ind=Adjacency.nonzero().t().contiguous()\n",
        "  newgraph=Data(x=nodeset,edge_index=edge_ind)\n",
        "  #print(\"Edge index of graph with all nodes\")\n",
        "  #print(edge_ind)\n",
        "  #print(newgraph.x)\n",
        "\n",
        "\n",
        "  #print(\"Finally nodes that remain in the graph\")\n",
        "  #print(keepnodescp)\n",
        "\n",
        "  extractededge=subgraph(keepnodescp,edge_ind,relabel_nodes=True)\n",
        "  extedge=subgraph(keepnodescp,edge_ind,relabel_nodes=False)\n",
        "\n",
        "  # print(\"Final edges with nodes same as before\")\n",
        "  # print(extedge[0])\n",
        "  # print(\"Final edges with relabelled nodes\")\n",
        "  # print(extractededge[0])\n",
        "\n",
        "\n",
        "  newnodes1=[nodeset[i] for i in keepnodescp]\n",
        "  newnodes=newnodes1\n",
        "  x1=torch.ones(len(newnodes),1)\n",
        "  x1=x1.float()\n",
        "  newnodes=torch.stack(newnodes)\n",
        "\n",
        "\n",
        "\n",
        "  newnodes=newnodes.float()\n",
        "\n",
        "\n",
        "  finalgraph=Data(x=x1,edge_index=extractededge[0])\n",
        "\n",
        "\n",
        "  embedding,out=model(finalgraph.x,finalgraph.edge_index,finalgraph.batch)\n",
        "  soft=torch.nn.Softmax(dim=1)\n",
        "  #print(soft(out))\n",
        "  #print(out)\n",
        "  explainergraph=to_networkx(finalgraph,to_undirected=True)\n",
        "  if show:\n",
        "    nx.draw_networkx(explainergraph, node_size=150, node_color='red',with_labels=False)\n",
        "  problities=soft(out)\n",
        "  explainernodes=nx.number_of_nodes(explainergraph)\n",
        "  explaineredges=nx.number_of_edges(explainergraph)\n",
        "  explainerdensity=(explaineredges)/(explainernodes*explainernodes)\n",
        "  return problities,explainerdensity,embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for Target Class Analysis"
      ],
      "metadata": {
        "id": "PCl-AWFWmY4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import torch\n",
        "\n",
        "def target_class_analysis(X, Adj, N, latent,numexplanations=50, numsample=10, label=0):\n",
        "    \"\"\"\n",
        "    Analyzes the robustness of a target class in graph data by sampling graphs\n",
        "    and evaluating accuracy, density, and other metrics.\n",
        "\n",
        "    Args:\n",
        "        X: Node feature matrix.\n",
        "        Adj: Adjacency matrix.\n",
        "        N: Number of nodes in the graph.\n",
        "        latent: Latent space embeddings of the original data.\n",
        "        numexplanations: Number of explanations to generate.\n",
        "        numsample: Number of samples to generate.\n",
        "        label: Target class label to analyze (default: 0).\n",
        "\n",
        "    Returns:\n",
        "        meanaccuracy: Mean accuracy.\n",
        "        stdaccuracy: Standard deviation of accuracies.\n",
        "        meandensity: Mean density.\n",
        "        stddensity: Standard deviation of densities.\n",
        "        MMD: Maximum Mean Discrepancy (MMD) distance.\n",
        "        degreedistlist: Degree distributions across samples.\n",
        "        cluslist: Clustering coefficients across samples.\n",
        "        speclist: Spectral metrics across samples.\n",
        "    \"\"\"\n",
        "    accuracy = []\n",
        "    density = []\n",
        "    embeddings = []\n",
        "    degreedistlist = []\n",
        "    cluslist = []\n",
        "    speclist = []\n",
        "\n",
        "    for _ in range(50):\n",
        "        newX = X  # Placeholder for potential feature perturbation\n",
        "        newAdj = Adj  # Placeholder for potential adjacency perturbation\n",
        "        max_prob = 0\n",
        "\n",
        "        for _ in range(numsample):\n",
        "            probabilities, explainerdensity, embedding = graphsampler(N, newX, newAdj)\n",
        "\n",
        "            if probabilities[0][label] > max_prob:\n",
        "                sampleaccuracy = probabilities[0][label]\n",
        "                sampledensity = explainerdensity\n",
        "                sampleembedding = embedding\n",
        "                sampledeg = degreedistb\n",
        "                sampleclus = clus\n",
        "                samplespec = spec\n",
        "                max_prob = probabilities[0][label]\n",
        "\n",
        "        accuracy.append(sampleaccuracy)\n",
        "        density.append(sampledensity)\n",
        "        embeddings.append(sampleembedding)\n",
        "        # degreedistlist.append(sampledeg)\n",
        "        # cluslist.append(sampleclus)\n",
        "        # speclist.append(samplespec)\n",
        "\n",
        "    meanaccuracy = torch.mean(torch.tensor(accuracy))\n",
        "    stdaccuracy = torch.std(torch.tensor(accuracy))\n",
        "    meandensity = statistics.mean(density)\n",
        "    stddensity = statistics.stdev(density)\n",
        "\n",
        "    MMD = mmd_kernel(torch.cat(latent, dim=0), torch.cat(embeddings, dim=0), gaussian_kernel)\n",
        "\n",
        "    return meanaccuracy, stdaccuracy, meandensity, stddensity\n"
      ],
      "metadata": {
        "id": "qRllXeKVB_Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the Above Function to generate metrics for a Target CLass"
      ],
      "metadata": {
        "id": "lFAgEIUUnINb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time=time.time()\n",
        "meanacc,stdacc, meandensity, stddensity, MMD, deg,cluslist,speclist  =target_class_analysis(X2,Adj2,N,latent_data2,50,20,1)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n"
      ],
      "metadata": {
        "id": "KRJc4c5TnCAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Approximate the Boundary Generative Distribution and Sample from it"
      ],
      "metadata": {
        "id": "LuJUMTWunXu5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0zMvj720ArQ"
      },
      "outputs": [],
      "source": [
        "def boundarysampler(class_indices,N, AdjList=AllAdj,Xlist=AllAdj,numtrials=100,numsample=100,pmix=0.5):\n",
        "  label1=class_indices[0]\n",
        "  label2=class_indices[1]\n",
        "  print(label1,label2)\n",
        "  X1=AllX[label1]\n",
        "  X2=AllX[label2]\n",
        "  Adj1=AdjList[label1]\n",
        "  Adj2=AdjList[label2]\n",
        "  X=pmix*X1+(1-pmix)*X2\n",
        "  Adj=pmix*Adj1+(1-pmix)*Adj2\n",
        "  accuracies=[]\n",
        "  embeddings=[]\n",
        "  acc2=[]\n",
        "  for _ in range(numtrials):\n",
        "    min_difference = float('inf')\n",
        "    sample_accuracy = None\n",
        "    sample_embedding = None\n",
        "\n",
        "    for _ in range(numsample):\n",
        "        probabilities, _, embedding = graphsampler(N, X, Adj, show=False)\n",
        "\n",
        "        # Difference in class scores\n",
        "        score_diff = abs(probabilities[0][label1] - probabilities[0][label2])\n",
        "\n",
        "        if score_diff < min_difference:\n",
        "            min_difference = score_diff\n",
        "            sample_accuracy = probabilities[0][label1]\n",
        "            otheracc=1-probabilities[0][label1]\n",
        "            sample_embedding = embedding\n",
        "\n",
        "    accuracies.append(sample_accuracy)\n",
        "    acc2.append(otheracc)\n",
        "    embeddings.append(sample_embedding)\n",
        "\n",
        "  # Calculate mean and standard deviation of accuracies\n",
        "  mean_accuracy = torch.mean(torch.tensor(accuracies))\n",
        "  mean_acc2=torch.mean(torch.tensor(acc2))\n",
        "  std_accuracy = torch.std(torch.tensor(accuracies))\n",
        "  return mean_accuracy,mean_acc2, std_accuracy, embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First find out whether two classes share a boundary by computing their adjacency score. If the score is above 0.9, consider that they share a boundary"
      ],
      "metadata": {
        "id": "q0ENgAclnllW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvNuT8MHvNXB",
        "outputId": "4325a1c8-0bc7-40ad-dd2a-74e6cbc883bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "classifier=model.classifier\n",
        "class_indices=[0,1]\n",
        "adjscore=adjacency_score(model,classifier,data1,data2,class_indices)\n",
        "print(adjscore)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9yrN1yP5uch"
      },
      "outputs": [],
      "source": [
        "#class_indices=[0,2]\n",
        "accuracies,acc2, stdacc, embeddings=boundarysampler([0,1],N,AllAdj,AllX,numtrials=2,numsample=300)\n",
        "print(accuracies)\n",
        "print(stdacc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw5C3I2Bg1eD"
      },
      "source": [
        "# Functions to Compute Boundary Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHGTBOVJnC-n"
      },
      "outputs": [],
      "source": [
        "# Computing boundary metrics\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def boundary_margin(embeddings_c1, embeddings_c2):\n",
        "    \"\"\"\n",
        "    Compute the boundary margin.\n",
        "\n",
        "    Args:\n",
        "    - embeddings_c1 (torch.Tensor): Embeddings of class c1 graphs.\n",
        "    - embeddings_c2 (torch.Tensor): Embeddings of boundary graphs between class c1 and c2.\n",
        "\n",
        "    Returns:\n",
        "    - margin (float): The boundary margin.\n",
        "\n",
        "    \"\"\"\n",
        "    embeddings_c1=torch.cat(embeddings_c1,dim=0)\n",
        "    embeddings_c2=torch.cat(embeddings_c2,dim=0)\n",
        "    distances = torch.norm(embeddings_c1 - embeddings_c2, dim=1)\n",
        "    margin = torch.min(distances).item()\n",
        "    return margin\n",
        "\n",
        "def boundary_thickness(embeddings_c1, embeddings_c1_c2, model, c1, c2, gamma=0.75, num_points=100):\n",
        "    thickness_values = []\n",
        "\n",
        "    for emb_c1, emb_c1_c2 in zip(embeddings_c1, embeddings_c1_c2):\n",
        "        t_values = torch.linspace(0, 1, num_points)\n",
        "        h_t = (1 - t_values).unsqueeze(1) * emb_c1 + t_values.unsqueeze(1) * emb_c1_c2\n",
        "        #print(model(h_t).size())\n",
        "\n",
        "        # Compute the logits\n",
        "        logits_h_t = model(h_t)  # Assuming `model` is your classifier\n",
        "        probs_h_t = F.softmax(logits_h_t, dim=1)\n",
        "\n",
        "        # Compute the integrand\n",
        "        integrand = (gamma > (probs_h_t[:, c1] - probs_h_t[:, c2])).float()\n",
        "\n",
        "        # Approximate the integral using the trapezoidal rule\n",
        "        integral = torch.trapz(integrand, t_values)\n",
        "\n",
        "        # Compute the thickness value\n",
        "        thickness_value = (emb_c1 - emb_c1_c2).norm() * integral.mean()\n",
        "        thickness_values.append(thickness_value.item())\n",
        "\n",
        "    return sum(thickness_values) / len(thickness_values)\n",
        "\n",
        "# def boundary_complexity(embeddings, D):\n",
        "#     \"\"\"\n",
        "#     Compute the boundary complexity.\n",
        "\n",
        "#     Args:\n",
        "#     - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
        "#     - D (int): Dimensionality of the embeddings.\n",
        "\n",
        "#     Returns:\n",
        "#     - complexity (float): The boundary complexity.\n",
        "#     \"\"\"\n",
        "#     # Compute the covariance matrix of the embeddings\n",
        "#     embeddings=torch.cat(embeddings,dim=0)\n",
        "#     covariance_matrix = torch.cov(embeddings.T)\n",
        "\n",
        "#     # Compute the eigenvalues of the covariance matrix\n",
        "#     eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
        "#     print(eigenvalues)\n",
        "\n",
        "#     # Normalize the eigenvalues\n",
        "#     eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
        "#     print(eigenvalues_normalized)\n",
        "\n",
        "#     # Compute the entropy of the normalized eigenvalues\n",
        "#     entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + 1e-7))\n",
        "#     print(entropy)\n",
        "\n",
        "#     # Normalize the entropy by dividing it by log(D)\n",
        "#     complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
        "\n",
        "#     return complexity.item()\n",
        "def boundary_complexity(embeddings, D, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Compute the boundary complexity.\n",
        "\n",
        "    Args:\n",
        "    - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
        "    - D (int): Dimensionality of the embeddings.\n",
        "    - epsilon (float): Small value added to eigenvalues to prevent log(0).\n",
        "\n",
        "    Returns:\n",
        "    - complexity (float): The boundary complexity.\n",
        "    \"\"\"\n",
        "    # Flatten and concatenate embeddings\n",
        "    embeddings = torch.cat(embeddings, dim=0)\n",
        "\n",
        "    # Compute the covariance matrix of the embeddings\n",
        "    covariance_matrix = torch.cov(embeddings.T)\n",
        "\n",
        "    # Add a small value to the diagonal for regularization\n",
        "    covariance_matrix += epsilon * torch.eye(covariance_matrix.size(0))\n",
        "\n",
        "    # Compute the eigenvalues of the covariance matrix\n",
        "    eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
        "\n",
        "    # Clamp eigenvalues to avoid very small negative values due to numerical errors\n",
        "    eigenvalues = torch.clamp(eigenvalues, min=epsilon)\n",
        "\n",
        "    # Normalize the eigenvalues\n",
        "    eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
        "\n",
        "    # Compute the entropy of the normalized eigenvalues\n",
        "    entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + epsilon))\n",
        "\n",
        "    # Normalize the entropy by dividing it by log(D)\n",
        "    complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
        "\n",
        "    return complexity.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMPPyubSnGC7",
        "outputId": "ae5381e1-475e-4aa0-f8a6-3de4c2da49dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.846235752105713\n",
            "64.27653145328725\n",
            "0.005051757209002972\n"
          ]
        }
      ],
      "source": [
        "boundaryembeddings=embeddings\n",
        "latent_data=latent_data2\n",
        "margin=boundary_margin(boundaryembeddings[:len(latent_data)],latent_data)\n",
        "print(margin)\n",
        "thickness=boundary_thickness(boundaryembeddings[:len(latent_data)],latent_data,model.classifier,1,0)\n",
        "print(thickness)\n",
        "complexity=boundary_complexity(boundaryembeddings[:len(latent_data)],64)\n",
        "print(complexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ImYLii172z"
      },
      "source": [
        "# Function for Sensitivity Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXUHpU_SZ9MN"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "def sensitivityanalysis(X,Adj,N,numsample,label=0):\n",
        "  epsilon=0.01\n",
        "  alpha=0.0\n",
        "  # perturbedX=perturbX(X,epsilon)\n",
        "  # perturbedAdj=perturbadj(Adj,epsilon)\n",
        "  meanaccuracy=[]\n",
        "  stdaccuracy=[]\n",
        "  meandensity=[]\n",
        "  stddensity=[]\n",
        "  MMD=[]\n",
        "\n",
        "  while(alpha==0.0):\n",
        "    accuracy=[]\n",
        "    density=[]\n",
        "    embeddings=[]\n",
        "    for i in range(100):\n",
        "      newX=X#nodefeatcomb(perturbedX,X,alpha)\n",
        "      newAdj=Adj#adjcomb(perturbedAdj,Adj,alpha)\n",
        "      max=0\n",
        "      for num in range(numsample):\n",
        "        problities,explainerdensity,embedding=graphsampler(N,newX,newAdj)\n",
        "        #max=problities[0][label]\n",
        "        if(problities[0][label]>max):\n",
        "          sampleaccuracy=problities[0][label]\n",
        "          sampledensity=explainerdensity\n",
        "          sampleembedding=embedding\n",
        "          max=problities[0][label]\n",
        "      accuracy.append(sampleaccuracy)\n",
        "      density.append(sampledensity)\n",
        "      embeddings.append(sampleembedding)\n",
        "    meanacc=torch.mean(torch.stack(accuracy))\n",
        "    stdacc=torch.std(torch.stack(accuracy))\n",
        "    meanden=statistics.mean(density)\n",
        "    stdden=statistics.stdev(density)\n",
        "    meanaccuracy.append(meanacc)\n",
        "    stdaccuracy.append(stdacc)\n",
        "    # meandensity.append(meanden)\n",
        "    # stddensity.append(stdden)\n",
        "    # MMDdist=mmd_kernel(torch.cat(latent,dim=0),torch.cat(embeddings,dim=0),gaussian_kernel)\n",
        "    # MMD.append(MMDdist)\n",
        "    alpha+=0.05\n",
        "  return meanaccuracy , stdaccuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu-YYMNDIxVj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_mean_with_error(mean, std, threshold, label, numsample,title=None, ax=None):\n",
        "    \"\"\"\n",
        "    Plot mean with error bars.\n",
        "\n",
        "    Parameters:\n",
        "        mean (array_like): Array containing mean values.\n",
        "        std (array_like): Array containing standard deviation values.\n",
        "        threshold (array_like): Array containing threshold values.\n",
        "        label (str): Label for the data.\n",
        "        color (str): Color of the line.\n",
        "        numsample (int): Sample number.\n",
        "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If not provided, a new figure will be created.\n",
        "    \"\"\"\n",
        "    # Flatten the arrays\n",
        "    mean=torch.tensor(mean,dtype=torch.float32)\n",
        "    std=torch.tensor(std,dtype=torch.float32)\n",
        "    mean = np.array(mean).flatten()\n",
        "    std = np.array(std).flatten()\n",
        "    threshold = np.array(threshold).flatten()\n",
        "    # Select color automatically\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "    color = colors[numsample % 10]  # Cycle through colors\n",
        "\n",
        "    # Plotting\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    ax.errorbar(threshold, mean, yerr=std, fmt='-', color=color, label=f'NumSample={numsample}')  # '-' for line\n",
        "\n",
        "    # Adding labels and title\n",
        "    ax.set_xlabel('Threshold')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title(title)\n",
        "\n",
        "    ax.legend(loc='lower right',fontsize='small')  # Show legend\n",
        "    ax.grid(True)  # Add grid\n",
        "# # Create a figure outside the function\n",
        "# fig, ax = plt.subplots()\n",
        "# plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=1,ax=ax)\n",
        "# plot_mean_with_error(Mean2,Std2,Threshold,label='class1',numsample=2,ax=ax)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sensitivity Analysis by Varying p_c and numsample"
      ],
      "metadata": {
        "id": "hvv67qlYiXwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpyb55jstCru"
      },
      "outputs": [],
      "source": [
        "samplerange=6\n",
        "fig1, ax1 = plt.subplots()\n",
        "fig2, ax2 = plt.subplots()\n",
        "fig3, ax3= plt.subplots()\n",
        "fig4, ax4= plt.subplots()\n",
        "for numsample in range(1,samplerange):\n",
        "  threshold=0.5\n",
        "  Threshold=[]\n",
        "  Mean1=[]\n",
        "  Std1=[]\n",
        "  Mean2=[]\n",
        "  Std2=[]\n",
        "  Mean3=[]\n",
        "  Std3=[]\n",
        "  Mean4=[]\n",
        "  Std4=[]\n",
        "  while(threshold<1):\n",
        "    Threshold.append(threshold)\n",
        "    data1 , data2, data3, data4=Dpc(dataset,threshold)\n",
        "    X1,Adj1=GraphRepModel(data1,N)\n",
        "    X2,Adj2=GraphRepModel(data2,N)\n",
        "    mean1,std1=sensitivityanalysis(X1,Adj1,N,numsample,0)\n",
        "    Mean1.append(mean1)\n",
        "    Std1.append(std1)\n",
        "    mean2,std2=sensitivityanalysis(X2,Adj2,N,numsample,1)\n",
        "    Mean2.append(mean2)\n",
        "    Std2.append(std2)\n",
        "    threshold+=0.05\n",
        "  plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=numsample,title='Sensitivity analysis on Star ',ax=ax1)\n",
        "  plot_mean_with_error(Mean2,Std2,Threshold,label='class2',numsample=numsample,title='Sensitivity analysis on Windmill',ax=ax2)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}