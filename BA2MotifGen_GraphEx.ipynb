{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and Importing Relevant Dependencies"
      ],
      "metadata": {
        "id": "kRnskV0k6Jbb"
      },
      "id": "kRnskV0k6Jbb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb8593b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbb8593b",
        "outputId": "01bc29de-bd4c-42ac-f6fd-4e8de88e81e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95abc879",
      "metadata": {
        "id": "95abc879"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import torch_geometric as torch_geometric\n",
        "import math\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.datasets import TUDataset, Planetoid, BAShapes\n",
        "from torch_geometric.datasets import Reddit\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "#from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import GCNConv,GINConv\n",
        "from torch.distributions import Bernoulli,Categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import BA2MotifDataset\n",
        "dataset=BA2MotifDataset(root=\"data\")"
      ],
      "metadata": {
        "id": "xZr-z96Kf8mE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464b6f6f-a5e7-4758-cccd-729a3dddb4eb"
      },
      "id": "xZr-z96Kf8mE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/flyingdoog/PGExplainer/raw/master/dataset/BA-2motif.pkl\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the Dataset"
      ],
      "metadata": {
        "id": "101w8BfT6W2x"
      },
      "id": "101w8BfT6W2x"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "data=dataset[0]\n",
        "print(data)\n",
        "exgraph=to_networkx(data,to_undirected=True)\n",
        "nx.draw_networkx(exgraph, node_size=150, node_color='red',with_labels=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oIBVyrGb_ODE",
        "outputId": "5c13d0a4-f1b7-4a69-9bd9-9eed411aea3e"
      },
      "id": "oIBVyrGb_ODE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[25, 10], edge_index=[2, 50], y=[1])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train-Test Split in 80:20 ratio**"
      ],
      "metadata": {
        "id": "_kBlyGz66a6x"
      },
      "id": "_kBlyGz66a6x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "148313c8",
        "outputId": "f2ea9096-5acc-4a83-e6c8-703df228ea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "Number of training graphs: 800\n",
            "Number of test graphs: 200\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "torch.manual_seed(12345)\n",
        "dataset.shuffle(dataset)\n",
        "print(len(dataset))\n",
        "\n",
        "train_dataset = dataset[:800]\n",
        "test_dataset = dataset[800:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "id": "148313c8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching using a Dataloader"
      ],
      "metadata": {
        "id": "xK3tyuTa6IJw"
      },
      "id": "xK3tyuTa6IJw"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7VeCeFfBnco",
        "outputId": "719890bf-d45d-49c7-e53d-d75c4b81887c"
      },
      "id": "R7VeCeFfBnco",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3242], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3242], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3246], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 4:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3249], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 5:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3246], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 6:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3251], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 7:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3242], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 8:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3257], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 9:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3252], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 10:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3237], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 11:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3247], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 12:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(x=[1600, 10], edge_index=[2, 3233], y=[64], batch=[1600], ptr=[65])\n",
            "\n",
            "Step 13:\n",
            "=======\n",
            "Number of graphs in the current batch: 32\n",
            "DataBatch(x=[800, 10], edge_index=[2, 1622], y=[32], batch=[800], ptr=[33])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# See how a data object looks like"
      ],
      "metadata": {
        "id": "Cqn_u08k6umJ"
      },
      "id": "Cqn_u08k6umJ"
    },
    {
      "cell_type": "code",
      "source": [
        "exdata=dataset[0]\n",
        "print(exdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "monzrAlmBrli",
        "outputId": "53d037ce-c086-4bd1-f39c-50c590b6089a"
      },
      "id": "monzrAlmBrli",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[25, 10], edge_index=[2, 50], y=[1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining and Training the Classifier"
      ],
      "metadata": {
        "id": "mKR08E7160Za"
      },
      "id": "mKR08E7160Za"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, inputdim,hidden_channels):\n",
        "        super(GCNEncoder, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(inputdim, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "        self.leaky_relu = torch.nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.bn(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x\n",
        "\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.linear = Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self,inputdim, hidden_channels, num_classes):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.encoder = GCNEncoder(inputdim,hidden_channels)\n",
        "        self.classifier = LinearClassifier(input_dim=hidden_channels, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Get the embeddings from the encoder\n",
        "        embeddings = self.encoder(x, edge_index, batch)\n",
        "\n",
        "        # Get the logits from the classifier\n",
        "        logits = self.classifier(embeddings)\n",
        "\n",
        "        return embeddings, logits\n",
        "num_features=10\n",
        "inputdim=num_features\n",
        "model=CombinedModel(inputdim, hidden_channels=64,num_classes=2)\n"
      ],
      "metadata": {
        "id": "X02b_YQGEH3Y"
      },
      "id": "X02b_YQGEH3Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Add a learning rate scheduler\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
        "# def train():\n",
        "#     model.train()\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "#             embedding,  out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "#             #print(out)\n",
        "#             loss = criterion(out, data.y)  # Compute the loss.\n",
        "#             loss.backward()  # Derive gradients.\n",
        "#             optimizer.step()  # Update parameters based on gradients.\n",
        "#             optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "#         # Update the learning rate scheduler\n",
        "#         scheduler.step()\n",
        "\n",
        "#         # Print the current learning rate every epoch (optional)\n",
        "#         print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\",loss)\n",
        "#         # train_acc = test(train_loader)\n",
        "#         # test_acc = test(test_loader)\n",
        "#         # print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "            # Forward pass\n",
        "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "            embedding, out = model(x, edge_index, batch)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(out, data.y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Print the current learning rate and loss every epoch\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}, Loss: {loss.item()}\")\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 2000\n",
        "\n",
        "# Call the training loop\n",
        "train()\n",
        "\n",
        "\n",
        "# # Set the number of epochs\n",
        "# num_epochs = 800\n",
        "\n",
        "# # Call the training loop\n",
        "# train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7X7tx_CEL6d",
        "outputId": "2e58563d-e826-4a25-c01b-08c227ca9f20"
      },
      "id": "D7X7tx_CEL6d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000, Learning Rate: 0.0099, Loss: 0.3005177974700928\n",
            "Epoch 2/2000, Learning Rate: 0.009801, Loss: 0.6440449357032776\n",
            "Epoch 3/2000, Learning Rate: 0.00970299, Loss: 0.6231232285499573\n",
            "Epoch 4/2000, Learning Rate: 0.0096059601, Loss: 0.3445626497268677\n",
            "Epoch 5/2000, Learning Rate: 0.009509900499, Loss: 0.5667904019355774\n",
            "Epoch 6/2000, Learning Rate: 0.00941480149401, Loss: 0.5152316689491272\n",
            "Epoch 7/2000, Learning Rate: 0.0093206534790699, Loss: 0.8426088094711304\n",
            "Epoch 8/2000, Learning Rate: 0.0092274469442792, Loss: 0.5653101205825806\n",
            "Epoch 9/2000, Learning Rate: 0.009135172474836408, Loss: 0.5057961940765381\n",
            "Epoch 10/2000, Learning Rate: 0.009043820750088045, Loss: 0.5383349061012268\n",
            "Epoch 11/2000, Learning Rate: 0.008953382542587164, Loss: 0.39111849665641785\n",
            "Epoch 12/2000, Learning Rate: 0.008863848717161293, Loss: 0.5515310764312744\n",
            "Epoch 13/2000, Learning Rate: 0.00877521022998968, Loss: 0.4282321631908417\n",
            "Epoch 14/2000, Learning Rate: 0.008687458127689783, Loss: 0.32521024346351624\n",
            "Epoch 15/2000, Learning Rate: 0.008600583546412886, Loss: 0.3923707902431488\n",
            "Epoch 16/2000, Learning Rate: 0.008514577710948757, Loss: 0.4631253778934479\n",
            "Epoch 17/2000, Learning Rate: 0.00842943193383927, Loss: 0.2661631405353546\n",
            "Epoch 18/2000, Learning Rate: 0.008345137614500876, Loss: 0.4845159351825714\n",
            "Epoch 19/2000, Learning Rate: 0.008261686238355867, Loss: 0.43266770243644714\n",
            "Epoch 20/2000, Learning Rate: 0.008179069375972308, Loss: 0.5327926278114319\n",
            "Epoch 21/2000, Learning Rate: 0.008097278682212584, Loss: 0.3143879175186157\n",
            "Epoch 22/2000, Learning Rate: 0.00801630589539046, Loss: 0.24274584650993347\n",
            "Epoch 23/2000, Learning Rate: 0.007936142836436554, Loss: 0.2559420168399811\n",
            "Epoch 24/2000, Learning Rate: 0.007856781408072189, Loss: 0.36042913794517517\n",
            "Epoch 25/2000, Learning Rate: 0.007778213593991467, Loss: 0.18618154525756836\n",
            "Epoch 26/2000, Learning Rate: 0.007700431458051552, Loss: 0.24686172604560852\n",
            "Epoch 27/2000, Learning Rate: 0.007623427143471037, Loss: 0.17167599499225616\n",
            "Epoch 28/2000, Learning Rate: 0.007547192872036326, Loss: 0.17961323261260986\n",
            "Epoch 29/2000, Learning Rate: 0.007471720943315963, Loss: 0.7954258322715759\n",
            "Epoch 30/2000, Learning Rate: 0.007397003733882804, Loss: 0.1886085569858551\n",
            "Epoch 31/2000, Learning Rate: 0.007323033696543976, Loss: 0.35247093439102173\n",
            "Epoch 32/2000, Learning Rate: 0.007249803359578536, Loss: 0.3092470169067383\n",
            "Epoch 33/2000, Learning Rate: 0.0071773053259827505, Loss: 0.3073098063468933\n",
            "Epoch 34/2000, Learning Rate: 0.007105532272722923, Loss: 0.5124225616455078\n",
            "Epoch 35/2000, Learning Rate: 0.007034476949995694, Loss: 0.13665103912353516\n",
            "Epoch 36/2000, Learning Rate: 0.006964132180495737, Loss: 0.140712708234787\n",
            "Epoch 37/2000, Learning Rate: 0.00689449085869078, Loss: 0.2547805607318878\n",
            "Epoch 38/2000, Learning Rate: 0.006825545950103872, Loss: 0.21495117247104645\n",
            "Epoch 39/2000, Learning Rate: 0.0067572904906028335, Loss: 0.7291263341903687\n",
            "Epoch 40/2000, Learning Rate: 0.006689717585696805, Loss: 0.45044761896133423\n",
            "Epoch 41/2000, Learning Rate: 0.006622820409839836, Loss: 0.3168826699256897\n",
            "Epoch 42/2000, Learning Rate: 0.006556592205741438, Loss: 0.30410587787628174\n",
            "Epoch 43/2000, Learning Rate: 0.006491026283684023, Loss: 0.07393328100442886\n",
            "Epoch 44/2000, Learning Rate: 0.006426116020847182, Loss: 0.14275594055652618\n",
            "Epoch 45/2000, Learning Rate: 0.006361854860638711, Loss: 0.1540854275226593\n",
            "Epoch 46/2000, Learning Rate: 0.006298236312032323, Loss: 0.09115829318761826\n",
            "Epoch 47/2000, Learning Rate: 0.006235253948912, Loss: 0.10263627767562866\n",
            "Epoch 48/2000, Learning Rate: 0.00617290140942288, Loss: 0.09933298081159592\n",
            "Epoch 49/2000, Learning Rate: 0.006111172395328651, Loss: 0.10179214179515839\n",
            "Epoch 50/2000, Learning Rate: 0.0060500606713753645, Loss: 0.08256042748689651\n",
            "Epoch 51/2000, Learning Rate: 0.005989560064661611, Loss: 0.49816691875457764\n",
            "Epoch 52/2000, Learning Rate: 0.005929664464014994, Loss: 0.6078556180000305\n",
            "Epoch 53/2000, Learning Rate: 0.0058703678193748445, Loss: 0.203311488032341\n",
            "Epoch 54/2000, Learning Rate: 0.005811664141181096, Loss: 0.23352061212062836\n",
            "Epoch 55/2000, Learning Rate: 0.0057535474997692845, Loss: 0.2954159080982208\n",
            "Epoch 56/2000, Learning Rate: 0.005696012024771591, Loss: 0.11245417594909668\n",
            "Epoch 57/2000, Learning Rate: 0.005639051904523875, Loss: 0.06953160464763641\n",
            "Epoch 58/2000, Learning Rate: 0.005582661385478636, Loss: 0.14914977550506592\n",
            "Epoch 59/2000, Learning Rate: 0.0055268347716238495, Loss: 0.11538493633270264\n",
            "Epoch 60/2000, Learning Rate: 0.005471566423907611, Loss: 0.0869736522436142\n",
            "Epoch 61/2000, Learning Rate: 0.005416850759668535, Loss: 0.3132268190383911\n",
            "Epoch 62/2000, Learning Rate: 0.005362682252071849, Loss: 0.09202075004577637\n",
            "Epoch 63/2000, Learning Rate: 0.005309055429551131, Loss: 0.12058622390031815\n",
            "Epoch 64/2000, Learning Rate: 0.00525596487525562, Loss: 0.25532203912734985\n",
            "Epoch 65/2000, Learning Rate: 0.005203405226503063, Loss: 0.14000272750854492\n",
            "Epoch 66/2000, Learning Rate: 0.005151371174238033, Loss: 0.42193493247032166\n",
            "Epoch 67/2000, Learning Rate: 0.005099857462495652, Loss: 0.3528802692890167\n",
            "Epoch 68/2000, Learning Rate: 0.005048858887870696, Loss: 0.39895668625831604\n",
            "Epoch 69/2000, Learning Rate: 0.004998370298991989, Loss: 0.10252731293439865\n",
            "Epoch 70/2000, Learning Rate: 0.00494838659600207, Loss: 0.19366119801998138\n",
            "Epoch 71/2000, Learning Rate: 0.004898902730042049, Loss: 0.14301565289497375\n",
            "Epoch 72/2000, Learning Rate: 0.004849913702741628, Loss: 0.24393001198768616\n",
            "Epoch 73/2000, Learning Rate: 0.004801414565714212, Loss: 0.15081879496574402\n",
            "Epoch 74/2000, Learning Rate: 0.0047534004200570695, Loss: 0.2702503502368927\n",
            "Epoch 75/2000, Learning Rate: 0.004705866415856499, Loss: 0.3125748634338379\n",
            "Epoch 76/2000, Learning Rate: 0.004658807751697934, Loss: 0.17585836350917816\n",
            "Epoch 77/2000, Learning Rate: 0.004612219674180955, Loss: 0.13998377323150635\n",
            "Epoch 78/2000, Learning Rate: 0.004566097477439145, Loss: 0.22715865075588226\n",
            "Epoch 79/2000, Learning Rate: 0.004520436502664754, Loss: 0.12865203619003296\n",
            "Epoch 80/2000, Learning Rate: 0.004475232137638106, Loss: 0.12444381415843964\n",
            "Epoch 81/2000, Learning Rate: 0.004430479816261725, Loss: 0.15074771642684937\n",
            "Epoch 82/2000, Learning Rate: 0.004386175018099108, Loss: 0.2636044919490814\n",
            "Epoch 83/2000, Learning Rate: 0.004342313267918117, Loss: 0.4197324216365814\n",
            "Epoch 84/2000, Learning Rate: 0.004298890135238936, Loss: 0.3342840373516083\n",
            "Epoch 85/2000, Learning Rate: 0.004255901233886547, Loss: 0.2540534734725952\n",
            "Epoch 86/2000, Learning Rate: 0.004213342221547681, Loss: 0.24618035554885864\n",
            "Epoch 87/2000, Learning Rate: 0.004171208799332205, Loss: 0.19202201068401337\n",
            "Epoch 88/2000, Learning Rate: 0.004129496711338883, Loss: 0.2393951565027237\n",
            "Epoch 89/2000, Learning Rate: 0.004088201744225493, Loss: 0.3785983920097351\n",
            "Epoch 90/2000, Learning Rate: 0.004047319726783238, Loss: 0.08894753456115723\n",
            "Epoch 91/2000, Learning Rate: 0.004006846529515406, Loss: 0.20566175878047943\n",
            "Epoch 92/2000, Learning Rate: 0.003966778064220252, Loss: 0.08258279412984848\n",
            "Epoch 93/2000, Learning Rate: 0.003927110283578049, Loss: 0.19526392221450806\n",
            "Epoch 94/2000, Learning Rate: 0.0038878391807422685, Loss: 0.35043662786483765\n",
            "Epoch 95/2000, Learning Rate: 0.0038489607889348456, Loss: 0.019728148356080055\n",
            "Epoch 96/2000, Learning Rate: 0.0038104711810454973, Loss: 0.14760863780975342\n",
            "Epoch 97/2000, Learning Rate: 0.0037723664692350424, Loss: 0.15212681889533997\n",
            "Epoch 98/2000, Learning Rate: 0.003734642804542692, Loss: 0.36032789945602417\n",
            "Epoch 99/2000, Learning Rate: 0.003697296376497265, Loss: 0.10466193407773972\n",
            "Epoch 100/2000, Learning Rate: 0.0036603234127322924, Loss: 0.16730937361717224\n",
            "Epoch 101/2000, Learning Rate: 0.0036237201786049693, Loss: 0.1412733793258667\n",
            "Epoch 102/2000, Learning Rate: 0.0035874829768189195, Loss: 0.1171380877494812\n",
            "Epoch 103/2000, Learning Rate: 0.0035516081470507305, Loss: 0.09057795256376266\n",
            "Epoch 104/2000, Learning Rate: 0.0035160920655802233, Loss: 0.12954895198345184\n",
            "Epoch 105/2000, Learning Rate: 0.003480931144924421, Loss: 0.1024027094244957\n",
            "Epoch 106/2000, Learning Rate: 0.003446121833475177, Loss: 0.21386772394180298\n",
            "Epoch 107/2000, Learning Rate: 0.003411660615140425, Loss: 0.0794181376695633\n",
            "Epoch 108/2000, Learning Rate: 0.003377544008989021, Loss: 0.052900154143571854\n",
            "Epoch 109/2000, Learning Rate: 0.003343768568899131, Loss: 0.28832465410232544\n",
            "Epoch 110/2000, Learning Rate: 0.00331033088321014, Loss: 0.17647825181484222\n",
            "Epoch 111/2000, Learning Rate: 0.0032772275743780384, Loss: 0.5325356721878052\n",
            "Epoch 112/2000, Learning Rate: 0.0032444552986342578, Loss: 0.19259098172187805\n",
            "Epoch 113/2000, Learning Rate: 0.0032120107456479153, Loss: 0.10100094228982925\n",
            "Epoch 114/2000, Learning Rate: 0.003179890638191436, Loss: 0.12904591858386993\n",
            "Epoch 115/2000, Learning Rate: 0.003148091731809522, Loss: 0.3148145377635956\n",
            "Epoch 116/2000, Learning Rate: 0.0031166108144914267, Loss: 0.09329423308372498\n",
            "Epoch 117/2000, Learning Rate: 0.0030854447063465122, Loss: 0.09224198013544083\n",
            "Epoch 118/2000, Learning Rate: 0.003054590259283047, Loss: 0.15119265019893646\n",
            "Epoch 119/2000, Learning Rate: 0.0030240443566902165, Loss: 0.1945209950208664\n",
            "Epoch 120/2000, Learning Rate: 0.002993803913123314, Loss: 0.1158023402094841\n",
            "Epoch 121/2000, Learning Rate: 0.002963865873992081, Loss: 0.2075619250535965\n",
            "Epoch 122/2000, Learning Rate: 0.00293422721525216, Loss: 0.17474354803562164\n",
            "Epoch 123/2000, Learning Rate: 0.0029048849430996383, Loss: 0.1013685092329979\n",
            "Epoch 124/2000, Learning Rate: 0.002875836093668642, Loss: 0.10450023412704468\n",
            "Epoch 125/2000, Learning Rate: 0.0028470777327319553, Loss: 0.07150650024414062\n",
            "Epoch 126/2000, Learning Rate: 0.0028186069554046356, Loss: 0.2906835079193115\n",
            "Epoch 127/2000, Learning Rate: 0.002790420885850589, Loss: 0.22353847324848175\n",
            "Epoch 128/2000, Learning Rate: 0.002762516676992083, Loss: 0.14787021279335022\n",
            "Epoch 129/2000, Learning Rate: 0.0027348915102221624, Loss: 0.11201406270265579\n",
            "Epoch 130/2000, Learning Rate: 0.0027075425951199406, Loss: 0.1634099930524826\n",
            "Epoch 131/2000, Learning Rate: 0.0026804671691687413, Loss: 0.2184535413980484\n",
            "Epoch 132/2000, Learning Rate: 0.0026536624974770537, Loss: 0.2573894262313843\n",
            "Epoch 133/2000, Learning Rate: 0.002627125872502283, Loss: 0.10874015837907791\n",
            "Epoch 134/2000, Learning Rate: 0.0026008546137772605, Loss: 0.09549500048160553\n",
            "Epoch 135/2000, Learning Rate: 0.0025748460676394878, Loss: 0.10536719113588333\n",
            "Epoch 136/2000, Learning Rate: 0.002549097606963093, Loss: 0.03118119016289711\n",
            "Epoch 137/2000, Learning Rate: 0.002523606630893462, Loss: 0.0788278877735138\n",
            "Epoch 138/2000, Learning Rate: 0.0024983705645845273, Loss: 0.12259294092655182\n",
            "Epoch 139/2000, Learning Rate: 0.002473386858938682, Loss: 0.08869768679141998\n",
            "Epoch 140/2000, Learning Rate: 0.002448652990349295, Loss: 0.0670405700802803\n",
            "Epoch 141/2000, Learning Rate: 0.0024241664604458023, Loss: 0.16123443841934204\n",
            "Epoch 142/2000, Learning Rate: 0.002399924795841344, Loss: 0.5208196640014648\n",
            "Epoch 143/2000, Learning Rate: 0.0023759255478829305, Loss: 0.2303295135498047\n",
            "Epoch 144/2000, Learning Rate: 0.002352166292404101, Loss: 0.2546244263648987\n",
            "Epoch 145/2000, Learning Rate: 0.00232864462948006, Loss: 0.25074559450149536\n",
            "Epoch 146/2000, Learning Rate: 0.0023053581831852595, Loss: 0.18063801527023315\n",
            "Epoch 147/2000, Learning Rate: 0.002282304601353407, Loss: 0.06359698623418808\n",
            "Epoch 148/2000, Learning Rate: 0.002259481555339873, Loss: 0.10665928572416306\n",
            "Epoch 149/2000, Learning Rate: 0.0022368867397864743, Loss: 0.24732182919979095\n",
            "Epoch 150/2000, Learning Rate: 0.0022145178723886093, Loss: 0.06592877209186554\n",
            "Epoch 151/2000, Learning Rate: 0.002192372693664723, Loss: 0.09978140145540237\n",
            "Epoch 152/2000, Learning Rate: 0.002170448966728076, Loss: 0.10637514293193817\n",
            "Epoch 153/2000, Learning Rate: 0.002148744477060795, Loss: 0.023770084604620934\n",
            "Epoch 154/2000, Learning Rate: 0.002127257032290187, Loss: 0.04895196855068207\n",
            "Epoch 155/2000, Learning Rate: 0.002105984461967285, Loss: 0.035470474511384964\n",
            "Epoch 156/2000, Learning Rate: 0.0020849246173476124, Loss: 0.03578776866197586\n",
            "Epoch 157/2000, Learning Rate: 0.0020640753711741364, Loss: 0.13175196945667267\n",
            "Epoch 158/2000, Learning Rate: 0.0020434346174623952, Loss: 0.12427182495594025\n",
            "Epoch 159/2000, Learning Rate: 0.0020230002712877714, Loss: 0.06780602037906647\n",
            "Epoch 160/2000, Learning Rate: 0.0020027702685748937, Loss: 0.12097574770450592\n",
            "Epoch 161/2000, Learning Rate: 0.0019827425658891446, Loss: 0.022786853834986687\n",
            "Epoch 162/2000, Learning Rate: 0.001962915140230253, Loss: 0.05159423500299454\n",
            "Epoch 163/2000, Learning Rate: 0.0019432859888279506, Loss: 0.08456769585609436\n",
            "Epoch 164/2000, Learning Rate: 0.0019238531289396711, Loss: 0.04767241328954697\n",
            "Epoch 165/2000, Learning Rate: 0.0019046145976502743, Loss: 0.01304033026099205\n",
            "Epoch 166/2000, Learning Rate: 0.0018855684516737715, Loss: 0.0481388084590435\n",
            "Epoch 167/2000, Learning Rate: 0.0018667127671570338, Loss: 0.12368851900100708\n",
            "Epoch 168/2000, Learning Rate: 0.0018480456394854635, Loss: 0.04232553020119667\n",
            "Epoch 169/2000, Learning Rate: 0.0018295651830906089, Loss: 0.05387173593044281\n",
            "Epoch 170/2000, Learning Rate: 0.0018112695312597028, Loss: 0.10161269456148148\n",
            "Epoch 171/2000, Learning Rate: 0.0017931568359471058, Loss: 0.12291853874921799\n",
            "Epoch 172/2000, Learning Rate: 0.0017752252675876346, Loss: 0.03725505992770195\n",
            "Epoch 173/2000, Learning Rate: 0.0017574730149117583, Loss: 0.15462079644203186\n",
            "Epoch 174/2000, Learning Rate: 0.0017398982847626407, Loss: 0.11163181066513062\n",
            "Epoch 175/2000, Learning Rate: 0.0017224993019150142, Loss: 0.07091826945543289\n",
            "Epoch 176/2000, Learning Rate: 0.001705274308895864, Loss: 0.10914481431245804\n",
            "Epoch 177/2000, Learning Rate: 0.0016882215658069054, Loss: 0.028880398720502853\n",
            "Epoch 178/2000, Learning Rate: 0.0016713393501488363, Loss: 0.023958351463079453\n",
            "Epoch 179/2000, Learning Rate: 0.001654625956647348, Loss: 0.172530859708786\n",
            "Epoch 180/2000, Learning Rate: 0.0016380796970808745, Loss: 0.0566047839820385\n",
            "Epoch 181/2000, Learning Rate: 0.0016216989001100659, Loss: 0.06745365262031555\n",
            "Epoch 182/2000, Learning Rate: 0.0016054819111089651, Loss: 0.04959553852677345\n",
            "Epoch 183/2000, Learning Rate: 0.0015894270919978755, Loss: 0.04575011134147644\n",
            "Epoch 184/2000, Learning Rate: 0.0015735328210778967, Loss: 0.14950339496135712\n",
            "Epoch 185/2000, Learning Rate: 0.0015577974928671179, Loss: 0.04962293803691864\n",
            "Epoch 186/2000, Learning Rate: 0.0015422195179384467, Loss: 0.08196146786212921\n",
            "Epoch 187/2000, Learning Rate: 0.0015267973227590622, Loss: 0.029807183891534805\n",
            "Epoch 188/2000, Learning Rate: 0.0015115293495314716, Loss: 0.049762289971113205\n",
            "Epoch 189/2000, Learning Rate: 0.001496414056036157, Loss: 0.32857778668403625\n",
            "Epoch 190/2000, Learning Rate: 0.0014814499154757953, Loss: 0.051056671887636185\n",
            "Epoch 191/2000, Learning Rate: 0.0014666354163210373, Loss: 0.05657832697033882\n",
            "Epoch 192/2000, Learning Rate: 0.0014519690621578268, Loss: 0.01713429018855095\n",
            "Epoch 193/2000, Learning Rate: 0.0014374493715362485, Loss: 0.030043920502066612\n",
            "Epoch 194/2000, Learning Rate: 0.001423074877820886, Loss: 0.044454652816057205\n",
            "Epoch 195/2000, Learning Rate: 0.0014088441290426772, Loss: 0.02800700068473816\n",
            "Epoch 196/2000, Learning Rate: 0.0013947556877522505, Loss: 0.1936148703098297\n",
            "Epoch 197/2000, Learning Rate: 0.001380808130874728, Loss: 0.07876143604516983\n",
            "Epoch 198/2000, Learning Rate: 0.0013670000495659808, Loss: 0.17924818396568298\n",
            "Epoch 199/2000, Learning Rate: 0.001353330049070321, Loss: 0.08164821565151215\n",
            "Epoch 200/2000, Learning Rate: 0.0013397967485796176, Loss: 0.03867771103978157\n",
            "Epoch 201/2000, Learning Rate: 0.0013263987810938215, Loss: 0.17863941192626953\n",
            "Epoch 202/2000, Learning Rate: 0.0013131347932828833, Loss: 0.11316286027431488\n",
            "Epoch 203/2000, Learning Rate: 0.0013000034453500545, Loss: 0.06711152195930481\n",
            "Epoch 204/2000, Learning Rate: 0.0012870034108965539, Loss: 0.1498166024684906\n",
            "Epoch 205/2000, Learning Rate: 0.0012741333767875883, Loss: 0.10692537575960159\n",
            "Epoch 206/2000, Learning Rate: 0.0012613920430197124, Loss: 0.13441826403141022\n",
            "Epoch 207/2000, Learning Rate: 0.0012487781225895152, Loss: 0.11452458798885345\n",
            "Epoch 208/2000, Learning Rate: 0.00123629034136362, Loss: 0.09241876751184464\n",
            "Epoch 209/2000, Learning Rate: 0.0012239274379499839, Loss: 0.0789576843380928\n",
            "Epoch 210/2000, Learning Rate: 0.001211688163570484, Loss: 0.06005735322833061\n",
            "Epoch 211/2000, Learning Rate: 0.001199571281934779, Loss: 0.1069505587220192\n",
            "Epoch 212/2000, Learning Rate: 0.0011875755691154314, Loss: 0.05886589363217354\n",
            "Epoch 213/2000, Learning Rate: 0.001175699813424277, Loss: 0.04108656570315361\n",
            "Epoch 214/2000, Learning Rate: 0.0011639428152900343, Loss: 0.09185320138931274\n",
            "Epoch 215/2000, Learning Rate: 0.001152303387137134, Loss: 0.13643360137939453\n",
            "Epoch 216/2000, Learning Rate: 0.0011407803532657627, Loss: 0.007189526688307524\n",
            "Epoch 217/2000, Learning Rate: 0.001129372549733105, Loss: 0.14982928335666656\n",
            "Epoch 218/2000, Learning Rate: 0.0011180788242357738, Loss: 0.09255503118038177\n",
            "Epoch 219/2000, Learning Rate: 0.001106898035993416, Loss: 0.012082982808351517\n",
            "Epoch 220/2000, Learning Rate: 0.0010958290556334817, Loss: 0.052757468074560165\n",
            "Epoch 221/2000, Learning Rate: 0.001084870765077147, Loss: 0.09999626874923706\n",
            "Epoch 222/2000, Learning Rate: 0.0010740220574263755, Loss: 0.10121755301952362\n",
            "Epoch 223/2000, Learning Rate: 0.0010632818368521118, Loss: 0.17900118231773376\n",
            "Epoch 224/2000, Learning Rate: 0.0010526490184835906, Loss: 0.027915677055716515\n",
            "Epoch 225/2000, Learning Rate: 0.0010421225282987547, Loss: 0.11463383585214615\n",
            "Epoch 226/2000, Learning Rate: 0.001031701303015767, Loss: 0.049367621541023254\n",
            "Epoch 227/2000, Learning Rate: 0.0010213842899856095, Loss: 0.12260448932647705\n",
            "Epoch 228/2000, Learning Rate: 0.0010111704470857534, Loss: 0.05467018857598305\n",
            "Epoch 229/2000, Learning Rate: 0.001001058742614896, Loss: 0.036104146391153336\n",
            "Epoch 230/2000, Learning Rate: 0.000991048155188747, Loss: 0.1231040507555008\n",
            "Epoch 231/2000, Learning Rate: 0.0009811376736368596, Loss: 0.024037253111600876\n",
            "Epoch 232/2000, Learning Rate: 0.000971326296900491, Loss: 0.28106755018234253\n",
            "Epoch 233/2000, Learning Rate: 0.000961613033931486, Loss: 0.21755561232566833\n",
            "Epoch 234/2000, Learning Rate: 0.0009519969035921711, Loss: 0.0737619400024414\n",
            "Epoch 235/2000, Learning Rate: 0.0009424769345562494, Loss: 0.07406134158372879\n",
            "Epoch 236/2000, Learning Rate: 0.0009330521652106869, Loss: 0.03143155202269554\n",
            "Epoch 237/2000, Learning Rate: 0.00092372164355858, Loss: 0.03744220361113548\n",
            "Epoch 238/2000, Learning Rate: 0.0009144844271229942, Loss: 0.206197127699852\n",
            "Epoch 239/2000, Learning Rate: 0.0009053395828517643, Loss: 0.07976068556308746\n",
            "Epoch 240/2000, Learning Rate: 0.0008962861870232467, Loss: 0.015433192253112793\n",
            "Epoch 241/2000, Learning Rate: 0.0008873233251530142, Loss: 0.09097698330879211\n",
            "Epoch 242/2000, Learning Rate: 0.0008784500919014841, Loss: 0.18485261499881744\n",
            "Epoch 243/2000, Learning Rate: 0.0008696655909824692, Loss: 0.1128300279378891\n",
            "Epoch 244/2000, Learning Rate: 0.0008609689350726445, Loss: 0.05812611058354378\n",
            "Epoch 245/2000, Learning Rate: 0.000852359245721918, Loss: 0.022498715668916702\n",
            "Epoch 246/2000, Learning Rate: 0.0008438356532646988, Loss: 0.015367966145277023\n",
            "Epoch 247/2000, Learning Rate: 0.0008353972967320518, Loss: 0.033235982060432434\n",
            "Epoch 248/2000, Learning Rate: 0.0008270433237647312, Loss: 0.09239573776721954\n",
            "Epoch 249/2000, Learning Rate: 0.0008187728905270839, Loss: 0.2550649046897888\n",
            "Epoch 250/2000, Learning Rate: 0.000810585161621813, Loss: 0.04555552452802658\n",
            "Epoch 251/2000, Learning Rate: 0.0008024793100055949, Loss: 0.023719456046819687\n",
            "Epoch 252/2000, Learning Rate: 0.000794454516905539, Loss: 0.16600461304187775\n",
            "Epoch 253/2000, Learning Rate: 0.0007865099717364836, Loss: 0.02651372365653515\n",
            "Epoch 254/2000, Learning Rate: 0.0007786448720191188, Loss: 0.015087568201124668\n",
            "Epoch 255/2000, Learning Rate: 0.0007708584232989276, Loss: 0.11157199740409851\n",
            "Epoch 256/2000, Learning Rate: 0.0007631498390659383, Loss: 0.20782750844955444\n",
            "Epoch 257/2000, Learning Rate: 0.000755518340675279, Loss: 0.03109542280435562\n",
            "Epoch 258/2000, Learning Rate: 0.0007479631572685261, Loss: 0.025651708245277405\n",
            "Epoch 259/2000, Learning Rate: 0.0007404835256958409, Loss: 0.44510510563850403\n",
            "Epoch 260/2000, Learning Rate: 0.0007330786904388825, Loss: 0.22249968349933624\n",
            "Epoch 261/2000, Learning Rate: 0.0007257479035344936, Loss: 0.02837378717958927\n",
            "Epoch 262/2000, Learning Rate: 0.0007184904244991487, Loss: 0.03123481199145317\n",
            "Epoch 263/2000, Learning Rate: 0.0007113055202541572, Loss: 0.1467185616493225\n",
            "Epoch 264/2000, Learning Rate: 0.0007041924650516157, Loss: 0.047794852405786514\n",
            "Epoch 265/2000, Learning Rate: 0.0006971505404010995, Loss: 0.2214599996805191\n",
            "Epoch 266/2000, Learning Rate: 0.0006901790349970885, Loss: 0.010237949900329113\n",
            "Epoch 267/2000, Learning Rate: 0.0006832772446471176, Loss: 0.05225813016295433\n",
            "Epoch 268/2000, Learning Rate: 0.0006764444722006464, Loss: 0.11035730689764023\n",
            "Epoch 269/2000, Learning Rate: 0.00066968002747864, Loss: 0.057375069707632065\n",
            "Epoch 270/2000, Learning Rate: 0.0006629832272038536, Loss: 0.06910393387079239\n",
            "Epoch 271/2000, Learning Rate: 0.000656353394931815, Loss: 0.06663019210100174\n",
            "Epoch 272/2000, Learning Rate: 0.0006497898609824969, Loss: 0.06003936380147934\n",
            "Epoch 273/2000, Learning Rate: 0.0006432919623726719, Loss: 0.10176955163478851\n",
            "Epoch 274/2000, Learning Rate: 0.0006368590427489453, Loss: 0.020863492041826248\n",
            "Epoch 275/2000, Learning Rate: 0.0006304904523214558, Loss: 0.128177672624588\n",
            "Epoch 276/2000, Learning Rate: 0.0006241855477982413, Loss: 0.06536771357059479\n",
            "Epoch 277/2000, Learning Rate: 0.0006179436923202589, Loss: 0.10128766298294067\n",
            "Epoch 278/2000, Learning Rate: 0.0006117642553970563, Loss: 0.07147244364023209\n",
            "Epoch 279/2000, Learning Rate: 0.0006056466128430857, Loss: 0.017151866108179092\n",
            "Epoch 280/2000, Learning Rate: 0.0005995901467146549, Loss: 0.06832890212535858\n",
            "Epoch 281/2000, Learning Rate: 0.0005935942452475083, Loss: 0.07779647409915924\n",
            "Epoch 282/2000, Learning Rate: 0.0005876583027950332, Loss: 0.09334607422351837\n",
            "Epoch 283/2000, Learning Rate: 0.0005817817197670828, Loss: 0.03672577440738678\n",
            "Epoch 284/2000, Learning Rate: 0.000575963902569412, Loss: 0.13575908541679382\n",
            "Epoch 285/2000, Learning Rate: 0.0005702042635437178, Loss: 0.14248450100421906\n",
            "Epoch 286/2000, Learning Rate: 0.0005645022209082806, Loss: 0.1962396204471588\n",
            "Epoch 287/2000, Learning Rate: 0.0005588571986991978, Loss: 0.1754567176103592\n",
            "Epoch 288/2000, Learning Rate: 0.0005532686267122058, Loss: 0.13582661747932434\n",
            "Epoch 289/2000, Learning Rate: 0.0005477359404450837, Loss: 0.02494584396481514\n",
            "Epoch 290/2000, Learning Rate: 0.0005422585810406329, Loss: 0.1503453105688095\n",
            "Epoch 291/2000, Learning Rate: 0.0005368359952302266, Loss: 0.08756986260414124\n",
            "Epoch 292/2000, Learning Rate: 0.0005314676352779243, Loss: 0.03943215683102608\n",
            "Epoch 293/2000, Learning Rate: 0.0005261529589251451, Loss: 0.03627225384116173\n",
            "Epoch 294/2000, Learning Rate: 0.0005208914293358937, Loss: 0.13265402615070343\n",
            "Epoch 295/2000, Learning Rate: 0.0005156825150425347, Loss: 0.008263267576694489\n",
            "Epoch 296/2000, Learning Rate: 0.0005105256898921093, Loss: 0.048727620393037796\n",
            "Epoch 297/2000, Learning Rate: 0.0005054204329931882, Loss: 0.16020897030830383\n",
            "Epoch 298/2000, Learning Rate: 0.0005003662286632563, Loss: 0.2257009893655777\n",
            "Epoch 299/2000, Learning Rate: 0.0004953625663766237, Loss: 0.0345347635447979\n",
            "Epoch 300/2000, Learning Rate: 0.0004904089407128574, Loss: 0.023983867838978767\n",
            "Epoch 301/2000, Learning Rate: 0.00048550485130572886, Loss: 0.058809712529182434\n",
            "Epoch 302/2000, Learning Rate: 0.00048064980279267157, Loss: 0.008464979939162731\n",
            "Epoch 303/2000, Learning Rate: 0.00047584330476474484, Loss: 0.004630567505955696\n",
            "Epoch 304/2000, Learning Rate: 0.0004710848717170974, Loss: 0.07486949861049652\n",
            "Epoch 305/2000, Learning Rate: 0.00046637402299992643, Loss: 0.15887269377708435\n",
            "Epoch 306/2000, Learning Rate: 0.00046171028276992715, Loss: 0.13662225008010864\n",
            "Epoch 307/2000, Learning Rate: 0.0004570931799422279, Loss: 0.051107194274663925\n",
            "Epoch 308/2000, Learning Rate: 0.0004525222481428056, Loss: 0.34662511944770813\n",
            "Epoch 309/2000, Learning Rate: 0.00044799702566137754, Loss: 0.26270490884780884\n",
            "Epoch 310/2000, Learning Rate: 0.0004435170554047638, Loss: 0.15157032012939453\n",
            "Epoch 311/2000, Learning Rate: 0.00043908188485071616, Loss: 0.16029565036296844\n",
            "Epoch 312/2000, Learning Rate: 0.000434691066002209, Loss: 0.05508194863796234\n",
            "Epoch 313/2000, Learning Rate: 0.0004303441553421869, Loss: 0.11938466131687164\n",
            "Epoch 314/2000, Learning Rate: 0.00042604071378876505, Loss: 0.08170780539512634\n",
            "Epoch 315/2000, Learning Rate: 0.0004217803066508774, Loss: 0.1167268380522728\n",
            "Epoch 316/2000, Learning Rate: 0.0004175625035843686, Loss: 0.019349437206983566\n",
            "Epoch 317/2000, Learning Rate: 0.0004133868785485249, Loss: 0.027651404961943626\n",
            "Epoch 318/2000, Learning Rate: 0.00040925300976303965, Loss: 0.02723907306790352\n",
            "Epoch 319/2000, Learning Rate: 0.00040516047966540923, Loss: 0.15731537342071533\n",
            "Epoch 320/2000, Learning Rate: 0.00040110887486875514, Loss: 0.1046474277973175\n",
            "Epoch 321/2000, Learning Rate: 0.0003970977861200676, Loss: 0.08776384592056274\n",
            "Epoch 322/2000, Learning Rate: 0.00039312680825886694, Loss: 0.09664440155029297\n",
            "Epoch 323/2000, Learning Rate: 0.0003891955401762783, Loss: 0.004695937503129244\n",
            "Epoch 324/2000, Learning Rate: 0.00038530358477451547, Loss: 0.027927251532673836\n",
            "Epoch 325/2000, Learning Rate: 0.00038145054892677033, Loss: 0.02042427286505699\n",
            "Epoch 326/2000, Learning Rate: 0.0003776360434375026, Loss: 0.11505360901355743\n",
            "Epoch 327/2000, Learning Rate: 0.00037385968300312757, Loss: 0.05055439472198486\n",
            "Epoch 328/2000, Learning Rate: 0.0003701210861730963, Loss: 0.06286179274320602\n",
            "Epoch 329/2000, Learning Rate: 0.0003664198753113653, Loss: 0.28719767928123474\n",
            "Epoch 330/2000, Learning Rate: 0.0003627556765582517, Loss: 0.13315285742282867\n",
            "Epoch 331/2000, Learning Rate: 0.0003591281197926692, Loss: 0.025954855605959892\n",
            "Epoch 332/2000, Learning Rate: 0.00035553683859474247, Loss: 0.06101515144109726\n",
            "Epoch 333/2000, Learning Rate: 0.00035198147020879504, Loss: 0.0797412320971489\n",
            "Epoch 334/2000, Learning Rate: 0.00034846165550670707, Loss: 0.004046018701046705\n",
            "Epoch 335/2000, Learning Rate: 0.00034497703895164, Loss: 0.012154736556112766\n",
            "Epoch 336/2000, Learning Rate: 0.0003415272685621236, Loss: 0.0417468287050724\n",
            "Epoch 337/2000, Learning Rate: 0.00033811199587650235, Loss: 0.016059130430221558\n",
            "Epoch 338/2000, Learning Rate: 0.0003347308759177373, Loss: 0.18429447710514069\n",
            "Epoch 339/2000, Learning Rate: 0.00033138356715855995, Loss: 0.05912773311138153\n",
            "Epoch 340/2000, Learning Rate: 0.00032806973148697435, Loss: 0.030581701546907425\n",
            "Epoch 341/2000, Learning Rate: 0.0003247890341721046, Loss: 0.09523288160562515\n",
            "Epoch 342/2000, Learning Rate: 0.0003215411438303836, Loss: 0.11602810770273209\n",
            "Epoch 343/2000, Learning Rate: 0.00031832573239207976, Loss: 0.04170960560441017\n",
            "Epoch 344/2000, Learning Rate: 0.00031514247506815894, Loss: 0.013204939663410187\n",
            "Epoch 345/2000, Learning Rate: 0.00031199105031747736, Loss: 0.02344468981027603\n",
            "Epoch 346/2000, Learning Rate: 0.0003088711398143026, Loss: 0.04056223854422569\n",
            "Epoch 347/2000, Learning Rate: 0.0003057824284161596, Loss: 0.00914791040122509\n",
            "Epoch 348/2000, Learning Rate: 0.000302724604131998, Loss: 0.0401250496506691\n",
            "Epoch 349/2000, Learning Rate: 0.000299697358090678, Loss: 0.015289770439267159\n",
            "Epoch 350/2000, Learning Rate: 0.00029670038450977125, Loss: 0.06509020924568176\n",
            "Epoch 351/2000, Learning Rate: 0.0002937333806646735, Loss: 0.027747634798288345\n",
            "Epoch 352/2000, Learning Rate: 0.0002907960468580268, Loss: 0.1958429515361786\n",
            "Epoch 353/2000, Learning Rate: 0.0002878880863894465, Loss: 0.14967027306556702\n",
            "Epoch 354/2000, Learning Rate: 0.00028500920552555206, Loss: 0.1078539714217186\n",
            "Epoch 355/2000, Learning Rate: 0.00028215911347029656, Loss: 0.049906566739082336\n",
            "Epoch 356/2000, Learning Rate: 0.0002793375223355936, Loss: 0.1207638680934906\n",
            "Epoch 357/2000, Learning Rate: 0.0002765441471122377, Loss: 0.10168541967868805\n",
            "Epoch 358/2000, Learning Rate: 0.00027377870564111533, Loss: 0.020269183441996574\n",
            "Epoch 359/2000, Learning Rate: 0.0002710409185847042, Loss: 0.021389106288552284\n",
            "Epoch 360/2000, Learning Rate: 0.00026833050939885714, Loss: 0.14506468176841736\n",
            "Epoch 361/2000, Learning Rate: 0.0002656472043048686, Loss: 0.19244442880153656\n",
            "Epoch 362/2000, Learning Rate: 0.0002629907322618199, Loss: 0.1020645946264267\n",
            "Epoch 363/2000, Learning Rate: 0.0002603608249392017, Loss: 0.009481658227741718\n",
            "Epoch 364/2000, Learning Rate: 0.00025775721668980966, Loss: 0.07907742261886597\n",
            "Epoch 365/2000, Learning Rate: 0.00025517964452291157, Loss: 0.03580527380108833\n",
            "Epoch 366/2000, Learning Rate: 0.00025262784807768247, Loss: 0.03100075200200081\n",
            "Epoch 367/2000, Learning Rate: 0.00025010156959690564, Loss: 0.06146276742219925\n",
            "Epoch 368/2000, Learning Rate: 0.00024760055390093657, Loss: 0.19711679220199585\n",
            "Epoch 369/2000, Learning Rate: 0.0002451245483619272, Loss: 0.17910102009773254\n",
            "Epoch 370/2000, Learning Rate: 0.00024267330287830794, Loss: 0.07057003676891327\n",
            "Epoch 371/2000, Learning Rate: 0.00024024656984952486, Loss: 0.10686282068490982\n",
            "Epoch 372/2000, Learning Rate: 0.00023784410415102962, Loss: 0.19130417704582214\n",
            "Epoch 373/2000, Learning Rate: 0.00023546566310951933, Loss: 0.06765775382518768\n",
            "Epoch 374/2000, Learning Rate: 0.00023311100647842415, Loss: 0.04479200020432472\n",
            "Epoch 375/2000, Learning Rate: 0.0002307798964136399, Loss: 0.0710568055510521\n",
            "Epoch 376/2000, Learning Rate: 0.0002284720974495035, Loss: 0.011024639010429382\n",
            "Epoch 377/2000, Learning Rate: 0.00022618737647500848, Loss: 0.1106528788805008\n",
            "Epoch 378/2000, Learning Rate: 0.00022392550271025838, Loss: 0.2315489649772644\n",
            "Epoch 379/2000, Learning Rate: 0.0002216862476831558, Loss: 0.182937890291214\n",
            "Epoch 380/2000, Learning Rate: 0.00021946938520632424, Loss: 0.1871296465396881\n",
            "Epoch 381/2000, Learning Rate: 0.000217274691354261, Loss: 0.024655453860759735\n",
            "Epoch 382/2000, Learning Rate: 0.00021510194444071838, Loss: 0.024346642196178436\n",
            "Epoch 383/2000, Learning Rate: 0.00021295092499631118, Loss: 0.1267545372247696\n",
            "Epoch 384/2000, Learning Rate: 0.00021082141574634808, Loss: 0.02056085877120495\n",
            "Epoch 385/2000, Learning Rate: 0.0002087132015888846, Loss: 0.006894746329635382\n",
            "Epoch 386/2000, Learning Rate: 0.00020662606957299576, Loss: 0.10869407653808594\n",
            "Epoch 387/2000, Learning Rate: 0.0002045598088772658, Loss: 0.09523793309926987\n",
            "Epoch 388/2000, Learning Rate: 0.00020251421078849315, Loss: 0.028636787086725235\n",
            "Epoch 389/2000, Learning Rate: 0.00020048906868060822, Loss: 0.03488808497786522\n",
            "Epoch 390/2000, Learning Rate: 0.00019848417799380214, Loss: 0.0675969198346138\n",
            "Epoch 391/2000, Learning Rate: 0.0001964993362138641, Loss: 0.06410426646471024\n",
            "Epoch 392/2000, Learning Rate: 0.00019453434285172548, Loss: 0.050496045500040054\n",
            "Epoch 393/2000, Learning Rate: 0.00019258899942320822, Loss: 0.053894344717264175\n",
            "Epoch 394/2000, Learning Rate: 0.00019066310942897615, Loss: 0.021332966163754463\n",
            "Epoch 395/2000, Learning Rate: 0.0001887564783346864, Loss: 0.026932314038276672\n",
            "Epoch 396/2000, Learning Rate: 0.00018686891355133953, Loss: 0.06489389389753342\n",
            "Epoch 397/2000, Learning Rate: 0.00018500022441582614, Loss: 0.19254878163337708\n",
            "Epoch 398/2000, Learning Rate: 0.00018315022217166787, Loss: 0.04089238494634628\n",
            "Epoch 399/2000, Learning Rate: 0.0001813187199499512, Loss: 0.007243121974170208\n",
            "Epoch 400/2000, Learning Rate: 0.0001795055327504517, Loss: 0.0751342698931694\n",
            "Epoch 401/2000, Learning Rate: 0.00017771047742294718, Loss: 0.04264489561319351\n",
            "Epoch 402/2000, Learning Rate: 0.0001759333726487177, Loss: 0.02343934215605259\n",
            "Epoch 403/2000, Learning Rate: 0.00017417403892223054, Loss: 0.0745345801115036\n",
            "Epoch 404/2000, Learning Rate: 0.00017243229853300824, Loss: 0.03581269830465317\n",
            "Epoch 405/2000, Learning Rate: 0.00017070797554767815, Loss: 0.0831223726272583\n",
            "Epoch 406/2000, Learning Rate: 0.00016900089579220136, Loss: 0.10688192397356033\n",
            "Epoch 407/2000, Learning Rate: 0.00016731088683427935, Loss: 0.07473593950271606\n",
            "Epoch 408/2000, Learning Rate: 0.00016563777796593654, Loss: 0.0634792372584343\n",
            "Epoch 409/2000, Learning Rate: 0.00016398140018627718, Loss: 0.02066628448665142\n",
            "Epoch 410/2000, Learning Rate: 0.0001623415861844144, Loss: 0.14275938272476196\n",
            "Epoch 411/2000, Learning Rate: 0.00016071817032257027, Loss: 0.03629976883530617\n",
            "Epoch 412/2000, Learning Rate: 0.00015911098861934458, Loss: 0.22550742328166962\n",
            "Epoch 413/2000, Learning Rate: 0.00015751987873315114, Loss: 0.029287414625287056\n",
            "Epoch 414/2000, Learning Rate: 0.00015594467994581963, Loss: 0.0621463768184185\n",
            "Epoch 415/2000, Learning Rate: 0.00015438523314636142, Loss: 0.0328042209148407\n",
            "Epoch 416/2000, Learning Rate: 0.0001528413808148978, Loss: 0.08194670081138611\n",
            "Epoch 417/2000, Learning Rate: 0.00015131296700674884, Loss: 0.03149446099996567\n",
            "Epoch 418/2000, Learning Rate: 0.00014979983733668135, Loss: 0.21495594084262848\n",
            "Epoch 419/2000, Learning Rate: 0.00014830183896331453, Loss: 0.18458180129528046\n",
            "Epoch 420/2000, Learning Rate: 0.00014681882057368138, Loss: 0.0795145258307457\n",
            "Epoch 421/2000, Learning Rate: 0.00014535063236794456, Loss: 0.1949123591184616\n",
            "Epoch 422/2000, Learning Rate: 0.00014389712604426512, Loss: 0.011623730883002281\n",
            "Epoch 423/2000, Learning Rate: 0.00014245815478382246, Loss: 0.00533644063398242\n",
            "Epoch 424/2000, Learning Rate: 0.00014103357323598422, Loss: 0.052104417234659195\n",
            "Epoch 425/2000, Learning Rate: 0.00013962323750362437, Loss: 0.05141535401344299\n",
            "Epoch 426/2000, Learning Rate: 0.0001382270051285881, Loss: 0.019947178661823273\n",
            "Epoch 427/2000, Learning Rate: 0.00013684473507730223, Loss: 0.027213990688323975\n",
            "Epoch 428/2000, Learning Rate: 0.0001354762877265292, Loss: 0.05331502482295036\n",
            "Epoch 429/2000, Learning Rate: 0.00013412152484926392, Loss: 0.09671501070261002\n",
            "Epoch 430/2000, Learning Rate: 0.00013278030960077128, Loss: 0.06371811032295227\n",
            "Epoch 431/2000, Learning Rate: 0.00013145250650476356, Loss: 0.0037279436364769936\n",
            "Epoch 432/2000, Learning Rate: 0.00013013798143971593, Loss: 0.0666680708527565\n",
            "Epoch 433/2000, Learning Rate: 0.00012883660162531878, Loss: 0.1315804421901703\n",
            "Epoch 434/2000, Learning Rate: 0.00012754823560906558, Loss: 0.3129715621471405\n",
            "Epoch 435/2000, Learning Rate: 0.00012627275325297491, Loss: 0.20072539150714874\n",
            "Epoch 436/2000, Learning Rate: 0.00012501002572044517, Loss: 0.49330270290374756\n",
            "Epoch 437/2000, Learning Rate: 0.00012375992546324072, Loss: 0.18618470430374146\n",
            "Epoch 438/2000, Learning Rate: 0.00012252232620860832, Loss: 0.00606243871152401\n",
            "Epoch 439/2000, Learning Rate: 0.00012129710294652224, Loss: 0.10240442305803299\n",
            "Epoch 440/2000, Learning Rate: 0.00012008413191705701, Loss: 0.010264716111123562\n",
            "Epoch 441/2000, Learning Rate: 0.00011888329059788645, Loss: 0.04005550593137741\n",
            "Epoch 442/2000, Learning Rate: 0.00011769445769190758, Loss: 0.37571367621421814\n",
            "Epoch 443/2000, Learning Rate: 0.00011651751311498851, Loss: 0.02921016700565815\n",
            "Epoch 444/2000, Learning Rate: 0.00011535233798383862, Loss: 0.06956665962934494\n",
            "Epoch 445/2000, Learning Rate: 0.00011419881460400023, Loss: 0.032413631677627563\n",
            "Epoch 446/2000, Learning Rate: 0.00011305682645796022, Loss: 0.020151637494564056\n",
            "Epoch 447/2000, Learning Rate: 0.00011192625819338061, Loss: 0.13470229506492615\n",
            "Epoch 448/2000, Learning Rate: 0.00011080699561144681, Loss: 0.02471993863582611\n",
            "Epoch 449/2000, Learning Rate: 0.00010969892565533234, Loss: 0.056982867419719696\n",
            "Epoch 450/2000, Learning Rate: 0.00010860193639877902, Loss: 0.0685216411948204\n",
            "Epoch 451/2000, Learning Rate: 0.00010751591703479123, Loss: 0.06304600089788437\n",
            "Epoch 452/2000, Learning Rate: 0.00010644075786444332, Loss: 0.07547210156917572\n",
            "Epoch 453/2000, Learning Rate: 0.00010537635028579889, Loss: 0.014335858635604382\n",
            "Epoch 454/2000, Learning Rate: 0.0001043225867829409, Loss: 0.025750374421477318\n",
            "Epoch 455/2000, Learning Rate: 0.00010327936091511149, Loss: 0.04580427706241608\n",
            "Epoch 456/2000, Learning Rate: 0.00010224656730596037, Loss: 0.10152542591094971\n",
            "Epoch 457/2000, Learning Rate: 0.00010122410163290076, Loss: 0.061018817126750946\n",
            "Epoch 458/2000, Learning Rate: 0.00010021186061657176, Loss: 0.034432925283908844\n",
            "Epoch 459/2000, Learning Rate: 9.920974201040605e-05, Loss: 0.21306200325489044\n",
            "Epoch 460/2000, Learning Rate: 9.821764459030198e-05, Loss: 0.017081353813409805\n",
            "Epoch 461/2000, Learning Rate: 9.723546814439895e-05, Loss: 0.012300217524170876\n",
            "Epoch 462/2000, Learning Rate: 9.626311346295496e-05, Loss: 0.023118652403354645\n",
            "Epoch 463/2000, Learning Rate: 9.53004823283254e-05, Loss: 0.026536328718066216\n",
            "Epoch 464/2000, Learning Rate: 9.434747750504215e-05, Loss: 0.047859836369752884\n",
            "Epoch 465/2000, Learning Rate: 9.340400272999173e-05, Loss: 0.3567649722099304\n",
            "Epoch 466/2000, Learning Rate: 9.246996270269181e-05, Loss: 0.03709724172949791\n",
            "Epoch 467/2000, Learning Rate: 9.15452630756649e-05, Loss: 0.028735296800732613\n",
            "Epoch 468/2000, Learning Rate: 9.062981044490825e-05, Loss: 0.043205372989177704\n",
            "Epoch 469/2000, Learning Rate: 8.972351234045917e-05, Loss: 0.04136411100625992\n",
            "Epoch 470/2000, Learning Rate: 8.882627721705458e-05, Loss: 0.025146689265966415\n",
            "Epoch 471/2000, Learning Rate: 8.793801444488403e-05, Loss: 0.029305674135684967\n",
            "Epoch 472/2000, Learning Rate: 8.70586343004352e-05, Loss: 0.16063806414604187\n",
            "Epoch 473/2000, Learning Rate: 8.618804795743084e-05, Loss: 0.025944575667381287\n",
            "Epoch 474/2000, Learning Rate: 8.532616747785654e-05, Loss: 0.0691160187125206\n",
            "Epoch 475/2000, Learning Rate: 8.447290580307797e-05, Loss: 0.01927686110138893\n",
            "Epoch 476/2000, Learning Rate: 8.362817674504719e-05, Loss: 0.013437374494969845\n",
            "Epoch 477/2000, Learning Rate: 8.279189497759673e-05, Loss: 0.05218157172203064\n",
            "Epoch 478/2000, Learning Rate: 8.196397602782075e-05, Loss: 0.02466762438416481\n",
            "Epoch 479/2000, Learning Rate: 8.114433626754255e-05, Loss: 0.20733562111854553\n",
            "Epoch 480/2000, Learning Rate: 8.033289290486713e-05, Loss: 0.06753760576248169\n",
            "Epoch 481/2000, Learning Rate: 7.952956397581846e-05, Loss: 0.018381766974925995\n",
            "Epoch 482/2000, Learning Rate: 7.873426833606028e-05, Loss: 0.09303970634937286\n",
            "Epoch 483/2000, Learning Rate: 7.794692565269967e-05, Loss: 0.22095072269439697\n",
            "Epoch 484/2000, Learning Rate: 7.716745639617267e-05, Loss: 0.0866466835141182\n",
            "Epoch 485/2000, Learning Rate: 7.639578183221094e-05, Loss: 0.022774875164031982\n",
            "Epoch 486/2000, Learning Rate: 7.563182401388883e-05, Loss: 0.08953110128641129\n",
            "Epoch 487/2000, Learning Rate: 7.487550577374994e-05, Loss: 0.12022370100021362\n",
            "Epoch 488/2000, Learning Rate: 7.412675071601244e-05, Loss: 0.05603015050292015\n",
            "Epoch 489/2000, Learning Rate: 7.338548320885231e-05, Loss: 0.08250255137681961\n",
            "Epoch 490/2000, Learning Rate: 7.26516283767638e-05, Loss: 0.07127945870161057\n",
            "Epoch 491/2000, Learning Rate: 7.192511209299615e-05, Loss: 0.008693171665072441\n",
            "Epoch 492/2000, Learning Rate: 7.120586097206619e-05, Loss: 0.013197300024330616\n",
            "Epoch 493/2000, Learning Rate: 7.049380236234553e-05, Loss: 0.2510741651058197\n",
            "Epoch 494/2000, Learning Rate: 6.978886433872207e-05, Loss: 0.09425342082977295\n",
            "Epoch 495/2000, Learning Rate: 6.909097569533485e-05, Loss: 0.009368130005896091\n",
            "Epoch 496/2000, Learning Rate: 6.84000659383815e-05, Loss: 0.2076493203639984\n",
            "Epoch 497/2000, Learning Rate: 6.771606527899769e-05, Loss: 0.019226454198360443\n",
            "Epoch 498/2000, Learning Rate: 6.703890462620771e-05, Loss: 0.025639526546001434\n",
            "Epoch 499/2000, Learning Rate: 6.636851557994564e-05, Loss: 0.01511949859559536\n",
            "Epoch 500/2000, Learning Rate: 6.570483042414618e-05, Loss: 0.1052827388048172\n",
            "Epoch 501/2000, Learning Rate: 6.504778211990472e-05, Loss: 0.07395113259553909\n",
            "Epoch 502/2000, Learning Rate: 6.439730429870567e-05, Loss: 0.03729994222521782\n",
            "Epoch 503/2000, Learning Rate: 6.375333125571861e-05, Loss: 0.012786106206476688\n",
            "Epoch 504/2000, Learning Rate: 6.311579794316142e-05, Loss: 0.032289281487464905\n",
            "Epoch 505/2000, Learning Rate: 6.24846399637298e-05, Loss: 0.011850811541080475\n",
            "Epoch 506/2000, Learning Rate: 6.185979356409251e-05, Loss: 0.03147193789482117\n",
            "Epoch 507/2000, Learning Rate: 6.124119562845158e-05, Loss: 0.23558463156223297\n",
            "Epoch 508/2000, Learning Rate: 6.0628783672167066e-05, Loss: 0.04837861284613609\n",
            "Epoch 509/2000, Learning Rate: 6.00224958354454e-05, Loss: 0.1053089126944542\n",
            "Epoch 510/2000, Learning Rate: 5.942227087709094e-05, Loss: 0.0580679252743721\n",
            "Epoch 511/2000, Learning Rate: 5.8828048168320034e-05, Loss: 0.019715171307325363\n",
            "Epoch 512/2000, Learning Rate: 5.823976768663683e-05, Loss: 0.08927062153816223\n",
            "Epoch 513/2000, Learning Rate: 5.765737000977046e-05, Loss: 0.006868591066449881\n",
            "Epoch 514/2000, Learning Rate: 5.708079630967276e-05, Loss: 0.023921068757772446\n",
            "Epoch 515/2000, Learning Rate: 5.650998834657603e-05, Loss: 0.11331138759851456\n",
            "Epoch 516/2000, Learning Rate: 5.594488846311027e-05, Loss: 0.0317608006298542\n",
            "Epoch 517/2000, Learning Rate: 5.5385439578479166e-05, Loss: 0.04846587032079697\n",
            "Epoch 518/2000, Learning Rate: 5.483158518269437e-05, Loss: 0.10749740153551102\n",
            "Epoch 519/2000, Learning Rate: 5.4283269330867425e-05, Loss: 0.08741313219070435\n",
            "Epoch 520/2000, Learning Rate: 5.374043663755875e-05, Loss: 0.05828133225440979\n",
            "Epoch 521/2000, Learning Rate: 5.320303227118316e-05, Loss: 0.5495309233665466\n",
            "Epoch 522/2000, Learning Rate: 5.267100194847133e-05, Loss: 0.05345681309700012\n",
            "Epoch 523/2000, Learning Rate: 5.2144291928986614e-05, Loss: 0.14044339954853058\n",
            "Epoch 524/2000, Learning Rate: 5.162284900969675e-05, Loss: 0.08926338702440262\n",
            "Epoch 525/2000, Learning Rate: 5.110662051959978e-05, Loss: 0.052987512201070786\n",
            "Epoch 526/2000, Learning Rate: 5.0595554314403785e-05, Loss: 0.1324130743741989\n",
            "Epoch 527/2000, Learning Rate: 5.008959877125975e-05, Loss: 0.20868007838726044\n",
            "Epoch 528/2000, Learning Rate: 4.958870278354715e-05, Loss: 0.009518793784081936\n",
            "Epoch 529/2000, Learning Rate: 4.909281575571168e-05, Loss: 0.03614936023950577\n",
            "Epoch 530/2000, Learning Rate: 4.8601887598154566e-05, Loss: 0.03796156495809555\n",
            "Epoch 531/2000, Learning Rate: 4.811586872217302e-05, Loss: 0.11335153132677078\n",
            "Epoch 532/2000, Learning Rate: 4.7634710034951284e-05, Loss: 0.08601455390453339\n",
            "Epoch 533/2000, Learning Rate: 4.715836293460177e-05, Loss: 0.14032995700836182\n",
            "Epoch 534/2000, Learning Rate: 4.668677930525575e-05, Loss: 0.07003173977136612\n",
            "Epoch 535/2000, Learning Rate: 4.621991151220319e-05, Loss: 0.04187079891562462\n",
            "Epoch 536/2000, Learning Rate: 4.575771239708116e-05, Loss: 0.05604067072272301\n",
            "Epoch 537/2000, Learning Rate: 4.530013527311035e-05, Loss: 0.03328200802206993\n",
            "Epoch 538/2000, Learning Rate: 4.484713392037925e-05, Loss: 0.020915605127811432\n",
            "Epoch 539/2000, Learning Rate: 4.4398662581175456e-05, Loss: 0.005522162187844515\n",
            "Epoch 540/2000, Learning Rate: 4.3954675955363704e-05, Loss: 0.014728225767612457\n",
            "Epoch 541/2000, Learning Rate: 4.351512919581007e-05, Loss: 0.11084524542093277\n",
            "Epoch 542/2000, Learning Rate: 4.307997790385197e-05, Loss: 0.03276821970939636\n",
            "Epoch 543/2000, Learning Rate: 4.264917812481345e-05, Loss: 0.164425328373909\n",
            "Epoch 544/2000, Learning Rate: 4.222268634356531e-05, Loss: 0.044080618768930435\n",
            "Epoch 545/2000, Learning Rate: 4.180045948012966e-05, Loss: 0.10764127224683762\n",
            "Epoch 546/2000, Learning Rate: 4.1382454885328364e-05, Loss: 0.07921293377876282\n",
            "Epoch 547/2000, Learning Rate: 4.096863033647508e-05, Loss: 0.016258927062153816\n",
            "Epoch 548/2000, Learning Rate: 4.0558944033110324e-05, Loss: 0.08834049105644226\n",
            "Epoch 549/2000, Learning Rate: 4.015335459277922e-05, Loss: 0.11987368762493134\n",
            "Epoch 550/2000, Learning Rate: 3.9751821046851424e-05, Loss: 0.20576605200767517\n",
            "Epoch 551/2000, Learning Rate: 3.9354302836382906e-05, Loss: 0.09472344815731049\n",
            "Epoch 552/2000, Learning Rate: 3.896075980801908e-05, Loss: 0.0576595813035965\n",
            "Epoch 553/2000, Learning Rate: 3.857115220993889e-05, Loss: 0.047810520976781845\n",
            "Epoch 554/2000, Learning Rate: 3.81854406878395e-05, Loss: 0.07384813576936722\n",
            "Epoch 555/2000, Learning Rate: 3.7803586280961107e-05, Loss: 0.033902764320373535\n",
            "Epoch 556/2000, Learning Rate: 3.7425550418151495e-05, Loss: 0.2443123757839203\n",
            "Epoch 557/2000, Learning Rate: 3.705129491396998e-05, Loss: 0.06339924037456512\n",
            "Epoch 558/2000, Learning Rate: 3.6680781964830284e-05, Loss: 0.18358264863491058\n",
            "Epoch 559/2000, Learning Rate: 3.631397414518198e-05, Loss: 0.10264233499765396\n",
            "Epoch 560/2000, Learning Rate: 3.5950834403730165e-05, Loss: 0.01849573850631714\n",
            "Epoch 561/2000, Learning Rate: 3.559132605969286e-05, Loss: 0.01146235316991806\n",
            "Epoch 562/2000, Learning Rate: 3.5235412799095935e-05, Loss: 0.01749284192919731\n",
            "Epoch 563/2000, Learning Rate: 3.488305867110497e-05, Loss: 0.03676403686404228\n",
            "Epoch 564/2000, Learning Rate: 3.453422808439392e-05, Loss: 0.1530752331018448\n",
            "Epoch 565/2000, Learning Rate: 3.418888580354998e-05, Loss: 0.03950316831469536\n",
            "Epoch 566/2000, Learning Rate: 3.384699694551448e-05, Loss: 0.0644509419798851\n",
            "Epoch 567/2000, Learning Rate: 3.350852697605933e-05, Loss: 0.012218213640153408\n",
            "Epoch 568/2000, Learning Rate: 3.317344170629874e-05, Loss: 0.014376302249729633\n",
            "Epoch 569/2000, Learning Rate: 3.284170728923575e-05, Loss: 0.06490486115217209\n",
            "Epoch 570/2000, Learning Rate: 3.2513290216343394e-05, Loss: 0.021693948656320572\n",
            "Epoch 571/2000, Learning Rate: 3.218815731417996e-05, Loss: 0.03205864876508713\n",
            "Epoch 572/2000, Learning Rate: 3.186627574103816e-05, Loss: 0.028998754918575287\n",
            "Epoch 573/2000, Learning Rate: 3.154761298362778e-05, Loss: 0.03617287799715996\n",
            "Epoch 574/2000, Learning Rate: 3.12321368537915e-05, Loss: 0.1954343020915985\n",
            "Epoch 575/2000, Learning Rate: 3.0919815485253584e-05, Loss: 0.08741698414087296\n",
            "Epoch 576/2000, Learning Rate: 3.0610617330401045e-05, Loss: 0.10221065580844879\n",
            "Epoch 577/2000, Learning Rate: 3.0304511157097033e-05, Loss: 0.2698419690132141\n",
            "Epoch 578/2000, Learning Rate: 3.0001466045526064e-05, Loss: 0.04756547510623932\n",
            "Epoch 579/2000, Learning Rate: 2.9701451385070803e-05, Loss: 0.00711004389449954\n",
            "Epoch 580/2000, Learning Rate: 2.9404436871220096e-05, Loss: 0.04926454275846481\n",
            "Epoch 581/2000, Learning Rate: 2.9110392502507896e-05, Loss: 0.1602841168642044\n",
            "Epoch 582/2000, Learning Rate: 2.8819288577482815e-05, Loss: 0.14915408194065094\n",
            "Epoch 583/2000, Learning Rate: 2.8531095691707987e-05, Loss: 0.04078914597630501\n",
            "Epoch 584/2000, Learning Rate: 2.8245784734790907e-05, Loss: 0.008951149880886078\n",
            "Epoch 585/2000, Learning Rate: 2.7963326887442997e-05, Loss: 0.015880037099123\n",
            "Epoch 586/2000, Learning Rate: 2.7683693618568566e-05, Loss: 0.030299829319119453\n",
            "Epoch 587/2000, Learning Rate: 2.740685668238288e-05, Loss: 0.28982865810394287\n",
            "Epoch 588/2000, Learning Rate: 2.713278811555905e-05, Loss: 0.07362502068281174\n",
            "Epoch 589/2000, Learning Rate: 2.686146023440346e-05, Loss: 0.008585757575929165\n",
            "Epoch 590/2000, Learning Rate: 2.6592845632059425e-05, Loss: 0.18796558678150177\n",
            "Epoch 591/2000, Learning Rate: 2.6326917175738832e-05, Loss: 0.06485474854707718\n",
            "Epoch 592/2000, Learning Rate: 2.6063648003981445e-05, Loss: 0.012749780900776386\n",
            "Epoch 593/2000, Learning Rate: 2.580301152394163e-05, Loss: 0.14001649618148804\n",
            "Epoch 594/2000, Learning Rate: 2.554498140870221e-05, Loss: 0.0247068889439106\n",
            "Epoch 595/2000, Learning Rate: 2.5289531594615188e-05, Loss: 0.1342669427394867\n",
            "Epoch 596/2000, Learning Rate: 2.5036636278669037e-05, Loss: 0.07935256510972977\n",
            "Epoch 597/2000, Learning Rate: 2.4786269915882345e-05, Loss: 0.020598217844963074\n",
            "Epoch 598/2000, Learning Rate: 2.453840721672352e-05, Loss: 0.1114841103553772\n",
            "Epoch 599/2000, Learning Rate: 2.4293023144556285e-05, Loss: 0.010168038308620453\n",
            "Epoch 600/2000, Learning Rate: 2.4050092913110723e-05, Loss: 0.017598580569028854\n",
            "Epoch 601/2000, Learning Rate: 2.3809591983979617e-05, Loss: 0.1639893651008606\n",
            "Epoch 602/2000, Learning Rate: 2.357149606413982e-05, Loss: 0.08967436105012894\n",
            "Epoch 603/2000, Learning Rate: 2.3335781103498423e-05, Loss: 0.14180757105350494\n",
            "Epoch 604/2000, Learning Rate: 2.3102423292463438e-05, Loss: 0.013869852758944035\n",
            "Epoch 605/2000, Learning Rate: 2.2871399059538803e-05, Loss: 0.031787000596523285\n",
            "Epoch 606/2000, Learning Rate: 2.2642685068943415e-05, Loss: 0.23611950874328613\n",
            "Epoch 607/2000, Learning Rate: 2.241625821825398e-05, Loss: 0.12804238498210907\n",
            "Epoch 608/2000, Learning Rate: 2.219209563607144e-05, Loss: 0.13082265853881836\n",
            "Epoch 609/2000, Learning Rate: 2.1970174679710727e-05, Loss: 0.024174202233552933\n",
            "Epoch 610/2000, Learning Rate: 2.175047293291362e-05, Loss: 0.00894281454384327\n",
            "Epoch 611/2000, Learning Rate: 2.1532968203584482e-05, Loss: 0.27939239144325256\n",
            "Epoch 612/2000, Learning Rate: 2.1317638521548636e-05, Loss: 0.008459387347102165\n",
            "Epoch 613/2000, Learning Rate: 2.1104462136333148e-05, Loss: 0.005834118463099003\n",
            "Epoch 614/2000, Learning Rate: 2.0893417514969817e-05, Loss: 0.005492204800248146\n",
            "Epoch 615/2000, Learning Rate: 2.068448333982012e-05, Loss: 0.19759996235370636\n",
            "Epoch 616/2000, Learning Rate: 2.047763850642192e-05, Loss: 0.04006292298436165\n",
            "Epoch 617/2000, Learning Rate: 2.02728621213577e-05, Loss: 0.04338175430893898\n",
            "Epoch 618/2000, Learning Rate: 2.0070133500144122e-05, Loss: 0.0057291993871331215\n",
            "Epoch 619/2000, Learning Rate: 1.986943216514268e-05, Loss: 0.11284994333982468\n",
            "Epoch 620/2000, Learning Rate: 1.9670737843491254e-05, Loss: 0.030414262786507607\n",
            "Epoch 621/2000, Learning Rate: 1.947403046505634e-05, Loss: 0.009001505561172962\n",
            "Epoch 622/2000, Learning Rate: 1.9279290160405776e-05, Loss: 0.014781556092202663\n",
            "Epoch 623/2000, Learning Rate: 1.9086497258801718e-05, Loss: 0.01671038381755352\n",
            "Epoch 624/2000, Learning Rate: 1.88956322862137e-05, Loss: 0.010495145805180073\n",
            "Epoch 625/2000, Learning Rate: 1.8706675963351564e-05, Loss: 0.07910404354333878\n",
            "Epoch 626/2000, Learning Rate: 1.8519609203718047e-05, Loss: 0.2385292798280716\n",
            "Epoch 627/2000, Learning Rate: 1.8334413111680868e-05, Loss: 0.011223047971725464\n",
            "Epoch 628/2000, Learning Rate: 1.8151068980564058e-05, Loss: 0.01817292906343937\n",
            "Epoch 629/2000, Learning Rate: 1.7969558290758418e-05, Loss: 0.09214495867490768\n",
            "Epoch 630/2000, Learning Rate: 1.7789862707850834e-05, Loss: 0.07448272407054901\n",
            "Epoch 631/2000, Learning Rate: 1.7611964080772325e-05, Loss: 0.022965487092733383\n",
            "Epoch 632/2000, Learning Rate: 1.74358444399646e-05, Loss: 0.06786949187517166\n",
            "Epoch 633/2000, Learning Rate: 1.7261485995564955e-05, Loss: 0.006605113856494427\n",
            "Epoch 634/2000, Learning Rate: 1.7088871135609306e-05, Loss: 0.3759087026119232\n",
            "Epoch 635/2000, Learning Rate: 1.6917982424253213e-05, Loss: 0.216225266456604\n",
            "Epoch 636/2000, Learning Rate: 1.6748802600010683e-05, Loss: 0.04193233698606491\n",
            "Epoch 637/2000, Learning Rate: 1.6581314574010574e-05, Loss: 0.019418271258473396\n",
            "Epoch 638/2000, Learning Rate: 1.6415501428270468e-05, Loss: 0.15723039209842682\n",
            "Epoch 639/2000, Learning Rate: 1.6251346413987764e-05, Loss: 0.0221815574914217\n",
            "Epoch 640/2000, Learning Rate: 1.6088832949847885e-05, Loss: 0.10989975929260254\n",
            "Epoch 641/2000, Learning Rate: 1.5927944620349405e-05, Loss: 0.012639403343200684\n",
            "Epoch 642/2000, Learning Rate: 1.576866517414591e-05, Loss: 0.06199956312775612\n",
            "Epoch 643/2000, Learning Rate: 1.561097852240445e-05, Loss: 0.034126486629247665\n",
            "Epoch 644/2000, Learning Rate: 1.5454868737180407e-05, Loss: 0.07244444638490677\n",
            "Epoch 645/2000, Learning Rate: 1.53003200498086e-05, Loss: 0.15962368249893188\n",
            "Epoch 646/2000, Learning Rate: 1.5147316849310516e-05, Loss: 0.06400888413190842\n",
            "Epoch 647/2000, Learning Rate: 1.4995843680817411e-05, Loss: 0.026113679632544518\n",
            "Epoch 648/2000, Learning Rate: 1.4845885244009237e-05, Loss: 0.10460825264453888\n",
            "Epoch 649/2000, Learning Rate: 1.4697426391569144e-05, Loss: 0.004875010345131159\n",
            "Epoch 650/2000, Learning Rate: 1.4550452127653453e-05, Loss: 0.07962861657142639\n",
            "Epoch 651/2000, Learning Rate: 1.4404947606376918e-05, Loss: 0.06913165003061295\n",
            "Epoch 652/2000, Learning Rate: 1.4260898130313148e-05, Loss: 0.007085309363901615\n",
            "Epoch 653/2000, Learning Rate: 1.4118289149010016e-05, Loss: 0.02916497178375721\n",
            "Epoch 654/2000, Learning Rate: 1.3977106257519915e-05, Loss: 0.008491256274282932\n",
            "Epoch 655/2000, Learning Rate: 1.3837335194944715e-05, Loss: 0.045234210789203644\n",
            "Epoch 656/2000, Learning Rate: 1.3698961842995269e-05, Loss: 0.07122121006250381\n",
            "Epoch 657/2000, Learning Rate: 1.3561972224565316e-05, Loss: 0.04497304931282997\n",
            "Epoch 658/2000, Learning Rate: 1.3426352502319663e-05, Loss: 0.07542253285646439\n",
            "Epoch 659/2000, Learning Rate: 1.3292088977296467e-05, Loss: 0.11159232258796692\n",
            "Epoch 660/2000, Learning Rate: 1.3159168087523503e-05, Loss: 0.03711935132741928\n",
            "Epoch 661/2000, Learning Rate: 1.3027576406648268e-05, Loss: 0.01769225485622883\n",
            "Epoch 662/2000, Learning Rate: 1.2897300642581785e-05, Loss: 0.10109145939350128\n",
            "Epoch 663/2000, Learning Rate: 1.2768327636155967e-05, Loss: 0.021046340465545654\n",
            "Epoch 664/2000, Learning Rate: 1.2640644359794408e-05, Loss: 0.027539202943444252\n",
            "Epoch 665/2000, Learning Rate: 1.2514237916196465e-05, Loss: 0.031920552253723145\n",
            "Epoch 666/2000, Learning Rate: 1.23890955370345e-05, Loss: 0.04958236962556839\n",
            "Epoch 667/2000, Learning Rate: 1.2265204581664155e-05, Loss: 0.21139931678771973\n",
            "Epoch 668/2000, Learning Rate: 1.2142552535847513e-05, Loss: 0.033710777759552\n",
            "Epoch 669/2000, Learning Rate: 1.2021127010489037e-05, Loss: 0.019327174872159958\n",
            "Epoch 670/2000, Learning Rate: 1.1900915740384146e-05, Loss: 0.07411542534828186\n",
            "Epoch 671/2000, Learning Rate: 1.1781906582980305e-05, Loss: 0.0384560152888298\n",
            "Epoch 672/2000, Learning Rate: 1.1664087517150503e-05, Loss: 0.10530517250299454\n",
            "Epoch 673/2000, Learning Rate: 1.1547446641978998e-05, Loss: 0.24912109971046448\n",
            "Epoch 674/2000, Learning Rate: 1.1431972175559208e-05, Loss: 0.007761063985526562\n",
            "Epoch 675/2000, Learning Rate: 1.1317652453803616e-05, Loss: 0.029011299833655357\n",
            "Epoch 676/2000, Learning Rate: 1.120447592926558e-05, Loss: 0.12648628652095795\n",
            "Epoch 677/2000, Learning Rate: 1.1092431169972925e-05, Loss: 0.03286806121468544\n",
            "Epoch 678/2000, Learning Rate: 1.0981506858273196e-05, Loss: 0.034919220954179764\n",
            "Epoch 679/2000, Learning Rate: 1.0871691789690463e-05, Loss: 0.02049393579363823\n",
            "Epoch 680/2000, Learning Rate: 1.0762974871793559e-05, Loss: 0.11166390031576157\n",
            "Epoch 681/2000, Learning Rate: 1.0655345123075623e-05, Loss: 0.21391578018665314\n",
            "Epoch 682/2000, Learning Rate: 1.0548791671844867e-05, Loss: 0.03326980024576187\n",
            "Epoch 683/2000, Learning Rate: 1.0443303755126419e-05, Loss: 0.015002679079771042\n",
            "Epoch 684/2000, Learning Rate: 1.0338870717575154e-05, Loss: 0.02562779374420643\n",
            "Epoch 685/2000, Learning Rate: 1.0235482010399403e-05, Loss: 0.016115397214889526\n",
            "Epoch 686/2000, Learning Rate: 1.0133127190295408e-05, Loss: 0.004274364095181227\n",
            "Epoch 687/2000, Learning Rate: 1.0031795918392453e-05, Loss: 0.131081223487854\n",
            "Epoch 688/2000, Learning Rate: 9.93147795920853e-06, Loss: 0.0270678848028183\n",
            "Epoch 689/2000, Learning Rate: 9.832163179616444e-06, Loss: 0.022458817809820175\n",
            "Epoch 690/2000, Learning Rate: 9.73384154782028e-06, Loss: 0.020703090354800224\n",
            "Epoch 691/2000, Learning Rate: 9.636503132342078e-06, Loss: 0.02519121766090393\n",
            "Epoch 692/2000, Learning Rate: 9.540138101018656e-06, Loss: 0.19022876024246216\n",
            "Epoch 693/2000, Learning Rate: 9.44473672000847e-06, Loss: 0.09911023080348969\n",
            "Epoch 694/2000, Learning Rate: 9.350289352808385e-06, Loss: 0.041062723845243454\n",
            "Epoch 695/2000, Learning Rate: 9.256786459280301e-06, Loss: 0.046014755964279175\n",
            "Epoch 696/2000, Learning Rate: 9.164218594687497e-06, Loss: 0.17044664919376373\n",
            "Epoch 697/2000, Learning Rate: 9.072576408740622e-06, Loss: 0.016961224377155304\n",
            "Epoch 698/2000, Learning Rate: 8.981850644653216e-06, Loss: 0.013533757999539375\n",
            "Epoch 699/2000, Learning Rate: 8.892032138206685e-06, Loss: 0.10086697340011597\n",
            "Epoch 700/2000, Learning Rate: 8.803111816824617e-06, Loss: 0.132712721824646\n",
            "Epoch 701/2000, Learning Rate: 8.71508069865637e-06, Loss: 0.0845755860209465\n",
            "Epoch 702/2000, Learning Rate: 8.627929891669807e-06, Loss: 0.030034955590963364\n",
            "Epoch 703/2000, Learning Rate: 8.541650592753109e-06, Loss: 0.05064058676362038\n",
            "Epoch 704/2000, Learning Rate: 8.456234086825578e-06, Loss: 0.026433832943439484\n",
            "Epoch 705/2000, Learning Rate: 8.371671745957323e-06, Loss: 0.006141507066786289\n",
            "Epoch 706/2000, Learning Rate: 8.287955028497749e-06, Loss: 0.0036018812097609043\n",
            "Epoch 707/2000, Learning Rate: 8.205075478212771e-06, Loss: 0.02444491721689701\n",
            "Epoch 708/2000, Learning Rate: 8.123024723430643e-06, Loss: 0.11188516020774841\n",
            "Epoch 709/2000, Learning Rate: 8.041794476196336e-06, Loss: 0.058046769350767136\n",
            "Epoch 710/2000, Learning Rate: 7.961376531434372e-06, Loss: 0.17189444601535797\n",
            "Epoch 711/2000, Learning Rate: 7.881762766120029e-06, Loss: 0.052122727036476135\n",
            "Epoch 712/2000, Learning Rate: 7.802945138458829e-06, Loss: 0.005257914774119854\n",
            "Epoch 713/2000, Learning Rate: 7.72491568707424e-06, Loss: 0.025258053094148636\n",
            "Epoch 714/2000, Learning Rate: 7.647666530203497e-06, Loss: 0.13855713605880737\n",
            "Epoch 715/2000, Learning Rate: 7.571189864901462e-06, Loss: 0.12289600819349289\n",
            "Epoch 716/2000, Learning Rate: 7.495477966252447e-06, Loss: 0.07212451845407486\n",
            "Epoch 717/2000, Learning Rate: 7.420523186589922e-06, Loss: 0.12275233119726181\n",
            "Epoch 718/2000, Learning Rate: 7.346317954724023e-06, Loss: 0.041812118142843246\n",
            "Epoch 719/2000, Learning Rate: 7.272854775176783e-06, Loss: 0.37586483359336853\n",
            "Epoch 720/2000, Learning Rate: 7.200126227425015e-06, Loss: 0.010595344938337803\n",
            "Epoch 721/2000, Learning Rate: 7.128124965150765e-06, Loss: 0.050465960055589676\n",
            "Epoch 722/2000, Learning Rate: 7.0568437154992575e-06, Loss: 0.01874425634741783\n",
            "Epoch 723/2000, Learning Rate: 6.9862752783442645e-06, Loss: 0.26056307554244995\n",
            "Epoch 724/2000, Learning Rate: 6.916412525560822e-06, Loss: 0.40897777676582336\n",
            "Epoch 725/2000, Learning Rate: 6.847248400305214e-06, Loss: 0.021632907912135124\n",
            "Epoch 726/2000, Learning Rate: 6.778775916302162e-06, Loss: 0.026399478316307068\n",
            "Epoch 727/2000, Learning Rate: 6.71098815713914e-06, Loss: 0.011629341170191765\n",
            "Epoch 728/2000, Learning Rate: 6.6438782755677485e-06, Loss: 0.05046496540307999\n",
            "Epoch 729/2000, Learning Rate: 6.577439492812071e-06, Loss: 0.06340532749891281\n",
            "Epoch 730/2000, Learning Rate: 6.511665097883951e-06, Loss: 0.0745745524764061\n",
            "Epoch 731/2000, Learning Rate: 6.446548446905111e-06, Loss: 0.05159235745668411\n",
            "Epoch 732/2000, Learning Rate: 6.38208296243606e-06, Loss: 0.010283692739903927\n",
            "Epoch 733/2000, Learning Rate: 6.318262132811699e-06, Loss: 0.009214709512889385\n",
            "Epoch 734/2000, Learning Rate: 6.2550795114835825e-06, Loss: 0.05311515927314758\n",
            "Epoch 735/2000, Learning Rate: 6.1925287163687465e-06, Loss: 0.07954344153404236\n",
            "Epoch 736/2000, Learning Rate: 6.130603429205059e-06, Loss: 0.010914654470980167\n",
            "Epoch 737/2000, Learning Rate: 6.0692973949130085e-06, Loss: 0.03718908131122589\n",
            "Epoch 738/2000, Learning Rate: 6.0086044209638785e-06, Loss: 0.20191054046154022\n",
            "Epoch 739/2000, Learning Rate: 5.948518376754239e-06, Loss: 0.008410836569964886\n",
            "Epoch 740/2000, Learning Rate: 5.889033192986697e-06, Loss: 0.28717735409736633\n",
            "Epoch 741/2000, Learning Rate: 5.83014286105683e-06, Loss: 0.11932193487882614\n",
            "Epoch 742/2000, Learning Rate: 5.771841432446262e-06, Loss: 0.03530745208263397\n",
            "Epoch 743/2000, Learning Rate: 5.7141230181217994e-06, Loss: 0.01734020933508873\n",
            "Epoch 744/2000, Learning Rate: 5.656981787940581e-06, Loss: 0.025926323607563972\n",
            "Epoch 745/2000, Learning Rate: 5.600411970061175e-06, Loss: 0.0972437784075737\n",
            "Epoch 746/2000, Learning Rate: 5.544407850360563e-06, Loss: 0.042178522795438766\n",
            "Epoch 747/2000, Learning Rate: 5.488963771856958e-06, Loss: 0.09303862601518631\n",
            "Epoch 748/2000, Learning Rate: 5.434074134138388e-06, Loss: 0.026863478124141693\n",
            "Epoch 749/2000, Learning Rate: 5.379733392797004e-06, Loss: 0.05210747569799423\n",
            "Epoch 750/2000, Learning Rate: 5.325936058869034e-06, Loss: 0.004093940835446119\n",
            "Epoch 751/2000, Learning Rate: 5.272676698280344e-06, Loss: 0.0983920618891716\n",
            "Epoch 752/2000, Learning Rate: 5.21994993129754e-06, Loss: 0.21268969774246216\n",
            "Epoch 753/2000, Learning Rate: 5.167750431984565e-06, Loss: 0.008937419392168522\n",
            "Epoch 754/2000, Learning Rate: 5.116072927664719e-06, Loss: 0.13680346310138702\n",
            "Epoch 755/2000, Learning Rate: 5.0649121983880715e-06, Loss: 0.01820298098027706\n",
            "Epoch 756/2000, Learning Rate: 5.0142630764041905e-06, Loss: 0.013484754599630833\n",
            "Epoch 757/2000, Learning Rate: 4.9641204456401485e-06, Loss: 0.009151170961558819\n",
            "Epoch 758/2000, Learning Rate: 4.914479241183747e-06, Loss: 0.12499277293682098\n",
            "Epoch 759/2000, Learning Rate: 4.86533444877191e-06, Loss: 0.01879230886697769\n",
            "Epoch 760/2000, Learning Rate: 4.816681104284191e-06, Loss: 0.10308465361595154\n",
            "Epoch 761/2000, Learning Rate: 4.768514293241349e-06, Loss: 0.3235814869403839\n",
            "Epoch 762/2000, Learning Rate: 4.720829150308935e-06, Loss: 0.060424283146858215\n",
            "Epoch 763/2000, Learning Rate: 4.673620858805846e-06, Loss: 0.01473414059728384\n",
            "Epoch 764/2000, Learning Rate: 4.626884650217787e-06, Loss: 0.00929673295468092\n",
            "Epoch 765/2000, Learning Rate: 4.5806158037156095e-06, Loss: 0.12100039422512054\n",
            "Epoch 766/2000, Learning Rate: 4.534809645678453e-06, Loss: 0.028692428022623062\n",
            "Epoch 767/2000, Learning Rate: 4.489461549221669e-06, Loss: 0.020595094189047813\n",
            "Epoch 768/2000, Learning Rate: 4.444566933729452e-06, Loss: 0.09246598184108734\n",
            "Epoch 769/2000, Learning Rate: 4.400121264392157e-06, Loss: 0.06061708927154541\n",
            "Epoch 770/2000, Learning Rate: 4.3561200517482355e-06, Loss: 0.13105182349681854\n",
            "Epoch 771/2000, Learning Rate: 4.312558851230753e-06, Loss: 0.024922989308834076\n",
            "Epoch 772/2000, Learning Rate: 4.269433262718446e-06, Loss: 0.07339540123939514\n",
            "Epoch 773/2000, Learning Rate: 4.226738930091261e-06, Loss: 0.02761569432914257\n",
            "Epoch 774/2000, Learning Rate: 4.184471540790349e-06, Loss: 0.02536212094128132\n",
            "Epoch 775/2000, Learning Rate: 4.142626825382445e-06, Loss: 0.0034382324665784836\n",
            "Epoch 776/2000, Learning Rate: 4.10120055712862e-06, Loss: 0.0939374566078186\n",
            "Epoch 777/2000, Learning Rate: 4.060188551557334e-06, Loss: 0.07038706541061401\n",
            "Epoch 778/2000, Learning Rate: 4.0195866660417606e-06, Loss: 0.06603380292654037\n",
            "Epoch 779/2000, Learning Rate: 3.979390799381343e-06, Loss: 0.050591301172971725\n",
            "Epoch 780/2000, Learning Rate: 3.93959689138753e-06, Loss: 0.092366062104702\n",
            "Epoch 781/2000, Learning Rate: 3.900200922473654e-06, Loss: 0.036762867122888565\n",
            "Epoch 782/2000, Learning Rate: 3.861198913248918e-06, Loss: 0.03158946335315704\n",
            "Epoch 783/2000, Learning Rate: 3.822586924116428e-06, Loss: 0.01325304340571165\n",
            "Epoch 784/2000, Learning Rate: 3.784361054875264e-06, Loss: 0.09015076607465744\n",
            "Epoch 785/2000, Learning Rate: 3.746517444326511e-06, Loss: 0.06754675507545471\n",
            "Epoch 786/2000, Learning Rate: 3.709052269883246e-06, Loss: 0.0739607885479927\n",
            "Epoch 787/2000, Learning Rate: 3.6719617471844135e-06, Loss: 0.08785856515169144\n",
            "Epoch 788/2000, Learning Rate: 3.635242129712569e-06, Loss: 0.027791842818260193\n",
            "Epoch 789/2000, Learning Rate: 3.5988897084154433e-06, Loss: 0.010554632171988487\n",
            "Epoch 790/2000, Learning Rate: 3.562900811331289e-06, Loss: 0.19412901997566223\n",
            "Epoch 791/2000, Learning Rate: 3.527271803217976e-06, Loss: 0.0676594153046608\n",
            "Epoch 792/2000, Learning Rate: 3.491999085185796e-06, Loss: 0.02455756440758705\n",
            "Epoch 793/2000, Learning Rate: 3.457079094333938e-06, Loss: 0.0625862404704094\n",
            "Epoch 794/2000, Learning Rate: 3.4225083033905987e-06, Loss: 0.01798374205827713\n",
            "Epoch 795/2000, Learning Rate: 3.3882832203566927e-06, Loss: 0.05861976742744446\n",
            "Epoch 796/2000, Learning Rate: 3.354400388153126e-06, Loss: 0.14310826361179352\n",
            "Epoch 797/2000, Learning Rate: 3.3208563842715948e-06, Loss: 0.058363381773233414\n",
            "Epoch 798/2000, Learning Rate: 3.287647820428879e-06, Loss: 0.039796099066734314\n",
            "Epoch 799/2000, Learning Rate: 3.25477134222459e-06, Loss: 0.03670606017112732\n",
            "Epoch 800/2000, Learning Rate: 3.2222236288023443e-06, Loss: 0.06698973476886749\n",
            "Epoch 801/2000, Learning Rate: 3.190001392514321e-06, Loss: 0.005902172531932592\n",
            "Epoch 802/2000, Learning Rate: 3.1581013785891775e-06, Loss: 0.01782563142478466\n",
            "Epoch 803/2000, Learning Rate: 3.1265203648032856e-06, Loss: 0.058256637305021286\n",
            "Epoch 804/2000, Learning Rate: 3.0952551611552528e-06, Loss: 0.0825742706656456\n",
            "Epoch 805/2000, Learning Rate: 3.0643026095437004e-06, Loss: 0.009944605641067028\n",
            "Epoch 806/2000, Learning Rate: 3.0336595834482633e-06, Loss: 0.06650440394878387\n",
            "Epoch 807/2000, Learning Rate: 3.0033229876137806e-06, Loss: 0.011644975282251835\n",
            "Epoch 808/2000, Learning Rate: 2.973289757737643e-06, Loss: 0.09976017475128174\n",
            "Epoch 809/2000, Learning Rate: 2.9435568601602665e-06, Loss: 0.03790884464979172\n",
            "Epoch 810/2000, Learning Rate: 2.914121291558664e-06, Loss: 0.031092049553990364\n",
            "Epoch 811/2000, Learning Rate: 2.8849800786430774e-06, Loss: 0.2404395192861557\n",
            "Epoch 812/2000, Learning Rate: 2.8561302778566464e-06, Loss: 0.02881682477891445\n",
            "Epoch 813/2000, Learning Rate: 2.82756897507808e-06, Loss: 0.09444636851549149\n",
            "Epoch 814/2000, Learning Rate: 2.799293285327299e-06, Loss: 0.1518309861421585\n",
            "Epoch 815/2000, Learning Rate: 2.771300352474026e-06, Loss: 0.09143971651792526\n",
            "Epoch 816/2000, Learning Rate: 2.7435873489492856e-06, Loss: 0.1018933355808258\n",
            "Epoch 817/2000, Learning Rate: 2.7161514754597925e-06, Loss: 0.002602290827780962\n",
            "Epoch 818/2000, Learning Rate: 2.6889899607051947e-06, Loss: 0.004782813601195812\n",
            "Epoch 819/2000, Learning Rate: 2.6621000610981427e-06, Loss: 0.012779404409229755\n",
            "Epoch 820/2000, Learning Rate: 2.635479060487161e-06, Loss: 0.004568031523376703\n",
            "Epoch 821/2000, Learning Rate: 2.6091242698822894e-06, Loss: 0.016980433836579323\n",
            "Epoch 822/2000, Learning Rate: 2.5830330271834666e-06, Loss: 0.032916802912950516\n",
            "Epoch 823/2000, Learning Rate: 2.557202696911632e-06, Loss: 0.014604374766349792\n",
            "Epoch 824/2000, Learning Rate: 2.5316306699425154e-06, Loss: 0.021893803030252457\n",
            "Epoch 825/2000, Learning Rate: 2.50631436324309e-06, Loss: 0.00193953444249928\n",
            "Epoch 826/2000, Learning Rate: 2.481251219610659e-06, Loss: 0.08491669595241547\n",
            "Epoch 827/2000, Learning Rate: 2.4564387074145525e-06, Loss: 0.08971511572599411\n",
            "Epoch 828/2000, Learning Rate: 2.431874320340407e-06, Loss: 0.01272678840905428\n",
            "Epoch 829/2000, Learning Rate: 2.4075555771370026e-06, Loss: 0.12290025502443314\n",
            "Epoch 830/2000, Learning Rate: 2.3834800213656328e-06, Loss: 0.08334042876958847\n",
            "Epoch 831/2000, Learning Rate: 2.3596452211519766e-06, Loss: 0.07200130820274353\n",
            "Epoch 832/2000, Learning Rate: 2.336048768940457e-06, Loss: 0.20290963351726532\n",
            "Epoch 833/2000, Learning Rate: 2.312688281251052e-06, Loss: 0.037243008613586426\n",
            "Epoch 834/2000, Learning Rate: 2.2895613984385416e-06, Loss: 0.009980987757444382\n",
            "Epoch 835/2000, Learning Rate: 2.266665784454156e-06, Loss: 0.11007954925298691\n",
            "Epoch 836/2000, Learning Rate: 2.2439991266096143e-06, Loss: 0.007084696553647518\n",
            "Epoch 837/2000, Learning Rate: 2.221559135343518e-06, Loss: 0.004145764280110598\n",
            "Epoch 838/2000, Learning Rate: 2.1993435439900828e-06, Loss: 0.14204373955726624\n",
            "Epoch 839/2000, Learning Rate: 2.1773501085501817e-06, Loss: 0.08697780966758728\n",
            "Epoch 840/2000, Learning Rate: 2.15557660746468e-06, Loss: 0.03052551858127117\n",
            "Epoch 841/2000, Learning Rate: 2.134020841390033e-06, Loss: 0.1956208348274231\n",
            "Epoch 842/2000, Learning Rate: 2.112680632976133e-06, Loss: 0.0737200602889061\n",
            "Epoch 843/2000, Learning Rate: 2.0915538266463713e-06, Loss: 0.07958723604679108\n",
            "Epoch 844/2000, Learning Rate: 2.0706382883799075e-06, Loss: 0.10553053766489029\n",
            "Epoch 845/2000, Learning Rate: 2.0499319054961085e-06, Loss: 0.14229394495487213\n",
            "Epoch 846/2000, Learning Rate: 2.0294325864411473e-06, Loss: 0.05513441935181618\n",
            "Epoch 847/2000, Learning Rate: 2.0091382605767357e-06, Loss: 0.04671864956617355\n",
            "Epoch 848/2000, Learning Rate: 1.9890468779709683e-06, Loss: 0.010964893735945225\n",
            "Epoch 849/2000, Learning Rate: 1.9691564091912586e-06, Loss: 0.01638541743159294\n",
            "Epoch 850/2000, Learning Rate: 1.949464845099346e-06, Loss: 0.01620488241314888\n",
            "Epoch 851/2000, Learning Rate: 1.9299701966483525e-06, Loss: 0.006391298025846481\n",
            "Epoch 852/2000, Learning Rate: 1.910670494681869e-06, Loss: 0.06346818804740906\n",
            "Epoch 853/2000, Learning Rate: 1.8915637897350502e-06, Loss: 0.03218217194080353\n",
            "Epoch 854/2000, Learning Rate: 1.8726481518376996e-06, Loss: 0.10128635168075562\n",
            "Epoch 855/2000, Learning Rate: 1.8539216703193227e-06, Loss: 0.006412869319319725\n",
            "Epoch 856/2000, Learning Rate: 1.8353824536161295e-06, Loss: 0.17682461440563202\n",
            "Epoch 857/2000, Learning Rate: 1.817028629079968e-06, Loss: 0.0064955465495586395\n",
            "Epoch 858/2000, Learning Rate: 1.7988583427891684e-06, Loss: 0.026174960657954216\n",
            "Epoch 859/2000, Learning Rate: 1.7808697593612768e-06, Loss: 0.007954615168273449\n",
            "Epoch 860/2000, Learning Rate: 1.763061061767664e-06, Loss: 0.015000633895397186\n",
            "Epoch 861/2000, Learning Rate: 1.7454304511499874e-06, Loss: 0.01658955030143261\n",
            "Epoch 862/2000, Learning Rate: 1.7279761466384874e-06, Loss: 0.002985256491228938\n",
            "Epoch 863/2000, Learning Rate: 1.7106963851721026e-06, Loss: 0.040514957159757614\n",
            "Epoch 864/2000, Learning Rate: 1.6935894213203816e-06, Loss: 0.01899329014122486\n",
            "Epoch 865/2000, Learning Rate: 1.6766535271071777e-06, Loss: 0.09580646455287933\n",
            "Epoch 866/2000, Learning Rate: 1.659886991836106e-06, Loss: 0.06816769391298294\n",
            "Epoch 867/2000, Learning Rate: 1.6432881219177447e-06, Loss: 0.027956727892160416\n",
            "Epoch 868/2000, Learning Rate: 1.6268552406985672e-06, Loss: 0.0670439675450325\n",
            "Epoch 869/2000, Learning Rate: 1.6105866882915816e-06, Loss: 0.04622439295053482\n",
            "Epoch 870/2000, Learning Rate: 1.5944808214086658e-06, Loss: 0.014799213036894798\n",
            "Epoch 871/2000, Learning Rate: 1.578536013194579e-06, Loss: 0.04319264739751816\n",
            "Epoch 872/2000, Learning Rate: 1.5627506530626331e-06, Loss: 0.044234391301870346\n",
            "Epoch 873/2000, Learning Rate: 1.5471231465320069e-06, Loss: 0.05730967968702316\n",
            "Epoch 874/2000, Learning Rate: 1.5316519150666867e-06, Loss: 0.05116536468267441\n",
            "Epoch 875/2000, Learning Rate: 1.51633539591602e-06, Loss: 0.157643124461174\n",
            "Epoch 876/2000, Learning Rate: 1.5011720419568597e-06, Loss: 0.024864839389920235\n",
            "Epoch 877/2000, Learning Rate: 1.486160321537291e-06, Loss: 0.013412200845777988\n",
            "Epoch 878/2000, Learning Rate: 1.471298718321918e-06, Loss: 0.08720614016056061\n",
            "Epoch 879/2000, Learning Rate: 1.4565857311386989e-06, Loss: 0.024902742356061935\n",
            "Epoch 880/2000, Learning Rate: 1.4420198738273118e-06, Loss: 0.024501586332917213\n",
            "Epoch 881/2000, Learning Rate: 1.4275996750890387e-06, Loss: 0.20376676321029663\n",
            "Epoch 882/2000, Learning Rate: 1.4133236783381483e-06, Loss: 0.04495111107826233\n",
            "Epoch 883/2000, Learning Rate: 1.3991904415547668e-06, Loss: 0.08056244254112244\n",
            "Epoch 884/2000, Learning Rate: 1.3851985371392192e-06, Loss: 0.06619240343570709\n",
            "Epoch 885/2000, Learning Rate: 1.371346551767827e-06, Loss: 0.0043365671299397945\n",
            "Epoch 886/2000, Learning Rate: 1.3576330862501486e-06, Loss: 0.05312822759151459\n",
            "Epoch 887/2000, Learning Rate: 1.344056755387647e-06, Loss: 0.06555972248315811\n",
            "Epoch 888/2000, Learning Rate: 1.3306161878337705e-06, Loss: 0.017045818269252777\n",
            "Epoch 889/2000, Learning Rate: 1.3173100259554327e-06, Loss: 0.041396163403987885\n",
            "Epoch 890/2000, Learning Rate: 1.3041369256958784e-06, Loss: 0.17561577260494232\n",
            "Epoch 891/2000, Learning Rate: 1.2910955564389195e-06, Loss: 0.036491964012384415\n",
            "Epoch 892/2000, Learning Rate: 1.2781846008745303e-06, Loss: 0.02973812073469162\n",
            "Epoch 893/2000, Learning Rate: 1.265402754865785e-06, Loss: 0.11857093870639801\n",
            "Epoch 894/2000, Learning Rate: 1.2527487273171272e-06, Loss: 0.011402095668017864\n",
            "Epoch 895/2000, Learning Rate: 1.240221240043956e-06, Loss: 0.05092688277363777\n",
            "Epoch 896/2000, Learning Rate: 1.2278190276435164e-06, Loss: 0.010087409056723118\n",
            "Epoch 897/2000, Learning Rate: 1.2155408373670813e-06, Loss: 0.01277707889676094\n",
            "Epoch 898/2000, Learning Rate: 1.2033854289934105e-06, Loss: 0.012367934919893742\n",
            "Epoch 899/2000, Learning Rate: 1.1913515747034764e-06, Loss: 0.005990911275148392\n",
            "Epoch 900/2000, Learning Rate: 1.1794380589564415e-06, Loss: 0.03559814766049385\n",
            "Epoch 901/2000, Learning Rate: 1.1676436783668772e-06, Loss: 0.08396889269351959\n",
            "Epoch 902/2000, Learning Rate: 1.1559672415832083e-06, Loss: 0.020981159061193466\n",
            "Epoch 903/2000, Learning Rate: 1.1444075691673763e-06, Loss: 0.11066406965255737\n",
            "Epoch 904/2000, Learning Rate: 1.1329634934757026e-06, Loss: 0.09400225430727005\n",
            "Epoch 905/2000, Learning Rate: 1.1216338585409455e-06, Loss: 0.12502062320709229\n",
            "Epoch 906/2000, Learning Rate: 1.110417519955536e-06, Loss: 0.14715664088726044\n",
            "Epoch 907/2000, Learning Rate: 1.0993133447559806e-06, Loss: 0.16157063841819763\n",
            "Epoch 908/2000, Learning Rate: 1.0883202113084208e-06, Loss: 0.04020281136035919\n",
            "Epoch 909/2000, Learning Rate: 1.0774370091953365e-06, Loss: 0.15333934128284454\n",
            "Epoch 910/2000, Learning Rate: 1.0666626391033831e-06, Loss: 0.013541645370423794\n",
            "Epoch 911/2000, Learning Rate: 1.0559960127123493e-06, Loss: 0.12436531484127045\n",
            "Epoch 912/2000, Learning Rate: 1.0454360525852258e-06, Loss: 0.0175738874822855\n",
            "Epoch 913/2000, Learning Rate: 1.0349816920593735e-06, Loss: 0.026452671736478806\n",
            "Epoch 914/2000, Learning Rate: 1.0246318751387798e-06, Loss: 0.24817059934139252\n",
            "Epoch 915/2000, Learning Rate: 1.0143855563873919e-06, Loss: 0.006468524225056171\n",
            "Epoch 916/2000, Learning Rate: 1.004241700823518e-06, Loss: 0.01602805219590664\n",
            "Epoch 917/2000, Learning Rate: 9.941992838152828e-07, Loss: 0.004758995026350021\n",
            "Epoch 918/2000, Learning Rate: 9.8425729097713e-07, Loss: 0.017318887636065483\n",
            "Epoch 919/2000, Learning Rate: 9.744147180673586e-07, Loss: 0.24588021636009216\n",
            "Epoch 920/2000, Learning Rate: 9.64670570886685e-07, Loss: 0.24649998545646667\n",
            "Epoch 921/2000, Learning Rate: 9.55023865177818e-07, Loss: 0.08496122062206268\n",
            "Epoch 922/2000, Learning Rate: 9.454736265260399e-07, Loss: 0.021397508680820465\n",
            "Epoch 923/2000, Learning Rate: 9.360188902607795e-07, Loss: 0.01629236899316311\n",
            "Epoch 924/2000, Learning Rate: 9.266587013581717e-07, Loss: 0.09382211416959763\n",
            "Epoch 925/2000, Learning Rate: 9.173921143445899e-07, Loss: 0.07314513623714447\n",
            "Epoch 926/2000, Learning Rate: 9.08218193201144e-07, Loss: 0.010095739737153053\n",
            "Epoch 927/2000, Learning Rate: 8.991360112691326e-07, Loss: 0.009260904043912888\n",
            "Epoch 928/2000, Learning Rate: 8.901446511564413e-07, Loss: 0.055384695529937744\n",
            "Epoch 929/2000, Learning Rate: 8.812432046448769e-07, Loss: 0.07354392111301422\n",
            "Epoch 930/2000, Learning Rate: 8.724307725984282e-07, Loss: 0.08318489789962769\n",
            "Epoch 931/2000, Learning Rate: 8.637064648724439e-07, Loss: 0.01855761930346489\n",
            "Epoch 932/2000, Learning Rate: 8.550694002237194e-07, Loss: 0.1693170964717865\n",
            "Epoch 933/2000, Learning Rate: 8.465187062214822e-07, Loss: 0.08783068507909775\n",
            "Epoch 934/2000, Learning Rate: 8.380535191592674e-07, Loss: 0.03629668429493904\n",
            "Epoch 935/2000, Learning Rate: 8.296729839676747e-07, Loss: 0.041123244911432266\n",
            "Epoch 936/2000, Learning Rate: 8.213762541279979e-07, Loss: 0.0554887093603611\n",
            "Epoch 937/2000, Learning Rate: 8.131624915867179e-07, Loss: 0.017915736883878708\n",
            "Epoch 938/2000, Learning Rate: 8.050308666708507e-07, Loss: 0.028185609728097916\n",
            "Epoch 939/2000, Learning Rate: 7.969805580041421e-07, Loss: 0.10766777396202087\n",
            "Epoch 940/2000, Learning Rate: 7.890107524241007e-07, Loss: 0.056215714663267136\n",
            "Epoch 941/2000, Learning Rate: 7.811206448998597e-07, Loss: 0.25041890144348145\n",
            "Epoch 942/2000, Learning Rate: 7.733094384508611e-07, Loss: 0.1822545826435089\n",
            "Epoch 943/2000, Learning Rate: 7.655763440663525e-07, Loss: 0.009889824315905571\n",
            "Epoch 944/2000, Learning Rate: 7.579205806256889e-07, Loss: 0.0013369577936828136\n",
            "Epoch 945/2000, Learning Rate: 7.50341374819432e-07, Loss: 0.07549601793289185\n",
            "Epoch 946/2000, Learning Rate: 7.428379610712378e-07, Loss: 0.03955728933215141\n",
            "Epoch 947/2000, Learning Rate: 7.354095814605254e-07, Loss: 0.08663088828325272\n",
            "Epoch 948/2000, Learning Rate: 7.280554856459201e-07, Loss: 0.04501130059361458\n",
            "Epoch 949/2000, Learning Rate: 7.207749307894608e-07, Loss: 0.05499511584639549\n",
            "Epoch 950/2000, Learning Rate: 7.135671814815662e-07, Loss: 0.19196322560310364\n",
            "Epoch 951/2000, Learning Rate: 7.064315096667505e-07, Loss: 0.05098606273531914\n",
            "Epoch 952/2000, Learning Rate: 6.993671945700831e-07, Loss: 0.10181466490030289\n",
            "Epoch 953/2000, Learning Rate: 6.923735226243822e-07, Loss: 0.14788679778575897\n",
            "Epoch 954/2000, Learning Rate: 6.854497873981384e-07, Loss: 0.14938589930534363\n",
            "Epoch 955/2000, Learning Rate: 6.78595289524157e-07, Loss: 0.14941027760505676\n",
            "Epoch 956/2000, Learning Rate: 6.718093366289155e-07, Loss: 0.16578936576843262\n",
            "Epoch 957/2000, Learning Rate: 6.650912432626263e-07, Loss: 0.1002286747097969\n",
            "Epoch 958/2000, Learning Rate: 6.5844033083e-07, Loss: 0.0304261464625597\n",
            "Epoch 959/2000, Learning Rate: 6.518559275216999e-07, Loss: 0.06034020707011223\n",
            "Epoch 960/2000, Learning Rate: 6.453373682464829e-07, Loss: 0.015035404823720455\n",
            "Epoch 961/2000, Learning Rate: 6.388839945640181e-07, Loss: 0.032995954155921936\n",
            "Epoch 962/2000, Learning Rate: 6.324951546183779e-07, Loss: 0.03594224900007248\n",
            "Epoch 963/2000, Learning Rate: 6.26170203072194e-07, Loss: 0.004384912550449371\n",
            "Epoch 964/2000, Learning Rate: 6.199085010414721e-07, Loss: 0.006388276815414429\n",
            "Epoch 965/2000, Learning Rate: 6.137094160310574e-07, Loss: 0.009875407442450523\n",
            "Epoch 966/2000, Learning Rate: 6.075723218707469e-07, Loss: 0.218694269657135\n",
            "Epoch 967/2000, Learning Rate: 6.014965986520394e-07, Loss: 0.05658581107854843\n",
            "Epoch 968/2000, Learning Rate: 5.95481632665519e-07, Loss: 0.024439232423901558\n",
            "Epoch 969/2000, Learning Rate: 5.895268163388638e-07, Loss: 0.05413936451077461\n",
            "Epoch 970/2000, Learning Rate: 5.836315481754752e-07, Loss: 0.011048538610339165\n",
            "Epoch 971/2000, Learning Rate: 5.777952326937204e-07, Loss: 0.21060603857040405\n",
            "Epoch 972/2000, Learning Rate: 5.720172803667831e-07, Loss: 0.07101451605558395\n",
            "Epoch 973/2000, Learning Rate: 5.662971075631153e-07, Loss: 0.12651900947093964\n",
            "Epoch 974/2000, Learning Rate: 5.606341364874841e-07, Loss: 0.1239369735121727\n",
            "Epoch 975/2000, Learning Rate: 5.550277951226093e-07, Loss: 0.03584151342511177\n",
            "Epoch 976/2000, Learning Rate: 5.494775171713832e-07, Loss: 0.06658120453357697\n",
            "Epoch 977/2000, Learning Rate: 5.439827419996694e-07, Loss: 0.181553915143013\n",
            "Epoch 978/2000, Learning Rate: 5.385429145796728e-07, Loss: 0.22788433730602264\n",
            "Epoch 979/2000, Learning Rate: 5.33157485433876e-07, Loss: 0.08277743309736252\n",
            "Epoch 980/2000, Learning Rate: 5.278259105795372e-07, Loss: 0.041563957929611206\n",
            "Epoch 981/2000, Learning Rate: 5.225476514737419e-07, Loss: 0.08170422166585922\n",
            "Epoch 982/2000, Learning Rate: 5.173221749590044e-07, Loss: 0.09359992295503616\n",
            "Epoch 983/2000, Learning Rate: 5.121489532094143e-07, Loss: 0.012062588706612587\n",
            "Epoch 984/2000, Learning Rate: 5.070274636773202e-07, Loss: 0.0666329637169838\n",
            "Epoch 985/2000, Learning Rate: 5.01957189040547e-07, Loss: 0.027456406503915787\n",
            "Epoch 986/2000, Learning Rate: 4.969376171501415e-07, Loss: 0.1634109616279602\n",
            "Epoch 987/2000, Learning Rate: 4.919682409786401e-07, Loss: 0.05214039608836174\n",
            "Epoch 988/2000, Learning Rate: 4.870485585688537e-07, Loss: 0.0875212773680687\n",
            "Epoch 989/2000, Learning Rate: 4.821780729831651e-07, Loss: 0.04858967661857605\n",
            "Epoch 990/2000, Learning Rate: 4.773562922533335e-07, Loss: 0.008723213337361813\n",
            "Epoch 991/2000, Learning Rate: 4.7258272933080015e-07, Loss: 0.04543943703174591\n",
            "Epoch 992/2000, Learning Rate: 4.6785690203749216e-07, Loss: 0.19000793993473053\n",
            "Epoch 993/2000, Learning Rate: 4.6317833301711725e-07, Loss: 0.12058750540018082\n",
            "Epoch 994/2000, Learning Rate: 4.5854654968694606e-07, Loss: 0.023159783333539963\n",
            "Epoch 995/2000, Learning Rate: 4.539610841900766e-07, Loss: 0.005460012704133987\n",
            "Epoch 996/2000, Learning Rate: 4.4942147334817586e-07, Loss: 0.14076344668865204\n",
            "Epoch 997/2000, Learning Rate: 4.449272586146941e-07, Loss: 0.0690574198961258\n",
            "Epoch 998/2000, Learning Rate: 4.4047798602854717e-07, Loss: 0.09166773408651352\n",
            "Epoch 999/2000, Learning Rate: 4.360732061682617e-07, Loss: 0.021985236555337906\n",
            "Epoch 1000/2000, Learning Rate: 4.317124741065791e-07, Loss: 0.13524730503559113\n",
            "Epoch 1001/2000, Learning Rate: 4.273953493655133e-07, Loss: 0.014752920717000961\n",
            "Epoch 1002/2000, Learning Rate: 4.2312139587185813e-07, Loss: 0.017277386039495468\n",
            "Epoch 1003/2000, Learning Rate: 4.1889018191313956e-07, Loss: 0.03394043818116188\n",
            "Epoch 1004/2000, Learning Rate: 4.147012800940082e-07, Loss: 0.017889782786369324\n",
            "Epoch 1005/2000, Learning Rate: 4.105542672930681e-07, Loss: 0.09773130714893341\n",
            "Epoch 1006/2000, Learning Rate: 4.064487246201374e-07, Loss: 0.13226424157619476\n",
            "Epoch 1007/2000, Learning Rate: 4.0238423737393603e-07, Loss: 0.08240453153848648\n",
            "Epoch 1008/2000, Learning Rate: 3.9836039500019665e-07, Loss: 0.08199746906757355\n",
            "Epoch 1009/2000, Learning Rate: 3.9437679105019466e-07, Loss: 0.02050027810037136\n",
            "Epoch 1010/2000, Learning Rate: 3.904330231396927e-07, Loss: 0.08341745287179947\n",
            "Epoch 1011/2000, Learning Rate: 3.865286929082958e-07, Loss: 0.01219132635742426\n",
            "Epoch 1012/2000, Learning Rate: 3.8266340597921284e-07, Loss: 0.04150339588522911\n",
            "Epoch 1013/2000, Learning Rate: 3.788367719194207e-07, Loss: 0.02419034205377102\n",
            "Epoch 1014/2000, Learning Rate: 3.750484042002265e-07, Loss: 0.044032152742147446\n",
            "Epoch 1015/2000, Learning Rate: 3.7129792015822424e-07, Loss: 0.059702396392822266\n",
            "Epoch 1016/2000, Learning Rate: 3.67584940956642e-07, Loss: 0.008590524084866047\n",
            "Epoch 1017/2000, Learning Rate: 3.6390909154707557e-07, Loss: 0.13999834656715393\n",
            "Epoch 1018/2000, Learning Rate: 3.602700006316048e-07, Loss: 0.05849170684814453\n",
            "Epoch 1019/2000, Learning Rate: 3.5666730062528875e-07, Loss: 0.0484735369682312\n",
            "Epoch 1020/2000, Learning Rate: 3.531006276190359e-07, Loss: 0.006841585505753756\n",
            "Epoch 1021/2000, Learning Rate: 3.495696213428455e-07, Loss: 0.18585222959518433\n",
            "Epoch 1022/2000, Learning Rate: 3.46073925129417e-07, Loss: 0.2770557999610901\n",
            "Epoch 1023/2000, Learning Rate: 3.4261318587812285e-07, Loss: 0.02176268771290779\n",
            "Epoch 1024/2000, Learning Rate: 3.3918705401934164e-07, Loss: 0.07676572352647781\n",
            "Epoch 1025/2000, Learning Rate: 3.3579518347914825e-07, Loss: 0.005160276312381029\n",
            "Epoch 1026/2000, Learning Rate: 3.3243723164435675e-07, Loss: 0.08357901871204376\n",
            "Epoch 1027/2000, Learning Rate: 3.291128593279132e-07, Loss: 0.05218500643968582\n",
            "Epoch 1028/2000, Learning Rate: 3.258217307346341e-07, Loss: 0.030105996876955032\n",
            "Epoch 1029/2000, Learning Rate: 3.225635134272877e-07, Loss: 0.058062709867954254\n",
            "Epoch 1030/2000, Learning Rate: 3.1933787829301483e-07, Loss: 0.004006994888186455\n",
            "Epoch 1031/2000, Learning Rate: 3.1614449951008466e-07, Loss: 0.006368630565702915\n",
            "Epoch 1032/2000, Learning Rate: 3.129830545149838e-07, Loss: 0.08953752368688583\n",
            "Epoch 1033/2000, Learning Rate: 3.0985322396983396e-07, Loss: 0.05555323138833046\n",
            "Epoch 1034/2000, Learning Rate: 3.067546917301356e-07, Loss: 0.13401615619659424\n",
            "Epoch 1035/2000, Learning Rate: 3.0368714481283426e-07, Loss: 0.07232850044965744\n",
            "Epoch 1036/2000, Learning Rate: 3.0065027336470594e-07, Loss: 0.004523953422904015\n",
            "Epoch 1037/2000, Learning Rate: 2.976437706310589e-07, Loss: 0.07788188755512238\n",
            "Epoch 1038/2000, Learning Rate: 2.946673329247483e-07, Loss: 0.13088297843933105\n",
            "Epoch 1039/2000, Learning Rate: 2.917206595955008e-07, Loss: 0.060975492000579834\n",
            "Epoch 1040/2000, Learning Rate: 2.888034529995458e-07, Loss: 0.15519589185714722\n",
            "Epoch 1041/2000, Learning Rate: 2.8591541846955034e-07, Loss: 0.1612256020307541\n",
            "Epoch 1042/2000, Learning Rate: 2.8305626428485485e-07, Loss: 0.04365064948797226\n",
            "Epoch 1043/2000, Learning Rate: 2.802257016420063e-07, Loss: 0.034108005464076996\n",
            "Epoch 1044/2000, Learning Rate: 2.774234446255862e-07, Loss: 0.07316911965608597\n",
            "Epoch 1045/2000, Learning Rate: 2.7464921017933036e-07, Loss: 0.059730395674705505\n",
            "Epoch 1046/2000, Learning Rate: 2.7190271807753703e-07, Loss: 0.1651603728532791\n",
            "Epoch 1047/2000, Learning Rate: 2.6918369089676164e-07, Loss: 0.07074178755283356\n",
            "Epoch 1048/2000, Learning Rate: 2.6649185398779404e-07, Loss: 0.025850072503089905\n",
            "Epoch 1049/2000, Learning Rate: 2.638269354479161e-07, Loss: 0.0039855800569057465\n",
            "Epoch 1050/2000, Learning Rate: 2.6118866609343695e-07, Loss: 0.05048855021595955\n",
            "Epoch 1051/2000, Learning Rate: 2.585767794325026e-07, Loss: 0.033497631549835205\n",
            "Epoch 1052/2000, Learning Rate: 2.5599101163817756e-07, Loss: 0.05194034427404404\n",
            "Epoch 1053/2000, Learning Rate: 2.534311015217958e-07, Loss: 0.07825301587581635\n",
            "Epoch 1054/2000, Learning Rate: 2.508967905065778e-07, Loss: 0.05909892916679382\n",
            "Epoch 1055/2000, Learning Rate: 2.4838782260151203e-07, Loss: 0.10544715076684952\n",
            "Epoch 1056/2000, Learning Rate: 2.459039443754969e-07, Loss: 0.01429013255983591\n",
            "Epoch 1057/2000, Learning Rate: 2.434449049317419e-07, Loss: 0.18807117640972137\n",
            "Epoch 1058/2000, Learning Rate: 2.410104558824245e-07, Loss: 0.013115453533828259\n",
            "Epoch 1059/2000, Learning Rate: 2.3860035132360024e-07, Loss: 0.11000461131334305\n",
            "Epoch 1060/2000, Learning Rate: 2.3621434781036424e-07, Loss: 0.07659858465194702\n",
            "Epoch 1061/2000, Learning Rate: 2.338522043322606e-07, Loss: 0.11590061336755753\n",
            "Epoch 1062/2000, Learning Rate: 2.31513682288938e-07, Loss: 0.015992583706974983\n",
            "Epoch 1063/2000, Learning Rate: 2.2919854546604862e-07, Loss: 0.10223715007305145\n",
            "Epoch 1064/2000, Learning Rate: 2.2690656001138815e-07, Loss: 0.014580736868083477\n",
            "Epoch 1065/2000, Learning Rate: 2.2463749441127427e-07, Loss: 0.26728710532188416\n",
            "Epoch 1066/2000, Learning Rate: 2.2239111946716152e-07, Loss: 0.08595892786979675\n",
            "Epoch 1067/2000, Learning Rate: 2.201672082724899e-07, Loss: 0.017866237089037895\n",
            "Epoch 1068/2000, Learning Rate: 2.17965536189765e-07, Loss: 0.005667671095579863\n",
            "Epoch 1069/2000, Learning Rate: 2.1578588082786733e-07, Loss: 0.020521124824881554\n",
            "Epoch 1070/2000, Learning Rate: 2.1362802201958866e-07, Loss: 0.03908121585845947\n",
            "Epoch 1071/2000, Learning Rate: 2.1149174179939276e-07, Loss: 0.027230631560087204\n",
            "Epoch 1072/2000, Learning Rate: 2.0937682438139883e-07, Loss: 0.049665067344903946\n",
            "Epoch 1073/2000, Learning Rate: 2.0728305613758484e-07, Loss: 0.016791829839348793\n",
            "Epoch 1074/2000, Learning Rate: 2.05210225576209e-07, Loss: 0.06717503815889359\n",
            "Epoch 1075/2000, Learning Rate: 2.031581233204469e-07, Loss: 0.014734397642314434\n",
            "Epoch 1076/2000, Learning Rate: 2.0112654208724244e-07, Loss: 0.031900107860565186\n",
            "Epoch 1077/2000, Learning Rate: 1.9911527666637002e-07, Loss: 0.14541290700435638\n",
            "Epoch 1078/2000, Learning Rate: 1.971241238997063e-07, Loss: 0.011579575017094612\n",
            "Epoch 1079/2000, Learning Rate: 1.9515288266070925e-07, Loss: 0.055413618683815\n",
            "Epoch 1080/2000, Learning Rate: 1.9320135383410216e-07, Loss: 0.10409150272607803\n",
            "Epoch 1081/2000, Learning Rate: 1.9126934029576115e-07, Loss: 0.011283869855105877\n",
            "Epoch 1082/2000, Learning Rate: 1.8935664689280354e-07, Loss: 0.0749112144112587\n",
            "Epoch 1083/2000, Learning Rate: 1.874630804238755e-07, Loss: 0.005154200829565525\n",
            "Epoch 1084/2000, Learning Rate: 1.8558844961963673e-07, Loss: 0.06824091076850891\n",
            "Epoch 1085/2000, Learning Rate: 1.8373256512344037e-07, Loss: 0.09368392080068588\n",
            "Epoch 1086/2000, Learning Rate: 1.8189523947220596e-07, Loss: 0.005925331264734268\n",
            "Epoch 1087/2000, Learning Rate: 1.800762870774839e-07, Loss: 0.1727079600095749\n",
            "Epoch 1088/2000, Learning Rate: 1.7827552420670904e-07, Loss: 0.014492286369204521\n",
            "Epoch 1089/2000, Learning Rate: 1.7649276896464196e-07, Loss: 0.01788994111120701\n",
            "Epoch 1090/2000, Learning Rate: 1.7472784127499553e-07, Loss: 0.04216787964105606\n",
            "Epoch 1091/2000, Learning Rate: 1.7298056286224556e-07, Loss: 0.038715023547410965\n",
            "Epoch 1092/2000, Learning Rate: 1.712507572336231e-07, Loss: 0.01886574737727642\n",
            "Epoch 1093/2000, Learning Rate: 1.6953824966128685e-07, Loss: 0.04157883673906326\n",
            "Epoch 1094/2000, Learning Rate: 1.6784286716467397e-07, Loss: 0.056488748639822006\n",
            "Epoch 1095/2000, Learning Rate: 1.6616443849302723e-07, Loss: 0.15470078587532043\n",
            "Epoch 1096/2000, Learning Rate: 1.6450279410809696e-07, Loss: 0.08022245764732361\n",
            "Epoch 1097/2000, Learning Rate: 1.6285776616701598e-07, Loss: 0.00820197444409132\n",
            "Epoch 1098/2000, Learning Rate: 1.6122918850534583e-07, Loss: 0.08896739780902863\n",
            "Epoch 1099/2000, Learning Rate: 1.5961689662029238e-07, Loss: 0.1704569011926651\n",
            "Epoch 1100/2000, Learning Rate: 1.5802072765408945e-07, Loss: 0.011207071132957935\n",
            "Epoch 1101/2000, Learning Rate: 1.5644052037754855e-07, Loss: 0.07958962768316269\n",
            "Epoch 1102/2000, Learning Rate: 1.5487611517377305e-07, Loss: 0.07441675662994385\n",
            "Epoch 1103/2000, Learning Rate: 1.5332735402203532e-07, Loss: 0.06986195594072342\n",
            "Epoch 1104/2000, Learning Rate: 1.5179408048181496e-07, Loss: 0.018559157848358154\n",
            "Epoch 1105/2000, Learning Rate: 1.5027613967699682e-07, Loss: 0.07930532842874527\n",
            "Epoch 1106/2000, Learning Rate: 1.4877337828022685e-07, Loss: 0.060839276760816574\n",
            "Epoch 1107/2000, Learning Rate: 1.472856444974246e-07, Loss: 0.11451280117034912\n",
            "Epoch 1108/2000, Learning Rate: 1.4581278805245035e-07, Loss: 0.017931753769516945\n",
            "Epoch 1109/2000, Learning Rate: 1.4435466017192586e-07, Loss: 0.006356198340654373\n",
            "Epoch 1110/2000, Learning Rate: 1.429111135702066e-07, Loss: 0.07073824107646942\n",
            "Epoch 1111/2000, Learning Rate: 1.4148200243450453e-07, Loss: 0.005640700459480286\n",
            "Epoch 1112/2000, Learning Rate: 1.4006718241015948e-07, Loss: 0.05408396199345589\n",
            "Epoch 1113/2000, Learning Rate: 1.386665105860579e-07, Loss: 0.016217004507780075\n",
            "Epoch 1114/2000, Learning Rate: 1.3727984548019731e-07, Loss: 0.01838800311088562\n",
            "Epoch 1115/2000, Learning Rate: 1.3590704702539533e-07, Loss: 0.019263364374637604\n",
            "Epoch 1116/2000, Learning Rate: 1.3454797655514138e-07, Loss: 0.12380366027355194\n",
            "Epoch 1117/2000, Learning Rate: 1.3320249678958998e-07, Loss: 0.06496427953243256\n",
            "Epoch 1118/2000, Learning Rate: 1.3187047182169408e-07, Loss: 0.0326414592564106\n",
            "Epoch 1119/2000, Learning Rate: 1.3055176710347714e-07, Loss: 0.003302808152511716\n",
            "Epoch 1120/2000, Learning Rate: 1.2924624943244237e-07, Loss: 0.10801473259925842\n",
            "Epoch 1121/2000, Learning Rate: 1.2795378693811795e-07, Loss: 0.013844989240169525\n",
            "Epoch 1122/2000, Learning Rate: 1.2667424906873677e-07, Loss: 0.009698375128209591\n",
            "Epoch 1123/2000, Learning Rate: 1.254075065780494e-07, Loss: 0.12012018263339996\n",
            "Epoch 1124/2000, Learning Rate: 1.241534315122689e-07, Loss: 0.012405362911522388\n",
            "Epoch 1125/2000, Learning Rate: 1.229118971971462e-07, Loss: 0.02251763641834259\n",
            "Epoch 1126/2000, Learning Rate: 1.2168277822517474e-07, Loss: 0.009948615916073322\n",
            "Epoch 1127/2000, Learning Rate: 1.20465950442923e-07, Loss: 0.03899993747472763\n",
            "Epoch 1128/2000, Learning Rate: 1.1926129093849376e-07, Loss: 0.08517719060182571\n",
            "Epoch 1129/2000, Learning Rate: 1.1806867802910882e-07, Loss: 0.08534126728773117\n",
            "Epoch 1130/2000, Learning Rate: 1.1688799124881773e-07, Loss: 0.014062629081308842\n",
            "Epoch 1131/2000, Learning Rate: 1.1571911133632956e-07, Loss: 0.05867689847946167\n",
            "Epoch 1132/2000, Learning Rate: 1.1456192022296626e-07, Loss: 0.06499295681715012\n",
            "Epoch 1133/2000, Learning Rate: 1.1341630102073659e-07, Loss: 0.02386428602039814\n",
            "Epoch 1134/2000, Learning Rate: 1.1228213801052923e-07, Loss: 0.02292747050523758\n",
            "Epoch 1135/2000, Learning Rate: 1.1115931663042393e-07, Loss: 0.0798477828502655\n",
            "Epoch 1136/2000, Learning Rate: 1.100477234641197e-07, Loss: 0.12563009560108185\n",
            "Epoch 1137/2000, Learning Rate: 1.0894724622947849e-07, Loss: 0.047627948224544525\n",
            "Epoch 1138/2000, Learning Rate: 1.078577737671837e-07, Loss: 0.049794211983680725\n",
            "Epoch 1139/2000, Learning Rate: 1.0677919602951186e-07, Loss: 0.11863639950752258\n",
            "Epoch 1140/2000, Learning Rate: 1.0571140406921674e-07, Loss: 0.006921716965734959\n",
            "Epoch 1141/2000, Learning Rate: 1.0465429002852456e-07, Loss: 0.02511603757739067\n",
            "Epoch 1142/2000, Learning Rate: 1.0360774712823931e-07, Loss: 0.009095351211726665\n",
            "Epoch 1143/2000, Learning Rate: 1.0257166965695692e-07, Loss: 0.13976936042308807\n",
            "Epoch 1144/2000, Learning Rate: 1.0154595296038734e-07, Loss: 0.04055814445018768\n",
            "Epoch 1145/2000, Learning Rate: 1.0053049343078346e-07, Loss: 0.14938265085220337\n",
            "Epoch 1146/2000, Learning Rate: 9.952518849647562e-08, Loss: 0.017788605764508247\n",
            "Epoch 1147/2000, Learning Rate: 9.852993661151087e-08, Loss: 0.13469354808330536\n",
            "Epoch 1148/2000, Learning Rate: 9.754463724539577e-08, Loss: 0.011346029117703438\n",
            "Epoch 1149/2000, Learning Rate: 9.65691908729418e-08, Loss: 0.13886719942092896\n",
            "Epoch 1150/2000, Learning Rate: 9.560349896421238e-08, Loss: 0.08182083815336227\n",
            "Epoch 1151/2000, Learning Rate: 9.464746397457025e-08, Loss: 0.02440546080470085\n",
            "Epoch 1152/2000, Learning Rate: 9.370098933482454e-08, Loss: 0.030208559706807137\n",
            "Epoch 1153/2000, Learning Rate: 9.27639794414763e-08, Loss: 0.5167912244796753\n",
            "Epoch 1154/2000, Learning Rate: 9.183633964706153e-08, Loss: 0.03422743082046509\n",
            "Epoch 1155/2000, Learning Rate: 9.091797625059091e-08, Loss: 0.119867242872715\n",
            "Epoch 1156/2000, Learning Rate: 9.0008796488085e-08, Loss: 0.14299696683883667\n",
            "Epoch 1157/2000, Learning Rate: 8.910870852320415e-08, Loss: 0.11619873344898224\n",
            "Epoch 1158/2000, Learning Rate: 8.82176214379721e-08, Loss: 0.14726056158542633\n",
            "Epoch 1159/2000, Learning Rate: 8.733544522359238e-08, Loss: 0.01859973557293415\n",
            "Epoch 1160/2000, Learning Rate: 8.646209077135645e-08, Loss: 0.08692914992570877\n",
            "Epoch 1161/2000, Learning Rate: 8.559746986364288e-08, Loss: 0.15422983467578888\n",
            "Epoch 1162/2000, Learning Rate: 8.474149516500645e-08, Loss: 0.029960162937641144\n",
            "Epoch 1163/2000, Learning Rate: 8.389408021335639e-08, Loss: 0.07436995953321457\n",
            "Epoch 1164/2000, Learning Rate: 8.305513941122282e-08, Loss: 0.011487701907753944\n",
            "Epoch 1165/2000, Learning Rate: 8.222458801711059e-08, Loss: 0.09512373059988022\n",
            "Epoch 1166/2000, Learning Rate: 8.140234213693949e-08, Loss: 0.039203494787216187\n",
            "Epoch 1167/2000, Learning Rate: 8.05883187155701e-08, Loss: 0.05958601459860802\n",
            "Epoch 1168/2000, Learning Rate: 7.978243552841439e-08, Loss: 0.1123816967010498\n",
            "Epoch 1169/2000, Learning Rate: 7.898461117313025e-08, Loss: 0.1976952999830246\n",
            "Epoch 1170/2000, Learning Rate: 7.819476506139894e-08, Loss: 0.012825936079025269\n",
            "Epoch 1171/2000, Learning Rate: 7.741281741078495e-08, Loss: 0.045302391052246094\n",
            "Epoch 1172/2000, Learning Rate: 7.66386892366771e-08, Loss: 0.030853085219860077\n",
            "Epoch 1173/2000, Learning Rate: 7.587230234431032e-08, Loss: 0.12906500697135925\n",
            "Epoch 1174/2000, Learning Rate: 7.511357932086721e-08, Loss: 0.022986501455307007\n",
            "Epoch 1175/2000, Learning Rate: 7.436244352765855e-08, Loss: 0.07247291505336761\n",
            "Epoch 1176/2000, Learning Rate: 7.361881909238196e-08, Loss: 0.05433109030127525\n",
            "Epoch 1177/2000, Learning Rate: 7.288263090145815e-08, Loss: 0.03751666098833084\n",
            "Epoch 1178/2000, Learning Rate: 7.215380459244357e-08, Loss: 0.08442328125238419\n",
            "Epoch 1179/2000, Learning Rate: 7.143226654651913e-08, Loss: 0.04679904878139496\n",
            "Epoch 1180/2000, Learning Rate: 7.071794388105394e-08, Loss: 0.013410281389951706\n",
            "Epoch 1181/2000, Learning Rate: 7.00107644422434e-08, Loss: 0.009649931453168392\n",
            "Epoch 1182/2000, Learning Rate: 6.931065679782097e-08, Loss: 0.014153329655528069\n",
            "Epoch 1183/2000, Learning Rate: 6.861755022984276e-08, Loss: 0.08440788835287094\n",
            "Epoch 1184/2000, Learning Rate: 6.793137472754433e-08, Loss: 0.024298768490552902\n",
            "Epoch 1185/2000, Learning Rate: 6.725206098026889e-08, Loss: 0.02562466822564602\n",
            "Epoch 1186/2000, Learning Rate: 6.657954037046621e-08, Loss: 0.12381266802549362\n",
            "Epoch 1187/2000, Learning Rate: 6.591374496676155e-08, Loss: 0.012158935889601707\n",
            "Epoch 1188/2000, Learning Rate: 6.525460751709394e-08, Loss: 0.13872766494750977\n",
            "Epoch 1189/2000, Learning Rate: 6.4602061441923e-08, Loss: 0.05220425873994827\n",
            "Epoch 1190/2000, Learning Rate: 6.395604082750376e-08, Loss: 0.08664283901453018\n",
            "Epoch 1191/2000, Learning Rate: 6.331648041922873e-08, Loss: 0.045862458646297455\n",
            "Epoch 1192/2000, Learning Rate: 6.268331561503644e-08, Loss: 0.08699946850538254\n",
            "Epoch 1193/2000, Learning Rate: 6.205648245888608e-08, Loss: 0.1037512794137001\n",
            "Epoch 1194/2000, Learning Rate: 6.143591763429722e-08, Loss: 0.1303451806306839\n",
            "Epoch 1195/2000, Learning Rate: 6.082155845795425e-08, Loss: 0.004767880775034428\n",
            "Epoch 1196/2000, Learning Rate: 6.021334287337471e-08, Loss: 0.04576687887310982\n",
            "Epoch 1197/2000, Learning Rate: 5.961120944464097e-08, Loss: 0.03246767818927765\n",
            "Epoch 1198/2000, Learning Rate: 5.901509735019456e-08, Loss: 0.014003681018948555\n",
            "Epoch 1199/2000, Learning Rate: 5.8424946376692616e-08, Loss: 0.2222180962562561\n",
            "Epoch 1200/2000, Learning Rate: 5.784069691292569e-08, Loss: 0.03427284210920334\n",
            "Epoch 1201/2000, Learning Rate: 5.726228994379643e-08, Loss: 0.17806538939476013\n",
            "Epoch 1202/2000, Learning Rate: 5.668966704435847e-08, Loss: 0.04862236604094505\n",
            "Epoch 1203/2000, Learning Rate: 5.6122770373914884e-08, Loss: 0.019042737782001495\n",
            "Epoch 1204/2000, Learning Rate: 5.5561542670175735e-08, Loss: 0.00253201462328434\n",
            "Epoch 1205/2000, Learning Rate: 5.500592724347398e-08, Loss: 0.10296332091093063\n",
            "Epoch 1206/2000, Learning Rate: 5.445586797103924e-08, Loss: 0.022292986512184143\n",
            "Epoch 1207/2000, Learning Rate: 5.391130929132885e-08, Loss: 0.07884599268436432\n",
            "Epoch 1208/2000, Learning Rate: 5.337219619841556e-08, Loss: 0.11185608804225922\n",
            "Epoch 1209/2000, Learning Rate: 5.2838474236431405e-08, Loss: 0.0068665482103824615\n",
            "Epoch 1210/2000, Learning Rate: 5.2310089494067094e-08, Loss: 0.052480753511190414\n",
            "Epoch 1211/2000, Learning Rate: 5.178698859912642e-08, Loss: 0.0072793373838067055\n",
            "Epoch 1212/2000, Learning Rate: 5.1269118713135155e-08, Loss: 0.01351278368383646\n",
            "Epoch 1213/2000, Learning Rate: 5.07564275260038e-08, Loss: 0.02614648826420307\n",
            "Epoch 1214/2000, Learning Rate: 5.0248863250743764e-08, Loss: 0.058806248009204865\n",
            "Epoch 1215/2000, Learning Rate: 4.9746374618236326e-08, Loss: 0.07964836061000824\n",
            "Epoch 1216/2000, Learning Rate: 4.924891087205396e-08, Loss: 0.00413557467982173\n",
            "Epoch 1217/2000, Learning Rate: 4.875642176333342e-08, Loss: 0.0749838650226593\n",
            "Epoch 1218/2000, Learning Rate: 4.826885754570009e-08, Loss: 0.13216473162174225\n",
            "Epoch 1219/2000, Learning Rate: 4.7786168970243084e-08, Loss: 0.08322618156671524\n",
            "Epoch 1220/2000, Learning Rate: 4.730830728054065e-08, Loss: 0.04465721547603607\n",
            "Epoch 1221/2000, Learning Rate: 4.683522420773525e-08, Loss: 0.011873177252709866\n",
            "Epoch 1222/2000, Learning Rate: 4.6366871965657896e-08, Loss: 0.022587323561310768\n",
            "Epoch 1223/2000, Learning Rate: 4.5903203246001317e-08, Loss: 0.014402654021978378\n",
            "Epoch 1224/2000, Learning Rate: 4.5444171213541305e-08, Loss: 0.23941725492477417\n",
            "Epoch 1225/2000, Learning Rate: 4.498972950140589e-08, Loss: 0.06670701503753662\n",
            "Epoch 1226/2000, Learning Rate: 4.4539832206391826e-08, Loss: 0.0333983413875103\n",
            "Epoch 1227/2000, Learning Rate: 4.409443388432791e-08, Loss: 0.011486712843179703\n",
            "Epoch 1228/2000, Learning Rate: 4.3653489545484626e-08, Loss: 0.10038843750953674\n",
            "Epoch 1229/2000, Learning Rate: 4.321695465002978e-08, Loss: 0.07370515912771225\n",
            "Epoch 1230/2000, Learning Rate: 4.2784785103529485e-08, Loss: 0.012554408051073551\n",
            "Epoch 1231/2000, Learning Rate: 4.235693725249419e-08, Loss: 0.05255831405520439\n",
            "Epoch 1232/2000, Learning Rate: 4.193336787996925e-08, Loss: 0.01147917378693819\n",
            "Epoch 1233/2000, Learning Rate: 4.151403420116956e-08, Loss: 0.023531697690486908\n",
            "Epoch 1234/2000, Learning Rate: 4.1098893859157865e-08, Loss: 0.0107637420296669\n",
            "Epoch 1235/2000, Learning Rate: 4.068790492056629e-08, Loss: 0.10958578437566757\n",
            "Epoch 1236/2000, Learning Rate: 4.028102587136063e-08, Loss: 0.03661284223198891\n",
            "Epoch 1237/2000, Learning Rate: 3.987821561264702e-08, Loss: 0.0777025893330574\n",
            "Epoch 1238/2000, Learning Rate: 3.947943345652055e-08, Loss: 0.1501469910144806\n",
            "Epoch 1239/2000, Learning Rate: 3.9084639121955343e-08, Loss: 0.10866810381412506\n",
            "Epoch 1240/2000, Learning Rate: 3.869379273073579e-08, Loss: 0.00609522545710206\n",
            "Epoch 1241/2000, Learning Rate: 3.830685480342843e-08, Loss: 0.2688981890678406\n",
            "Epoch 1242/2000, Learning Rate: 3.7923786255394146e-08, Loss: 0.01619676873087883\n",
            "Epoch 1243/2000, Learning Rate: 3.75445483928402e-08, Loss: 0.05171186104416847\n",
            "Epoch 1244/2000, Learning Rate: 3.71691029089118e-08, Loss: 0.055194538086652756\n",
            "Epoch 1245/2000, Learning Rate: 3.679741187982268e-08, Loss: 0.2543798089027405\n",
            "Epoch 1246/2000, Learning Rate: 3.642943776102445e-08, Loss: 0.22765576839447021\n",
            "Epoch 1247/2000, Learning Rate: 3.606514338341421e-08, Loss: 0.09934212267398834\n",
            "Epoch 1248/2000, Learning Rate: 3.570449194958007e-08, Loss: 0.0067572640255093575\n",
            "Epoch 1249/2000, Learning Rate: 3.534744703008427e-08, Loss: 0.08167726546525955\n",
            "Epoch 1250/2000, Learning Rate: 3.499397255978342e-08, Loss: 0.013781452551484108\n",
            "Epoch 1251/2000, Learning Rate: 3.464403283418559e-08, Loss: 0.029706643894314766\n",
            "Epoch 1252/2000, Learning Rate: 3.429759250584373e-08, Loss: 0.21933767199516296\n",
            "Epoch 1253/2000, Learning Rate: 3.395461658078529e-08, Loss: 0.18049800395965576\n",
            "Epoch 1254/2000, Learning Rate: 3.361507041497744e-08, Loss: 0.05819306895136833\n",
            "Epoch 1255/2000, Learning Rate: 3.327891971082766e-08, Loss: 0.007970884442329407\n",
            "Epoch 1256/2000, Learning Rate: 3.2946130513719386e-08, Loss: 0.06476175040006638\n",
            "Epoch 1257/2000, Learning Rate: 3.261666920858219e-08, Loss: 0.026086881756782532\n",
            "Epoch 1258/2000, Learning Rate: 3.2290502516496367e-08, Loss: 0.07343248277902603\n",
            "Epoch 1259/2000, Learning Rate: 3.19675974913314e-08, Loss: 0.11407779902219772\n",
            "Epoch 1260/2000, Learning Rate: 3.1647921516418086e-08, Loss: 0.006460808217525482\n",
            "Epoch 1261/2000, Learning Rate: 3.1331442301253905e-08, Loss: 0.05941515788435936\n",
            "Epoch 1262/2000, Learning Rate: 3.1018127878241364e-08, Loss: 0.029148459434509277\n",
            "Epoch 1263/2000, Learning Rate: 3.070794659945895e-08, Loss: 0.084908626973629\n",
            "Epoch 1264/2000, Learning Rate: 3.040086713346436e-08, Loss: 0.14386790990829468\n",
            "Epoch 1265/2000, Learning Rate: 3.009685846212971e-08, Loss: 0.03379128873348236\n",
            "Epoch 1266/2000, Learning Rate: 2.9795889877508414e-08, Loss: 0.052797190845012665\n",
            "Epoch 1267/2000, Learning Rate: 2.949793097873333e-08, Loss: 0.092654287815094\n",
            "Epoch 1268/2000, Learning Rate: 2.9202951668945998e-08, Loss: 0.09608496725559235\n",
            "Epoch 1269/2000, Learning Rate: 2.891092215225654e-08, Loss: 0.01790737546980381\n",
            "Epoch 1270/2000, Learning Rate: 2.8621812930733973e-08, Loss: 0.10354936122894287\n",
            "Epoch 1271/2000, Learning Rate: 2.8335594801426635e-08, Loss: 0.018535194918513298\n",
            "Epoch 1272/2000, Learning Rate: 2.8052238853412368e-08, Loss: 0.013729668222367764\n",
            "Epoch 1273/2000, Learning Rate: 2.7771716464878245e-08, Loss: 0.07120010256767273\n",
            "Epoch 1274/2000, Learning Rate: 2.7493999300229463e-08, Loss: 0.006647142115980387\n",
            "Epoch 1275/2000, Learning Rate: 2.7219059307227167e-08, Loss: 0.02682376466691494\n",
            "Epoch 1276/2000, Learning Rate: 2.6946868714154896e-08, Loss: 0.04207879304885864\n",
            "Epoch 1277/2000, Learning Rate: 2.6677400027013346e-08, Loss: 0.00411610584706068\n",
            "Epoch 1278/2000, Learning Rate: 2.6410626026743213e-08, Loss: 0.09393481910228729\n",
            "Epoch 1279/2000, Learning Rate: 2.614651976647578e-08, Loss: 0.02632412314414978\n",
            "Epoch 1280/2000, Learning Rate: 2.5885054568811023e-08, Loss: 0.051593951880931854\n",
            "Epoch 1281/2000, Learning Rate: 2.5626204023122914e-08, Loss: 0.07491829991340637\n",
            "Epoch 1282/2000, Learning Rate: 2.5369941982891684e-08, Loss: 0.1431158035993576\n",
            "Epoch 1283/2000, Learning Rate: 2.511624256306277e-08, Loss: 0.12998756766319275\n",
            "Epoch 1284/2000, Learning Rate: 2.486508013743214e-08, Loss: 0.041501179337501526\n",
            "Epoch 1285/2000, Learning Rate: 2.461642933605782e-08, Loss: 0.041041940450668335\n",
            "Epoch 1286/2000, Learning Rate: 2.437026504269724e-08, Loss: 0.011120475828647614\n",
            "Epoch 1287/2000, Learning Rate: 2.412656239227027e-08, Loss: 0.018738389015197754\n",
            "Epoch 1288/2000, Learning Rate: 2.3885296768347567e-08, Loss: 0.02184154838323593\n",
            "Epoch 1289/2000, Learning Rate: 2.3646443800664092e-08, Loss: 0.033614810556173325\n",
            "Epoch 1290/2000, Learning Rate: 2.340997936265745e-08, Loss: 0.018035028129816055\n",
            "Epoch 1291/2000, Learning Rate: 2.3175879569030874e-08, Loss: 0.025147536769509315\n",
            "Epoch 1292/2000, Learning Rate: 2.2944120773340563e-08, Loss: 0.01393052563071251\n",
            "Epoch 1293/2000, Learning Rate: 2.2714679565607156e-08, Loss: 0.0068392204120755196\n",
            "Epoch 1294/2000, Learning Rate: 2.2487532769951085e-08, Loss: 0.02390441671013832\n",
            "Epoch 1295/2000, Learning Rate: 2.2262657442251574e-08, Loss: 0.019840050488710403\n",
            "Epoch 1296/2000, Learning Rate: 2.204003086782906e-08, Loss: 0.08333700150251389\n",
            "Epoch 1297/2000, Learning Rate: 2.1819630559150768e-08, Loss: 0.017208881676197052\n",
            "Epoch 1298/2000, Learning Rate: 2.1601434253559262e-08, Loss: 0.027380112558603287\n",
            "Epoch 1299/2000, Learning Rate: 2.138541991102367e-08, Loss: 0.10023655742406845\n",
            "Epoch 1300/2000, Learning Rate: 2.1171565711913434e-08, Loss: 0.00618453323841095\n",
            "Epoch 1301/2000, Learning Rate: 2.09598500547943e-08, Loss: 0.1268547922372818\n",
            "Epoch 1302/2000, Learning Rate: 2.0750251554246355e-08, Loss: 0.19523301720619202\n",
            "Epoch 1303/2000, Learning Rate: 2.054274903870389e-08, Loss: 0.05863393843173981\n",
            "Epoch 1304/2000, Learning Rate: 2.033732154831685e-08, Loss: 0.09616328030824661\n",
            "Epoch 1305/2000, Learning Rate: 2.0133948332833682e-08, Loss: 0.030398571863770485\n",
            "Epoch 1306/2000, Learning Rate: 1.9932608849505345e-08, Loss: 0.03725457563996315\n",
            "Epoch 1307/2000, Learning Rate: 1.9733282761010292e-08, Loss: 0.005354909226298332\n",
            "Epoch 1308/2000, Learning Rate: 1.953594993340019e-08, Loss: 0.04413122311234474\n",
            "Epoch 1309/2000, Learning Rate: 1.9340590434066186e-08, Loss: 0.0838775709271431\n",
            "Epoch 1310/2000, Learning Rate: 1.9147184529725523e-08, Loss: 0.16428163647651672\n",
            "Epoch 1311/2000, Learning Rate: 1.895571268442827e-08, Loss: 0.020868150517344475\n",
            "Epoch 1312/2000, Learning Rate: 1.8766155557583985e-08, Loss: 0.1654062271118164\n",
            "Epoch 1313/2000, Learning Rate: 1.8578494002008144e-08, Loss: 0.10828651487827301\n",
            "Epoch 1314/2000, Learning Rate: 1.8392709061988063e-08, Loss: 0.018543332815170288\n",
            "Epoch 1315/2000, Learning Rate: 1.8208781971368183e-08, Loss: 0.018353626132011414\n",
            "Epoch 1316/2000, Learning Rate: 1.80266941516545e-08, Loss: 0.00487157516181469\n",
            "Epoch 1317/2000, Learning Rate: 1.7846427210137956e-08, Loss: 0.023745527490973473\n",
            "Epoch 1318/2000, Learning Rate: 1.7667962938036577e-08, Loss: 0.009886303916573524\n",
            "Epoch 1319/2000, Learning Rate: 1.749128330865621e-08, Loss: 0.01843908429145813\n",
            "Epoch 1320/2000, Learning Rate: 1.731637047556965e-08, Loss: 0.08380074054002762\n",
            "Epoch 1321/2000, Learning Rate: 1.714320677081395e-08, Loss: 0.007000827230513096\n",
            "Epoch 1322/2000, Learning Rate: 1.6971774703105812e-08, Loss: 0.04943988844752312\n",
            "Epoch 1323/2000, Learning Rate: 1.6802056956074753e-08, Loss: 0.06758220493793488\n",
            "Epoch 1324/2000, Learning Rate: 1.6634036386514005e-08, Loss: 0.012337230145931244\n",
            "Epoch 1325/2000, Learning Rate: 1.6467696022648865e-08, Loss: 0.02328253537416458\n",
            "Epoch 1326/2000, Learning Rate: 1.6303019062422378e-08, Loss: 0.11410003900527954\n",
            "Epoch 1327/2000, Learning Rate: 1.6139988871798153e-08, Loss: 0.007206500507891178\n",
            "Epoch 1328/2000, Learning Rate: 1.597858898308017e-08, Loss: 0.19288477301597595\n",
            "Epoch 1329/2000, Learning Rate: 1.581880309324937e-08, Loss: 0.06732350587844849\n",
            "Epoch 1330/2000, Learning Rate: 1.5660615062316876e-08, Loss: 0.17464375495910645\n",
            "Epoch 1331/2000, Learning Rate: 1.5504008911693706e-08, Loss: 0.12313143163919449\n",
            "Epoch 1332/2000, Learning Rate: 1.5348968822576767e-08, Loss: 0.21442006528377533\n",
            "Epoch 1333/2000, Learning Rate: 1.5195479134350998e-08, Loss: 0.07563533633947372\n",
            "Epoch 1334/2000, Learning Rate: 1.5043524343007487e-08, Loss: 0.25643202662467957\n",
            "Epoch 1335/2000, Learning Rate: 1.4893089099577412e-08, Loss: 0.051031190901994705\n",
            "Epoch 1336/2000, Learning Rate: 1.4744158208581637e-08, Loss: 0.09032987058162689\n",
            "Epoch 1337/2000, Learning Rate: 1.459671662649582e-08, Loss: 0.26709669828414917\n",
            "Epoch 1338/2000, Learning Rate: 1.4450749460230861e-08, Loss: 0.024750206619501114\n",
            "Epoch 1339/2000, Learning Rate: 1.4306241965628553e-08, Loss: 0.01867731288075447\n",
            "Epoch 1340/2000, Learning Rate: 1.4163179545972267e-08, Loss: 0.06953518837690353\n",
            "Epoch 1341/2000, Learning Rate: 1.4021547750512545e-08, Loss: 0.1899377852678299\n",
            "Epoch 1342/2000, Learning Rate: 1.388133227300742e-08, Loss: 0.010953743942081928\n",
            "Epoch 1343/2000, Learning Rate: 1.3742518950277344e-08, Loss: 0.012181860394775867\n",
            "Epoch 1344/2000, Learning Rate: 1.3605093760774571e-08, Loss: 0.0983225554227829\n",
            "Epoch 1345/2000, Learning Rate: 1.3469042823166826e-08, Loss: 0.09360038489103317\n",
            "Epoch 1346/2000, Learning Rate: 1.3334352394935157e-08, Loss: 0.0336415059864521\n",
            "Epoch 1347/2000, Learning Rate: 1.3201008870985806e-08, Loss: 0.20648033916950226\n",
            "Epoch 1348/2000, Learning Rate: 1.3068998782275947e-08, Loss: 0.11643289029598236\n",
            "Epoch 1349/2000, Learning Rate: 1.2938308794453187e-08, Loss: 0.06688515096902847\n",
            "Epoch 1350/2000, Learning Rate: 1.2808925706508656e-08, Loss: 0.18498937785625458\n",
            "Epoch 1351/2000, Learning Rate: 1.268083644944357e-08, Loss: 0.052923090755939484\n",
            "Epoch 1352/2000, Learning Rate: 1.2554028084949135e-08, Loss: 0.041027698665857315\n",
            "Epoch 1353/2000, Learning Rate: 1.2428487804099643e-08, Loss: 0.10905015468597412\n",
            "Epoch 1354/2000, Learning Rate: 1.2304202926058647e-08, Loss: 0.006287307478487492\n",
            "Epoch 1355/2000, Learning Rate: 1.218116089679806e-08, Loss: 0.011370918713510036\n",
            "Epoch 1356/2000, Learning Rate: 1.205934928783008e-08, Loss: 0.06132318824529648\n",
            "Epoch 1357/2000, Learning Rate: 1.193875579495178e-08, Loss: 0.4769750237464905\n",
            "Epoch 1358/2000, Learning Rate: 1.1819368237002262e-08, Loss: 0.1557844579219818\n",
            "Epoch 1359/2000, Learning Rate: 1.170117455463224e-08, Loss: 0.12155025452375412\n",
            "Epoch 1360/2000, Learning Rate: 1.1584162809085918e-08, Loss: 0.04693399369716644\n",
            "Epoch 1361/2000, Learning Rate: 1.1468321180995058e-08, Loss: 0.19045783579349518\n",
            "Epoch 1362/2000, Learning Rate: 1.1353637969185108e-08, Loss: 0.10924547910690308\n",
            "Epoch 1363/2000, Learning Rate: 1.1240101589493257e-08, Loss: 0.033101849257946014\n",
            "Epoch 1364/2000, Learning Rate: 1.1127700573598325e-08, Loss: 0.024762986227869987\n",
            "Epoch 1365/2000, Learning Rate: 1.1016423567862341e-08, Loss: 0.03410691022872925\n",
            "Epoch 1366/2000, Learning Rate: 1.0906259332183717e-08, Loss: 0.03816612437367439\n",
            "Epoch 1367/2000, Learning Rate: 1.0797196738861879e-08, Loss: 0.10099594295024872\n",
            "Epoch 1368/2000, Learning Rate: 1.068922477147326e-08, Loss: 0.1979253590106964\n",
            "Epoch 1369/2000, Learning Rate: 1.0582332523758527e-08, Loss: 0.049983613193035126\n",
            "Epoch 1370/2000, Learning Rate: 1.047650919852094e-08, Loss: 0.02584514021873474\n",
            "Epoch 1371/2000, Learning Rate: 1.037174410653573e-08, Loss: 0.01999536342918873\n",
            "Epoch 1372/2000, Learning Rate: 1.0268026665470373e-08, Loss: 0.007220621686428785\n",
            "Epoch 1373/2000, Learning Rate: 1.016534639881567e-08, Loss: 0.05279606208205223\n",
            "Epoch 1374/2000, Learning Rate: 1.0063692934827513e-08, Loss: 0.046868856996297836\n",
            "Epoch 1375/2000, Learning Rate: 9.963056005479237e-09, Loss: 0.0437212772667408\n",
            "Epoch 1376/2000, Learning Rate: 9.863425445424445e-09, Loss: 0.09915365278720856\n",
            "Epoch 1377/2000, Learning Rate: 9.7647911909702e-09, Loss: 0.010302236303687096\n",
            "Epoch 1378/2000, Learning Rate: 9.667143279060498e-09, Loss: 0.004717119038105011\n",
            "Epoch 1379/2000, Learning Rate: 9.570471846269893e-09, Loss: 0.03264392912387848\n",
            "Epoch 1380/2000, Learning Rate: 9.474767127807194e-09, Loss: 0.0425417535007\n",
            "Epoch 1381/2000, Learning Rate: 9.380019456529123e-09, Loss: 0.2170855700969696\n",
            "Epoch 1382/2000, Learning Rate: 9.286219261963832e-09, Loss: 0.09419737011194229\n",
            "Epoch 1383/2000, Learning Rate: 9.193357069344193e-09, Loss: 0.10779432952404022\n",
            "Epoch 1384/2000, Learning Rate: 9.101423498650752e-09, Loss: 0.11200613528490067\n",
            "Epoch 1385/2000, Learning Rate: 9.010409263664244e-09, Loss: 0.019776377826929092\n",
            "Epoch 1386/2000, Learning Rate: 8.920305171027601e-09, Loss: 0.01029620785266161\n",
            "Epoch 1387/2000, Learning Rate: 8.831102119317325e-09, Loss: 0.018180960789322853\n",
            "Epoch 1388/2000, Learning Rate: 8.742791098124152e-09, Loss: 0.07294653356075287\n",
            "Epoch 1389/2000, Learning Rate: 8.65536318714291e-09, Loss: 0.0821600928902626\n",
            "Epoch 1390/2000, Learning Rate: 8.568809555271482e-09, Loss: 0.03773713484406471\n",
            "Epoch 1391/2000, Learning Rate: 8.483121459718767e-09, Loss: 0.15774430334568024\n",
            "Epoch 1392/2000, Learning Rate: 8.398290245121579e-09, Loss: 0.03097536601126194\n",
            "Epoch 1393/2000, Learning Rate: 8.314307342670363e-09, Loss: 0.04063665494322777\n",
            "Epoch 1394/2000, Learning Rate: 8.231164269243658e-09, Loss: 0.011924626305699348\n",
            "Epoch 1395/2000, Learning Rate: 8.148852626551222e-09, Loss: 0.06843762844800949\n",
            "Epoch 1396/2000, Learning Rate: 8.06736410028571e-09, Loss: 0.01561415009200573\n",
            "Epoch 1397/2000, Learning Rate: 7.986690459282853e-09, Loss: 0.01872573420405388\n",
            "Epoch 1398/2000, Learning Rate: 7.906823554690025e-09, Loss: 0.032685697078704834\n",
            "Epoch 1399/2000, Learning Rate: 7.827755319143124e-09, Loss: 0.08849044144153595\n",
            "Epoch 1400/2000, Learning Rate: 7.749477765951693e-09, Loss: 0.021849259734153748\n",
            "Epoch 1401/2000, Learning Rate: 7.671982988292175e-09, Loss: 0.02902565337717533\n",
            "Epoch 1402/2000, Learning Rate: 7.595263158409254e-09, Loss: 0.06657597422599792\n",
            "Epoch 1403/2000, Learning Rate: 7.51931052682516e-09, Loss: 0.18526755273342133\n",
            "Epoch 1404/2000, Learning Rate: 7.444117421556909e-09, Loss: 0.0011783945374190807\n",
            "Epoch 1405/2000, Learning Rate: 7.36967624734134e-09, Loss: 0.06937265396118164\n",
            "Epoch 1406/2000, Learning Rate: 7.2959794848679265e-09, Loss: 0.03294530138373375\n",
            "Epoch 1407/2000, Learning Rate: 7.223019690019247e-09, Loss: 0.08626923710107803\n",
            "Epoch 1408/2000, Learning Rate: 7.150789493119055e-09, Loss: 0.022875187918543816\n",
            "Epoch 1409/2000, Learning Rate: 7.0792815981878645e-09, Loss: 0.2714555859565735\n",
            "Epoch 1410/2000, Learning Rate: 7.008488782205986e-09, Loss: 0.030538199469447136\n",
            "Epoch 1411/2000, Learning Rate: 6.938403894383926e-09, Loss: 0.02164485491812229\n",
            "Epoch 1412/2000, Learning Rate: 6.869019855440087e-09, Loss: 0.09404992312192917\n",
            "Epoch 1413/2000, Learning Rate: 6.800329656885686e-09, Loss: 0.07277185469865799\n",
            "Epoch 1414/2000, Learning Rate: 6.732326360316829e-09, Loss: 0.12147177755832672\n",
            "Epoch 1415/2000, Learning Rate: 6.665003096713661e-09, Loss: 0.010113147087395191\n",
            "Epoch 1416/2000, Learning Rate: 6.598353065746524e-09, Loss: 0.017306748777627945\n",
            "Epoch 1417/2000, Learning Rate: 6.532369535089059e-09, Loss: 0.02900087647140026\n",
            "Epoch 1418/2000, Learning Rate: 6.467045839738168e-09, Loss: 0.10893206298351288\n",
            "Epoch 1419/2000, Learning Rate: 6.402375381340787e-09, Loss: 0.0914720818400383\n",
            "Epoch 1420/2000, Learning Rate: 6.3383516275273795e-09, Loss: 0.01950214058160782\n",
            "Epoch 1421/2000, Learning Rate: 6.274968111252105e-09, Loss: 0.07138635963201523\n",
            "Epoch 1422/2000, Learning Rate: 6.212218430139584e-09, Loss: 0.10707104951143265\n",
            "Epoch 1423/2000, Learning Rate: 6.1500962458381885e-09, Loss: 0.15308666229248047\n",
            "Epoch 1424/2000, Learning Rate: 6.0885952833798065e-09, Loss: 0.05224063619971275\n",
            "Epoch 1425/2000, Learning Rate: 6.0277093305460085e-09, Loss: 0.16058267652988434\n",
            "Epoch 1426/2000, Learning Rate: 5.967432237240548e-09, Loss: 0.26560521125793457\n",
            "Epoch 1427/2000, Learning Rate: 5.907757914868143e-09, Loss: 0.04601893573999405\n",
            "Epoch 1428/2000, Learning Rate: 5.848680335719461e-09, Loss: 0.05367393419146538\n",
            "Epoch 1429/2000, Learning Rate: 5.7901935323622666e-09, Loss: 0.09888418763875961\n",
            "Epoch 1430/2000, Learning Rate: 5.732291597038644e-09, Loss: 0.08205819129943848\n",
            "Epoch 1431/2000, Learning Rate: 5.674968681068257e-09, Loss: 0.023671792820096016\n",
            "Epoch 1432/2000, Learning Rate: 5.618218994257575e-09, Loss: 0.009075693786144257\n",
            "Epoch 1433/2000, Learning Rate: 5.562036804314999e-09, Loss: 0.07408814877271652\n",
            "Epoch 1434/2000, Learning Rate: 5.506416436271849e-09, Loss: 0.08946926146745682\n",
            "Epoch 1435/2000, Learning Rate: 5.451352271909131e-09, Loss: 0.1354055404663086\n",
            "Epoch 1436/2000, Learning Rate: 5.39683874919004e-09, Loss: 0.028573518618941307\n",
            "Epoch 1437/2000, Learning Rate: 5.342870361698139e-09, Loss: 0.01293858326971531\n",
            "Epoch 1438/2000, Learning Rate: 5.289441658081158e-09, Loss: 0.036721743643283844\n",
            "Epoch 1439/2000, Learning Rate: 5.236547241500347e-09, Loss: 0.07912162691354752\n",
            "Epoch 1440/2000, Learning Rate: 5.184181769085343e-09, Loss: 0.0676090195775032\n",
            "Epoch 1441/2000, Learning Rate: 5.13233995139449e-09, Loss: 0.012950719334185123\n",
            "Epoch 1442/2000, Learning Rate: 5.081016551880545e-09, Loss: 0.16638872027397156\n",
            "Epoch 1443/2000, Learning Rate: 5.0302063863617395e-09, Loss: 0.029775965958833694\n",
            "Epoch 1444/2000, Learning Rate: 4.979904322498122e-09, Loss: 0.039524469524621964\n",
            "Epoch 1445/2000, Learning Rate: 4.930105279273141e-09, Loss: 0.025549262762069702\n",
            "Epoch 1446/2000, Learning Rate: 4.880804226480409e-09, Loss: 0.04050157591700554\n",
            "Epoch 1447/2000, Learning Rate: 4.831996184215605e-09, Loss: 0.06728022545576096\n",
            "Epoch 1448/2000, Learning Rate: 4.783676222373449e-09, Loss: 0.006588231772184372\n",
            "Epoch 1449/2000, Learning Rate: 4.735839460149715e-09, Loss: 0.003006245708093047\n",
            "Epoch 1450/2000, Learning Rate: 4.688481065548218e-09, Loss: 0.026495682075619698\n",
            "Epoch 1451/2000, Learning Rate: 4.641596254892735e-09, Loss: 0.1837235391139984\n",
            "Epoch 1452/2000, Learning Rate: 4.595180292343808e-09, Loss: 0.0961368978023529\n",
            "Epoch 1453/2000, Learning Rate: 4.54922848942037e-09, Loss: 0.17859946191310883\n",
            "Epoch 1454/2000, Learning Rate: 4.5037362045261665e-09, Loss: 0.0376681312918663\n",
            "Epoch 1455/2000, Learning Rate: 4.458698842480905e-09, Loss: 0.0547906868159771\n",
            "Epoch 1456/2000, Learning Rate: 4.414111854056096e-09, Loss: 0.11944548040628433\n",
            "Epoch 1457/2000, Learning Rate: 4.369970735515535e-09, Loss: 0.004868596792221069\n",
            "Epoch 1458/2000, Learning Rate: 4.326271028160379e-09, Loss: 0.011132410727441311\n",
            "Epoch 1459/2000, Learning Rate: 4.283008317878776e-09, Loss: 0.023454856127500534\n",
            "Epoch 1460/2000, Learning Rate: 4.240178234699988e-09, Loss: 0.09729928523302078\n",
            "Epoch 1461/2000, Learning Rate: 4.197776452352988e-09, Loss: 0.026876427233219147\n",
            "Epoch 1462/2000, Learning Rate: 4.155798687829458e-09, Loss: 0.08860552310943604\n",
            "Epoch 1463/2000, Learning Rate: 4.114240700951164e-09, Loss: 0.05625530332326889\n",
            "Epoch 1464/2000, Learning Rate: 4.073098293941652e-09, Loss: 0.020092159509658813\n",
            "Epoch 1465/2000, Learning Rate: 4.032367311002235e-09, Loss: 0.029496606439352036\n",
            "Epoch 1466/2000, Learning Rate: 3.9920436378922126e-09, Loss: 0.2806389331817627\n",
            "Epoch 1467/2000, Learning Rate: 3.9521232015132905e-09, Loss: 0.012806851416826248\n",
            "Epoch 1468/2000, Learning Rate: 3.912601969498157e-09, Loss: 0.09099423885345459\n",
            "Epoch 1469/2000, Learning Rate: 3.873475949803175e-09, Loss: 0.07401618361473083\n",
            "Epoch 1470/2000, Learning Rate: 3.834741190305143e-09, Loss: 0.07761134207248688\n",
            "Epoch 1471/2000, Learning Rate: 3.796393778402092e-09, Loss: 0.006199010647833347\n",
            "Epoch 1472/2000, Learning Rate: 3.758429840618071e-09, Loss: 0.008346484042704105\n",
            "Epoch 1473/2000, Learning Rate: 3.7208455422118905e-09, Loss: 0.09406237304210663\n",
            "Epoch 1474/2000, Learning Rate: 3.6836370867897716e-09, Loss: 0.12516088783740997\n",
            "Epoch 1475/2000, Learning Rate: 3.646800715921874e-09, Loss: 0.035823214799165726\n",
            "Epoch 1476/2000, Learning Rate: 3.6103327087626554e-09, Loss: 0.02587958239018917\n",
            "Epoch 1477/2000, Learning Rate: 3.574229381675029e-09, Loss: 0.020808663219213486\n",
            "Epoch 1478/2000, Learning Rate: 3.5384870878582783e-09, Loss: 0.2895260751247406\n",
            "Epoch 1479/2000, Learning Rate: 3.5031022169796956e-09, Loss: 0.06945357471704483\n",
            "Epoch 1480/2000, Learning Rate: 3.4680711948098986e-09, Loss: 0.04223204031586647\n",
            "Epoch 1481/2000, Learning Rate: 3.4333904828617996e-09, Loss: 0.015017155557870865\n",
            "Epoch 1482/2000, Learning Rate: 3.3990565780331818e-09, Loss: 0.0010727186454460025\n",
            "Epoch 1483/2000, Learning Rate: 3.36506601225285e-09, Loss: 0.06394177675247192\n",
            "Epoch 1484/2000, Learning Rate: 3.3314153521303216e-09, Loss: 0.07081322371959686\n",
            "Epoch 1485/2000, Learning Rate: 3.2981011986090184e-09, Loss: 0.016376076266169548\n",
            "Epoch 1486/2000, Learning Rate: 3.2651201866229282e-09, Loss: 0.137598916888237\n",
            "Epoch 1487/2000, Learning Rate: 3.232468984756699e-09, Loss: 0.07719983160495758\n",
            "Epoch 1488/2000, Learning Rate: 3.200144294909132e-09, Loss: 0.1253623366355896\n",
            "Epoch 1489/2000, Learning Rate: 3.1681428519600407e-09, Loss: 0.11109795421361923\n",
            "Epoch 1490/2000, Learning Rate: 3.1364614234404405e-09, Loss: 0.04319126158952713\n",
            "Epoch 1491/2000, Learning Rate: 3.105096809206036e-09, Loss: 0.08768080174922943\n",
            "Epoch 1492/2000, Learning Rate: 3.0740458411139755e-09, Loss: 0.053463056683540344\n",
            "Epoch 1493/2000, Learning Rate: 3.043305382702836e-09, Loss: 0.011502055451273918\n",
            "Epoch 1494/2000, Learning Rate: 3.0128723288758074e-09, Loss: 0.05634869635105133\n",
            "Epoch 1495/2000, Learning Rate: 2.982743605587049e-09, Loss: 0.02835826762020588\n",
            "Epoch 1496/2000, Learning Rate: 2.9529161695311787e-09, Loss: 0.03777001425623894\n",
            "Epoch 1497/2000, Learning Rate: 2.923387007835867e-09, Loss: 0.1398998349905014\n",
            "Epoch 1498/2000, Learning Rate: 2.8941531377575083e-09, Loss: 0.2021411955356598\n",
            "Epoch 1499/2000, Learning Rate: 2.865211606379933e-09, Loss: 0.2598211169242859\n",
            "Epoch 1500/2000, Learning Rate: 2.836559490316134e-09, Loss: 0.12217974662780762\n",
            "Epoch 1501/2000, Learning Rate: 2.8081938954129725e-09, Loss: 0.020758138969540596\n",
            "Epoch 1502/2000, Learning Rate: 2.7801119564588426e-09, Loss: 0.15761077404022217\n",
            "Epoch 1503/2000, Learning Rate: 2.752310836894254e-09, Loss: 0.06103140488266945\n",
            "Epoch 1504/2000, Learning Rate: 2.7247877285253116e-09, Loss: 0.16444340348243713\n",
            "Epoch 1505/2000, Learning Rate: 2.6975398512400584e-09, Loss: 0.02718583308160305\n",
            "Epoch 1506/2000, Learning Rate: 2.670564452727658e-09, Loss: 0.07721180468797684\n",
            "Epoch 1507/2000, Learning Rate: 2.643858808200381e-09, Loss: 0.27869972586631775\n",
            "Epoch 1508/2000, Learning Rate: 2.6174202201183774e-09, Loss: 0.016858378425240517\n",
            "Epoch 1509/2000, Learning Rate: 2.5912460179171937e-09, Loss: 0.059191204607486725\n",
            "Epoch 1510/2000, Learning Rate: 2.5653335577380217e-09, Loss: 0.04080403968691826\n",
            "Epoch 1511/2000, Learning Rate: 2.5396802221606415e-09, Loss: 0.19329611957073212\n",
            "Epoch 1512/2000, Learning Rate: 2.514283419939035e-09, Loss: 0.009836387820541859\n",
            "Epoch 1513/2000, Learning Rate: 2.489140585739645e-09, Loss: 0.20302006602287292\n",
            "Epoch 1514/2000, Learning Rate: 2.4642491798822485e-09, Loss: 0.011148927733302116\n",
            "Epoch 1515/2000, Learning Rate: 2.4396066880834258e-09, Loss: 0.04821174219250679\n",
            "Epoch 1516/2000, Learning Rate: 2.4152106212025914e-09, Loss: 0.092680923640728\n",
            "Epoch 1517/2000, Learning Rate: 2.3910585149905655e-09, Loss: 0.10134909301996231\n",
            "Epoch 1518/2000, Learning Rate: 2.36714792984066e-09, Loss: 0.07283865660429001\n",
            "Epoch 1519/2000, Learning Rate: 2.3434764505422532e-09, Loss: 0.034994129091501236\n",
            "Epoch 1520/2000, Learning Rate: 2.3200416860368305e-09, Loss: 0.012082206085324287\n",
            "Epoch 1521/2000, Learning Rate: 2.296841269176462e-09, Loss: 0.06353960931301117\n",
            "Epoch 1522/2000, Learning Rate: 2.2738728564846975e-09, Loss: 0.023584701120853424\n",
            "Epoch 1523/2000, Learning Rate: 2.2511341279198507e-09, Loss: 0.07795044779777527\n",
            "Epoch 1524/2000, Learning Rate: 2.2286227866406524e-09, Loss: 0.028175828978419304\n",
            "Epoch 1525/2000, Learning Rate: 2.2063365587742458e-09, Loss: 0.13716739416122437\n",
            "Epoch 1526/2000, Learning Rate: 2.184273193186503e-09, Loss: 0.036890026181936264\n",
            "Epoch 1527/2000, Learning Rate: 2.162430461254638e-09, Loss: 0.06219030171632767\n",
            "Epoch 1528/2000, Learning Rate: 2.1408061566420918e-09, Loss: 0.017584290355443954\n",
            "Epoch 1529/2000, Learning Rate: 2.1193980950756707e-09, Loss: 0.03684835880994797\n",
            "Epoch 1530/2000, Learning Rate: 2.098204114124914e-09, Loss: 0.015149411745369434\n",
            "Epoch 1531/2000, Learning Rate: 2.0772220729836648e-09, Loss: 0.024942517280578613\n",
            "Epoch 1532/2000, Learning Rate: 2.056449852253828e-09, Loss: 0.1034073457121849\n",
            "Epoch 1533/2000, Learning Rate: 2.0358853537312896e-09, Loss: 0.005372080486267805\n",
            "Epoch 1534/2000, Learning Rate: 2.015526500193977e-09, Loss: 0.03996060788631439\n",
            "Epoch 1535/2000, Learning Rate: 1.995371235192037e-09, Loss: 0.019399724900722504\n",
            "Epoch 1536/2000, Learning Rate: 1.9754175228401167e-09, Loss: 0.020068729296326637\n",
            "Epoch 1537/2000, Learning Rate: 1.9556633476117154e-09, Loss: 0.03119495138525963\n",
            "Epoch 1538/2000, Learning Rate: 1.936106714135598e-09, Loss: 0.039760030806064606\n",
            "Epoch 1539/2000, Learning Rate: 1.9167456469942423e-09, Loss: 0.038781389594078064\n",
            "Epoch 1540/2000, Learning Rate: 1.8975781905242997e-09, Loss: 0.026026979088783264\n",
            "Epoch 1541/2000, Learning Rate: 1.8786024086190566e-09, Loss: 0.022547950968146324\n",
            "Epoch 1542/2000, Learning Rate: 1.8598163845328661e-09, Loss: 0.0405392125248909\n",
            "Epoch 1543/2000, Learning Rate: 1.8412182206875373e-09, Loss: 0.03751677647233009\n",
            "Epoch 1544/2000, Learning Rate: 1.822806038480662e-09, Loss: 0.006283153314143419\n",
            "Epoch 1545/2000, Learning Rate: 1.8045779780958553e-09, Loss: 0.1024458110332489\n",
            "Epoch 1546/2000, Learning Rate: 1.7865321983148967e-09, Loss: 0.03286224603652954\n",
            "Epoch 1547/2000, Learning Rate: 1.7686668763317476e-09, Loss: 0.032650526612997055\n",
            "Epoch 1548/2000, Learning Rate: 1.7509802075684302e-09, Loss: 0.30386146903038025\n",
            "Epoch 1549/2000, Learning Rate: 1.733470405492746e-09, Loss: 0.05907600373029709\n",
            "Epoch 1550/2000, Learning Rate: 1.7161357014378185e-09, Loss: 0.10154730826616287\n",
            "Epoch 1551/2000, Learning Rate: 1.6989743444234402e-09, Loss: 0.04405122622847557\n",
            "Epoch 1552/2000, Learning Rate: 1.681984600979206e-09, Loss: 0.02547149360179901\n",
            "Epoch 1553/2000, Learning Rate: 1.6651647549694138e-09, Loss: 0.15420788526535034\n",
            "Epoch 1554/2000, Learning Rate: 1.6485131074197196e-09, Loss: 0.2634003162384033\n",
            "Epoch 1555/2000, Learning Rate: 1.6320279763455224e-09, Loss: 0.01404651440680027\n",
            "Epoch 1556/2000, Learning Rate: 1.615707696582067e-09, Loss: 0.015791084617376328\n",
            "Epoch 1557/2000, Learning Rate: 1.5995506196162463e-09, Loss: 0.07184936851263046\n",
            "Epoch 1558/2000, Learning Rate: 1.583555113420084e-09, Loss: 0.0204151663929224\n",
            "Epoch 1559/2000, Learning Rate: 1.567719562285883e-09, Loss: 0.09764362871646881\n",
            "Epoch 1560/2000, Learning Rate: 1.5520423666630241e-09, Loss: 0.03565685823559761\n",
            "Epoch 1561/2000, Learning Rate: 1.5365219429963939e-09, Loss: 0.08195783942937851\n",
            "Epoch 1562/2000, Learning Rate: 1.52115672356643e-09, Loss: 0.02501007169485092\n",
            "Epoch 1563/2000, Learning Rate: 1.5059451563307657e-09, Loss: 0.11315220594406128\n",
            "Epoch 1564/2000, Learning Rate: 1.490885704767458e-09, Loss: 0.13562330603599548\n",
            "Epoch 1565/2000, Learning Rate: 1.4759768477197835e-09, Loss: 0.16778574883937836\n",
            "Epoch 1566/2000, Learning Rate: 1.4612170792425857e-09, Loss: 0.07139284908771515\n",
            "Epoch 1567/2000, Learning Rate: 1.4466049084501598e-09, Loss: 0.13057860732078552\n",
            "Epoch 1568/2000, Learning Rate: 1.4321388593656582e-09, Loss: 0.03233150392770767\n",
            "Epoch 1569/2000, Learning Rate: 1.4178174707720015e-09, Loss: 0.14329205453395844\n",
            "Epoch 1570/2000, Learning Rate: 1.4036392960642816e-09, Loss: 0.08170647919178009\n",
            "Epoch 1571/2000, Learning Rate: 1.3896029031036387e-09, Loss: 0.03078717552125454\n",
            "Epoch 1572/2000, Learning Rate: 1.3757068740726024e-09, Loss: 0.0248591136187315\n",
            "Epoch 1573/2000, Learning Rate: 1.3619498053318763e-09, Loss: 0.011046476662158966\n",
            "Epoch 1574/2000, Learning Rate: 1.3483303072785576e-09, Loss: 0.009437816217541695\n",
            "Epoch 1575/2000, Learning Rate: 1.334847004205772e-09, Loss: 0.030783643946051598\n",
            "Epoch 1576/2000, Learning Rate: 1.3214985341637142e-09, Loss: 0.07905358821153641\n",
            "Epoch 1577/2000, Learning Rate: 1.308283548822077e-09, Loss: 0.12769854068756104\n",
            "Epoch 1578/2000, Learning Rate: 1.2952007133338562e-09, Loss: 0.010255387984216213\n",
            "Epoch 1579/2000, Learning Rate: 1.2822487062005177e-09, Loss: 0.022217176854610443\n",
            "Epoch 1580/2000, Learning Rate: 1.2694262191385126e-09, Loss: 0.0819779708981514\n",
            "Epoch 1581/2000, Learning Rate: 1.2567319569471275e-09, Loss: 0.278281033039093\n",
            "Epoch 1582/2000, Learning Rate: 1.2441646373776563e-09, Loss: 0.1381998360157013\n",
            "Epoch 1583/2000, Learning Rate: 1.2317229910038797e-09, Loss: 0.15565413236618042\n",
            "Epoch 1584/2000, Learning Rate: 1.2194057610938409e-09, Loss: 0.008825521916151047\n",
            "Epoch 1585/2000, Learning Rate: 1.2072117034829024e-09, Loss: 0.07975049316883087\n",
            "Epoch 1586/2000, Learning Rate: 1.1951395864480733e-09, Loss: 0.009837549179792404\n",
            "Epoch 1587/2000, Learning Rate: 1.1831881905835926e-09, Loss: 0.1004432663321495\n",
            "Epoch 1588/2000, Learning Rate: 1.1713563086777566e-09, Loss: 0.12369435280561447\n",
            "Epoch 1589/2000, Learning Rate: 1.159642745590979e-09, Loss: 0.21120786666870117\n",
            "Epoch 1590/2000, Learning Rate: 1.1480463181350693e-09, Loss: 0.04335394874215126\n",
            "Epoch 1591/2000, Learning Rate: 1.1365658549537186e-09, Loss: 0.07636650651693344\n",
            "Epoch 1592/2000, Learning Rate: 1.1252001964041814e-09, Loss: 0.030339056625962257\n",
            "Epoch 1593/2000, Learning Rate: 1.1139481944401395e-09, Loss: 0.13584017753601074\n",
            "Epoch 1594/2000, Learning Rate: 1.1028087124957382e-09, Loss: 0.07399536669254303\n",
            "Epoch 1595/2000, Learning Rate: 1.0917806253707807e-09, Loss: 0.2874772548675537\n",
            "Epoch 1596/2000, Learning Rate: 1.0808628191170729e-09, Loss: 0.01735641062259674\n",
            "Epoch 1597/2000, Learning Rate: 1.0700541909259022e-09, Loss: 0.03801228478550911\n",
            "Epoch 1598/2000, Learning Rate: 1.0593536490166431e-09, Loss: 0.06596391648054123\n",
            "Epoch 1599/2000, Learning Rate: 1.0487601125264766e-09, Loss: 0.0031332869548350573\n",
            "Epoch 1600/2000, Learning Rate: 1.0382725114012119e-09, Loss: 0.030708448961377144\n",
            "Epoch 1601/2000, Learning Rate: 1.0278897862871998e-09, Loss: 0.0251763965934515\n",
            "Epoch 1602/2000, Learning Rate: 1.0176108884243278e-09, Loss: 0.044427309185266495\n",
            "Epoch 1603/2000, Learning Rate: 1.0074347795400846e-09, Loss: 0.09163723886013031\n",
            "Epoch 1604/2000, Learning Rate: 9.973604317446838e-10, Loss: 0.06788860261440277\n",
            "Epoch 1605/2000, Learning Rate: 9.873868274272369e-10, Loss: 0.04997538402676582\n",
            "Epoch 1606/2000, Learning Rate: 9.775129591529646e-10, Loss: 0.011357881128787994\n",
            "Epoch 1607/2000, Learning Rate: 9.67737829561435e-10, Loss: 0.00985188689082861\n",
            "Epoch 1608/2000, Learning Rate: 9.580604512658205e-10, Loss: 0.03481140732765198\n",
            "Epoch 1609/2000, Learning Rate: 9.484798467531622e-10, Loss: 0.06532994657754898\n",
            "Epoch 1610/2000, Learning Rate: 9.389950482856306e-10, Loss: 0.020768821239471436\n",
            "Epoch 1611/2000, Learning Rate: 9.296050978027743e-10, Loss: 0.049424268305301666\n",
            "Epoch 1612/2000, Learning Rate: 9.203090468247465e-10, Loss: 0.18077422678470612\n",
            "Epoch 1613/2000, Learning Rate: 9.11105956356499e-10, Loss: 0.1042550802230835\n",
            "Epoch 1614/2000, Learning Rate: 9.01994896792934e-10, Loss: 0.029656769707798958\n",
            "Epoch 1615/2000, Learning Rate: 8.929749478250047e-10, Loss: 0.007577951997518539\n",
            "Epoch 1616/2000, Learning Rate: 8.840451983467546e-10, Loss: 0.013724387623369694\n",
            "Epoch 1617/2000, Learning Rate: 8.752047463632871e-10, Loss: 0.08350004255771637\n",
            "Epoch 1618/2000, Learning Rate: 8.664526988996542e-10, Loss: 0.18145263195037842\n",
            "Epoch 1619/2000, Learning Rate: 8.577881719106577e-10, Loss: 0.009649083018302917\n",
            "Epoch 1620/2000, Learning Rate: 8.492102901915511e-10, Loss: 0.010516039095818996\n",
            "Epoch 1621/2000, Learning Rate: 8.407181872896356e-10, Loss: 0.03707120940089226\n",
            "Epoch 1622/2000, Learning Rate: 8.323110054167392e-10, Loss: 0.031006675213575363\n",
            "Epoch 1623/2000, Learning Rate: 8.239878953625718e-10, Loss: 0.08257126063108444\n",
            "Epoch 1624/2000, Learning Rate: 8.15748016408946e-10, Loss: 0.06623028218746185\n",
            "Epoch 1625/2000, Learning Rate: 8.075905362448565e-10, Loss: 0.10292259603738785\n",
            "Epoch 1626/2000, Learning Rate: 7.995146308824079e-10, Loss: 0.046944137662649155\n",
            "Epoch 1627/2000, Learning Rate: 7.915194845735838e-10, Loss: 0.2036452442407608\n",
            "Epoch 1628/2000, Learning Rate: 7.836042897278479e-10, Loss: 0.006568132434040308\n",
            "Epoch 1629/2000, Learning Rate: 7.757682468305695e-10, Loss: 0.012345796450972557\n",
            "Epoch 1630/2000, Learning Rate: 7.680105643622638e-10, Loss: 0.0804099440574646\n",
            "Epoch 1631/2000, Learning Rate: 7.603304587186411e-10, Loss: 0.024215228855609894\n",
            "Epoch 1632/2000, Learning Rate: 7.527271541314547e-10, Loss: 0.08233343064785004\n",
            "Epoch 1633/2000, Learning Rate: 7.451998825901401e-10, Loss: 0.042671333998441696\n",
            "Epoch 1634/2000, Learning Rate: 7.377478837642386e-10, Loss: 0.08168160170316696\n",
            "Epoch 1635/2000, Learning Rate: 7.303704049265963e-10, Loss: 0.007221062667667866\n",
            "Epoch 1636/2000, Learning Rate: 7.230667008773303e-10, Loss: 0.1405131071805954\n",
            "Epoch 1637/2000, Learning Rate: 7.15836033868557e-10, Loss: 0.033229995518922806\n",
            "Epoch 1638/2000, Learning Rate: 7.086776735298714e-10, Loss: 0.13984480500221252\n",
            "Epoch 1639/2000, Learning Rate: 7.015908967945727e-10, Loss: 0.04715270549058914\n",
            "Epoch 1640/2000, Learning Rate: 6.94574987826627e-10, Loss: 0.06739870458841324\n",
            "Epoch 1641/2000, Learning Rate: 6.876292379483607e-10, Loss: 0.011934241279959679\n",
            "Epoch 1642/2000, Learning Rate: 6.807529455688772e-10, Loss: 0.004519651178270578\n",
            "Epoch 1643/2000, Learning Rate: 6.739454161131884e-10, Loss: 0.1159670501947403\n",
            "Epoch 1644/2000, Learning Rate: 6.672059619520565e-10, Loss: 0.017137467861175537\n",
            "Epoch 1645/2000, Learning Rate: 6.605339023325359e-10, Loss: 0.018872957676649094\n",
            "Epoch 1646/2000, Learning Rate: 6.539285633092105e-10, Loss: 0.009120604023337364\n",
            "Epoch 1647/2000, Learning Rate: 6.473892776761184e-10, Loss: 0.03112671710550785\n",
            "Epoch 1648/2000, Learning Rate: 6.409153848993573e-10, Loss: 0.1319027692079544\n",
            "Epoch 1649/2000, Learning Rate: 6.345062310503636e-10, Loss: 0.05696873366832733\n",
            "Epoch 1650/2000, Learning Rate: 6.2816116873986e-10, Loss: 0.06301206350326538\n",
            "Epoch 1651/2000, Learning Rate: 6.218795570524613e-10, Loss: 0.009530623443424702\n",
            "Epoch 1652/2000, Learning Rate: 6.156607614819368e-10, Loss: 0.10811319202184677\n",
            "Epoch 1653/2000, Learning Rate: 6.095041538671174e-10, Loss: 0.10834131389856339\n",
            "Epoch 1654/2000, Learning Rate: 6.034091123284463e-10, Loss: 0.024601437151432037\n",
            "Epoch 1655/2000, Learning Rate: 5.973750212051618e-10, Loss: 0.008175504393875599\n",
            "Epoch 1656/2000, Learning Rate: 5.914012709931102e-10, Loss: 0.07878335565328598\n",
            "Epoch 1657/2000, Learning Rate: 5.854872582831791e-10, Loss: 0.0337931290268898\n",
            "Epoch 1658/2000, Learning Rate: 5.796323857003473e-10, Loss: 0.031015774235129356\n",
            "Epoch 1659/2000, Learning Rate: 5.738360618433439e-10, Loss: 0.13949300348758698\n",
            "Epoch 1660/2000, Learning Rate: 5.680977012249104e-10, Loss: 0.14018625020980835\n",
            "Epoch 1661/2000, Learning Rate: 5.624167242126613e-10, Loss: 0.041919417679309845\n",
            "Epoch 1662/2000, Learning Rate: 5.567925569705347e-10, Loss: 0.08469362556934357\n",
            "Epoch 1663/2000, Learning Rate: 5.512246314008294e-10, Loss: 0.02028295025229454\n",
            "Epoch 1664/2000, Learning Rate: 5.45712385086821e-10, Loss: 0.03129546344280243\n",
            "Epoch 1665/2000, Learning Rate: 5.402552612359528e-10, Loss: 0.051233865320682526\n",
            "Epoch 1666/2000, Learning Rate: 5.348527086235933e-10, Loss: 0.02124609984457493\n",
            "Epoch 1667/2000, Learning Rate: 5.295041815373573e-10, Loss: 0.037163145840168\n",
            "Epoch 1668/2000, Learning Rate: 5.242091397219838e-10, Loss: 0.03672337904572487\n",
            "Epoch 1669/2000, Learning Rate: 5.18967048324764e-10, Loss: 0.029095472767949104\n",
            "Epoch 1670/2000, Learning Rate: 5.137773778415163e-10, Loss: 0.07813016325235367\n",
            "Epoch 1671/2000, Learning Rate: 5.086396040631011e-10, Loss: 0.02742619626224041\n",
            "Epoch 1672/2000, Learning Rate: 5.035532080224701e-10, Loss: 0.054154980927705765\n",
            "Epoch 1673/2000, Learning Rate: 4.985176759422454e-10, Loss: 0.06372106075286865\n",
            "Epoch 1674/2000, Learning Rate: 4.935324991828229e-10, Loss: 0.12788081169128418\n",
            "Epoch 1675/2000, Learning Rate: 4.885971741909947e-10, Loss: 0.0069086248986423016\n",
            "Epoch 1676/2000, Learning Rate: 4.837112024490848e-10, Loss: 0.015717582777142525\n",
            "Epoch 1677/2000, Learning Rate: 4.788740904245939e-10, Loss: 0.005197629332542419\n",
            "Epoch 1678/2000, Learning Rate: 4.74085349520348e-10, Loss: 0.06766512989997864\n",
            "Epoch 1679/2000, Learning Rate: 4.693444960251445e-10, Loss: 0.21175554394721985\n",
            "Epoch 1680/2000, Learning Rate: 4.6465105106489305e-10, Loss: 0.20521840453147888\n",
            "Epoch 1681/2000, Learning Rate: 4.600045405542441e-10, Loss: 0.042645398527383804\n",
            "Epoch 1682/2000, Learning Rate: 4.5540449514870165e-10, Loss: 0.07835336774587631\n",
            "Epoch 1683/2000, Learning Rate: 4.5085045019721463e-10, Loss: 0.021617835387587547\n",
            "Epoch 1684/2000, Learning Rate: 4.4634194569524247e-10, Loss: 0.05112653598189354\n",
            "Epoch 1685/2000, Learning Rate: 4.4187852623829e-10, Loss: 0.03598106652498245\n",
            "Epoch 1686/2000, Learning Rate: 4.374597409759071e-10, Loss: 0.022510381415486336\n",
            "Epoch 1687/2000, Learning Rate: 4.3308514356614805e-10, Loss: 0.021780073642730713\n",
            "Epoch 1688/2000, Learning Rate: 4.2875429213048657e-10, Loss: 0.040583424270153046\n",
            "Epoch 1689/2000, Learning Rate: 4.244667492091817e-10, Loss: 0.09418733417987823\n",
            "Epoch 1690/2000, Learning Rate: 4.202220817170899e-10, Loss: 0.018745003268122673\n",
            "Epoch 1691/2000, Learning Rate: 4.1601986089991897e-10, Loss: 0.07469504326581955\n",
            "Epoch 1692/2000, Learning Rate: 4.1185966229091977e-10, Loss: 0.044238246977329254\n",
            "Epoch 1693/2000, Learning Rate: 4.0774106566801056e-10, Loss: 0.017108671367168427\n",
            "Epoch 1694/2000, Learning Rate: 4.0366365501133044e-10, Loss: 0.22322280704975128\n",
            "Epoch 1695/2000, Learning Rate: 3.996270184612171e-10, Loss: 0.17540283501148224\n",
            "Epoch 1696/2000, Learning Rate: 3.9563074827660496e-10, Loss: 0.031553663313388824\n",
            "Epoch 1697/2000, Learning Rate: 3.916744407938389e-10, Loss: 0.022102370858192444\n",
            "Epoch 1698/2000, Learning Rate: 3.877576963859005e-10, Loss: 0.040245767682790756\n",
            "Epoch 1699/2000, Learning Rate: 3.838801194220415e-10, Loss: 0.028051115572452545\n",
            "Epoch 1700/2000, Learning Rate: 3.8004131822782106e-10, Loss: 0.13395936787128448\n",
            "Epoch 1701/2000, Learning Rate: 3.7624090504554284e-10, Loss: 0.0409601554274559\n",
            "Epoch 1702/2000, Learning Rate: 3.7247849599508743e-10, Loss: 0.0114515395835042\n",
            "Epoch 1703/2000, Learning Rate: 3.6875371103513654e-10, Loss: 0.058697886765003204\n",
            "Epoch 1704/2000, Learning Rate: 3.650661739247852e-10, Loss: 0.13372065126895905\n",
            "Epoch 1705/2000, Learning Rate: 3.614155121855373e-10, Loss: 0.013347371481359005\n",
            "Epoch 1706/2000, Learning Rate: 3.578013570636819e-10, Loss: 0.008982626721262932\n",
            "Epoch 1707/2000, Learning Rate: 3.542233434930451e-10, Loss: 0.023876167833805084\n",
            "Epoch 1708/2000, Learning Rate: 3.5068111005811466e-10, Loss: 0.015796015039086342\n",
            "Epoch 1709/2000, Learning Rate: 3.471742989575335e-10, Loss: 0.04953444004058838\n",
            "Epoch 1710/2000, Learning Rate: 3.437025559679582e-10, Loss: 0.020256664603948593\n",
            "Epoch 1711/2000, Learning Rate: 3.402655304082786e-10, Loss: 0.009973443113267422\n",
            "Epoch 1712/2000, Learning Rate: 3.368628751041958e-10, Loss: 0.04013221710920334\n",
            "Epoch 1713/2000, Learning Rate: 3.3349424635315384e-10, Loss: 0.24328619241714478\n",
            "Epoch 1714/2000, Learning Rate: 3.301593038896223e-10, Loss: 0.04148729145526886\n",
            "Epoch 1715/2000, Learning Rate: 3.268577108507261e-10, Loss: 0.008300633169710636\n",
            "Epoch 1716/2000, Learning Rate: 3.235891337422188e-10, Loss: 0.00479923514649272\n",
            "Epoch 1717/2000, Learning Rate: 3.2035324240479663e-10, Loss: 0.05174728482961655\n",
            "Epoch 1718/2000, Learning Rate: 3.1714970998074865e-10, Loss: 0.012319150380790234\n",
            "Epoch 1719/2000, Learning Rate: 3.1397821288094116e-10, Loss: 0.036149535328149796\n",
            "Epoch 1720/2000, Learning Rate: 3.108384307521317e-10, Loss: 0.0108823012560606\n",
            "Epoch 1721/2000, Learning Rate: 3.0773004644461043e-10, Loss: 0.013262633234262466\n",
            "Epoch 1722/2000, Learning Rate: 3.046527459801643e-10, Loss: 0.09875243157148361\n",
            "Epoch 1723/2000, Learning Rate: 3.0160621852036266e-10, Loss: 0.04893450811505318\n",
            "Epoch 1724/2000, Learning Rate: 2.98590156335159e-10, Loss: 0.27027803659439087\n",
            "Epoch 1725/2000, Learning Rate: 2.956042547718074e-10, Loss: 0.024511851370334625\n",
            "Epoch 1726/2000, Learning Rate: 2.926482122240893e-10, Loss: 0.04658098146319389\n",
            "Epoch 1727/2000, Learning Rate: 2.8972173010184843e-10, Loss: 0.08666400611400604\n",
            "Epoch 1728/2000, Learning Rate: 2.8682451280082996e-10, Loss: 0.02625185437500477\n",
            "Epoch 1729/2000, Learning Rate: 2.8395626767282164e-10, Loss: 0.006260521709918976\n",
            "Epoch 1730/2000, Learning Rate: 2.8111670499609344e-10, Loss: 0.08215392380952835\n",
            "Epoch 1731/2000, Learning Rate: 2.783055379461325e-10, Loss: 0.0023557820823043585\n",
            "Epoch 1732/2000, Learning Rate: 2.7552248256667116e-10, Loss: 0.11382336914539337\n",
            "Epoch 1733/2000, Learning Rate: 2.7276725774100446e-10, Loss: 0.08879444748163223\n",
            "Epoch 1734/2000, Learning Rate: 2.700395851635944e-10, Loss: 0.2068292796611786\n",
            "Epoch 1735/2000, Learning Rate: 2.6733918931195845e-10, Loss: 0.008499308489263058\n",
            "Epoch 1736/2000, Learning Rate: 2.6466579741883887e-10, Loss: 0.03134620189666748\n",
            "Epoch 1737/2000, Learning Rate: 2.6201913944465046e-10, Loss: 0.04234909638762474\n",
            "Epoch 1738/2000, Learning Rate: 2.5939894805020396e-10, Loss: 0.05198706313967705\n",
            "Epoch 1739/2000, Learning Rate: 2.568049585697019e-10, Loss: 0.03859435394406319\n",
            "Epoch 1740/2000, Learning Rate: 2.542369089840049e-10, Loss: 0.023062657564878464\n",
            "Epoch 1741/2000, Learning Rate: 2.516945398941648e-10, Loss: 0.13630618155002594\n",
            "Epoch 1742/2000, Learning Rate: 2.4917759449522316e-10, Loss: 0.035814691334962845\n",
            "Epoch 1743/2000, Learning Rate: 2.4668581855027095e-10, Loss: 0.012883099727332592\n",
            "Epoch 1744/2000, Learning Rate: 2.442189603647682e-10, Loss: 0.03366442397236824\n",
            "Epoch 1745/2000, Learning Rate: 2.417767707611205e-10, Loss: 0.06608356535434723\n",
            "Epoch 1746/2000, Learning Rate: 2.393590030535093e-10, Loss: 0.002652931958436966\n",
            "Epoch 1747/2000, Learning Rate: 2.369654130229742e-10, Loss: 0.15107432007789612\n",
            "Epoch 1748/2000, Learning Rate: 2.3459575889274444e-10, Loss: 0.21101124584674835\n",
            "Epoch 1749/2000, Learning Rate: 2.3224980130381698e-10, Loss: 0.037244368344545364\n",
            "Epoch 1750/2000, Learning Rate: 2.299273032907788e-10, Loss: 0.023036107420921326\n",
            "Epoch 1751/2000, Learning Rate: 2.27628030257871e-10, Loss: 0.028456421568989754\n",
            "Epoch 1752/2000, Learning Rate: 2.253517499552923e-10, Loss: 0.002782474271953106\n",
            "Epoch 1753/2000, Learning Rate: 2.2309823245573937e-10, Loss: 0.011035148985683918\n",
            "Epoch 1754/2000, Learning Rate: 2.2086725013118196e-10, Loss: 0.08517485857009888\n",
            "Epoch 1755/2000, Learning Rate: 2.1865857762987014e-10, Loss: 0.10470204800367355\n",
            "Epoch 1756/2000, Learning Rate: 2.1647199185357144e-10, Loss: 0.06774955987930298\n",
            "Epoch 1757/2000, Learning Rate: 2.1430727193503573e-10, Loss: 0.23024359345436096\n",
            "Epoch 1758/2000, Learning Rate: 2.1216419921568537e-10, Loss: 0.042445503175258636\n",
            "Epoch 1759/2000, Learning Rate: 2.1004255722352853e-10, Loss: 0.10867238789796829\n",
            "Epoch 1760/2000, Learning Rate: 2.0794213165129325e-10, Loss: 0.003868249710649252\n",
            "Epoch 1761/2000, Learning Rate: 2.058627103347803e-10, Loss: 0.028229013085365295\n",
            "Epoch 1762/2000, Learning Rate: 2.0380408323143252e-10, Loss: 0.018037673085927963\n",
            "Epoch 1763/2000, Learning Rate: 2.0176604239911818e-10, Loss: 0.008620182983577251\n",
            "Epoch 1764/2000, Learning Rate: 1.99748381975127e-10, Loss: 0.048771265894174576\n",
            "Epoch 1765/2000, Learning Rate: 1.9775089815537572e-10, Loss: 0.04947669431567192\n",
            "Epoch 1766/2000, Learning Rate: 1.9577338917382197e-10, Loss: 0.22952747344970703\n",
            "Epoch 1767/2000, Learning Rate: 1.9381565528208375e-10, Loss: 0.11634717881679535\n",
            "Epoch 1768/2000, Learning Rate: 1.918774987292629e-10, Loss: 0.24416127800941467\n",
            "Epoch 1769/2000, Learning Rate: 1.8995872374197028e-10, Loss: 0.02211637608706951\n",
            "Epoch 1770/2000, Learning Rate: 1.880591365045506e-10, Loss: 0.07792205363512039\n",
            "Epoch 1771/2000, Learning Rate: 1.8617854513950507e-10, Loss: 0.02421547658741474\n",
            "Epoch 1772/2000, Learning Rate: 1.8431675968811003e-10, Loss: 0.19478049874305725\n",
            "Epoch 1773/2000, Learning Rate: 1.8247359209122892e-10, Loss: 0.046269018203020096\n",
            "Epoch 1774/2000, Learning Rate: 1.8064885617031662e-10, Loss: 0.03402082249522209\n",
            "Epoch 1775/2000, Learning Rate: 1.7884236760861344e-10, Loss: 0.15904119610786438\n",
            "Epoch 1776/2000, Learning Rate: 1.7705394393252732e-10, Loss: 0.023759856820106506\n",
            "Epoch 1777/2000, Learning Rate: 1.7528340449320203e-10, Loss: 0.04269455000758171\n",
            "Epoch 1778/2000, Learning Rate: 1.7353057044827e-10, Loss: 0.06458638608455658\n",
            "Epoch 1779/2000, Learning Rate: 1.717952647437873e-10, Loss: 0.0033539615105837584\n",
            "Epoch 1780/2000, Learning Rate: 1.7007731209634942e-10, Loss: 0.05884372442960739\n",
            "Epoch 1781/2000, Learning Rate: 1.6837653897538594e-10, Loss: 0.031085992231965065\n",
            "Epoch 1782/2000, Learning Rate: 1.6669277358563207e-10, Loss: 0.023517683148384094\n",
            "Epoch 1783/2000, Learning Rate: 1.6502584584977575e-10, Loss: 0.024256087839603424\n",
            "Epoch 1784/2000, Learning Rate: 1.63375587391278e-10, Loss: 0.016442708671092987\n",
            "Epoch 1785/2000, Learning Rate: 1.617418315173652e-10, Loss: 0.0406116284430027\n",
            "Epoch 1786/2000, Learning Rate: 1.6012441320219155e-10, Loss: 0.19681893289089203\n",
            "Epoch 1787/2000, Learning Rate: 1.5852316907016963e-10, Loss: 0.011807143688201904\n",
            "Epoch 1788/2000, Learning Rate: 1.5693793737946794e-10, Loss: 0.032667070627212524\n",
            "Epoch 1789/2000, Learning Rate: 1.5536855800567325e-10, Loss: 0.007118110544979572\n",
            "Epoch 1790/2000, Learning Rate: 1.538148724256165e-10, Loss: 0.11478856205940247\n",
            "Epoch 1791/2000, Learning Rate: 1.5227672370136033e-10, Loss: 0.06842135637998581\n",
            "Epoch 1792/2000, Learning Rate: 1.5075395646434673e-10, Loss: 0.011865141801536083\n",
            "Epoch 1793/2000, Learning Rate: 1.4924641689970326e-10, Loss: 0.1055581346154213\n",
            "Epoch 1794/2000, Learning Rate: 1.4775395273070622e-10, Loss: 0.0314997136592865\n",
            "Epoch 1795/2000, Learning Rate: 1.4627641320339914e-10, Loss: 0.013013769872486591\n",
            "Epoch 1796/2000, Learning Rate: 1.4481364907136516e-10, Loss: 0.27027082443237305\n",
            "Epoch 1797/2000, Learning Rate: 1.4336551258065151e-10, Loss: 0.030444180592894554\n",
            "Epoch 1798/2000, Learning Rate: 1.41931857454845e-10, Loss: 0.034288838505744934\n",
            "Epoch 1799/2000, Learning Rate: 1.4051253888029656e-10, Loss: 0.04556454345583916\n",
            "Epoch 1800/2000, Learning Rate: 1.391074134914936e-10, Loss: 0.22920319437980652\n",
            "Epoch 1801/2000, Learning Rate: 1.3771633935657867e-10, Loss: 0.0293598473072052\n",
            "Epoch 1802/2000, Learning Rate: 1.3633917596301288e-10, Loss: 0.05292358621954918\n",
            "Epoch 1803/2000, Learning Rate: 1.3497578420338275e-10, Loss: 0.15522834658622742\n",
            "Epoch 1804/2000, Learning Rate: 1.3362602636134891e-10, Loss: 0.02301815338432789\n",
            "Epoch 1805/2000, Learning Rate: 1.3228976609773542e-10, Loss: 0.005832272581756115\n",
            "Epoch 1806/2000, Learning Rate: 1.3096686843675807e-10, Loss: 0.16631026566028595\n",
            "Epoch 1807/2000, Learning Rate: 1.296571997523905e-10, Loss: 0.012195678427815437\n",
            "Epoch 1808/2000, Learning Rate: 1.2836062775486659e-10, Loss: 0.019314950332045555\n",
            "Epoch 1809/2000, Learning Rate: 1.2707702147731792e-10, Loss: 0.030675478279590607\n",
            "Epoch 1810/2000, Learning Rate: 1.2580625126254474e-10, Loss: 0.008608490228652954\n",
            "Epoch 1811/2000, Learning Rate: 1.245481887499193e-10, Loss: 0.003221317660063505\n",
            "Epoch 1812/2000, Learning Rate: 1.233027068624201e-10, Loss: 0.0063499207608401775\n",
            "Epoch 1813/2000, Learning Rate: 1.2206967979379591e-10, Loss: 0.07527021318674088\n",
            "Epoch 1814/2000, Learning Rate: 1.2084898299585796e-10, Loss: 0.014985773712396622\n",
            "Epoch 1815/2000, Learning Rate: 1.1964049316589938e-10, Loss: 0.03066050261259079\n",
            "Epoch 1816/2000, Learning Rate: 1.1844408823424037e-10, Loss: 0.071726955473423\n",
            "Epoch 1817/2000, Learning Rate: 1.1725964735189796e-10, Loss: 0.006802782416343689\n",
            "Epoch 1818/2000, Learning Rate: 1.1608705087837898e-10, Loss: 0.057851456105709076\n",
            "Epoch 1819/2000, Learning Rate: 1.149261803695952e-10, Loss: 0.10764018446207047\n",
            "Epoch 1820/2000, Learning Rate: 1.1377691856589924e-10, Loss: 0.02169491909444332\n",
            "Epoch 1821/2000, Learning Rate: 1.1263914938024025e-10, Loss: 0.12324898689985275\n",
            "Epoch 1822/2000, Learning Rate: 1.1151275788643785e-10, Loss: 0.009561124257743359\n",
            "Epoch 1823/2000, Learning Rate: 1.1039763030757347e-10, Loss: 0.030382951721549034\n",
            "Epoch 1824/2000, Learning Rate: 1.0929365400449772e-10, Loss: 0.04769100621342659\n",
            "Epoch 1825/2000, Learning Rate: 1.0820071746445274e-10, Loss: 0.01863413117825985\n",
            "Epoch 1826/2000, Learning Rate: 1.0711871028980821e-10, Loss: 0.016828766092658043\n",
            "Epoch 1827/2000, Learning Rate: 1.0604752318691013e-10, Loss: 0.046216871589422226\n",
            "Epoch 1828/2000, Learning Rate: 1.0498704795504102e-10, Loss: 0.04471989721059799\n",
            "Epoch 1829/2000, Learning Rate: 1.0393717747549061e-10, Loss: 0.0046155392192304134\n",
            "Epoch 1830/2000, Learning Rate: 1.028978057007357e-10, Loss: 0.11862364411354065\n",
            "Epoch 1831/2000, Learning Rate: 1.0186882764372834e-10, Loss: 0.039945051074028015\n",
            "Epoch 1832/2000, Learning Rate: 1.0085013936729104e-10, Loss: 0.04278525337576866\n",
            "Epoch 1833/2000, Learning Rate: 9.984163797361814e-11, Loss: 0.3241935968399048\n",
            "Epoch 1834/2000, Learning Rate: 9.884322159388196e-11, Loss: 0.10592152923345566\n",
            "Epoch 1835/2000, Learning Rate: 9.785478937794313e-11, Loss: 0.03120339661836624\n",
            "Epoch 1836/2000, Learning Rate: 9.68762414841637e-11, Loss: 0.06090148910880089\n",
            "Epoch 1837/2000, Learning Rate: 9.590747906932206e-11, Loss: 0.005344835575670004\n",
            "Epoch 1838/2000, Learning Rate: 9.494840427862884e-11, Loss: 0.007004790473729372\n",
            "Epoch 1839/2000, Learning Rate: 9.399892023584255e-11, Loss: 0.10407591611146927\n",
            "Epoch 1840/2000, Learning Rate: 9.305893103348413e-11, Loss: 0.0700450912117958\n",
            "Epoch 1841/2000, Learning Rate: 9.212834172314929e-11, Loss: 0.13078585267066956\n",
            "Epoch 1842/2000, Learning Rate: 9.120705830591779e-11, Loss: 0.0475369393825531\n",
            "Epoch 1843/2000, Learning Rate: 9.029498772285862e-11, Loss: 0.02501942403614521\n",
            "Epoch 1844/2000, Learning Rate: 8.939203784563003e-11, Loss: 0.014766326174139977\n",
            "Epoch 1845/2000, Learning Rate: 8.849811746717373e-11, Loss: 0.09495516121387482\n",
            "Epoch 1846/2000, Learning Rate: 8.7613136292502e-11, Loss: 0.07985222339630127\n",
            "Epoch 1847/2000, Learning Rate: 8.673700492957697e-11, Loss: 0.14821337163448334\n",
            "Epoch 1848/2000, Learning Rate: 8.586963488028119e-11, Loss: 0.04025769233703613\n",
            "Epoch 1849/2000, Learning Rate: 8.501093853147838e-11, Loss: 0.06292414665222168\n",
            "Epoch 1850/2000, Learning Rate: 8.416082914616359e-11, Loss: 0.05168844014406204\n",
            "Epoch 1851/2000, Learning Rate: 8.331922085470195e-11, Loss: 0.03252154588699341\n",
            "Epoch 1852/2000, Learning Rate: 8.248602864615493e-11, Loss: 0.016429612413048744\n",
            "Epoch 1853/2000, Learning Rate: 8.166116835969337e-11, Loss: 0.04441915452480316\n",
            "Epoch 1854/2000, Learning Rate: 8.084455667609643e-11, Loss: 0.03921802341938019\n",
            "Epoch 1855/2000, Learning Rate: 8.003611110933547e-11, Loss: 0.1211768165230751\n",
            "Epoch 1856/2000, Learning Rate: 7.923574999824212e-11, Loss: 0.0971079021692276\n",
            "Epoch 1857/2000, Learning Rate: 7.84433924982597e-11, Loss: 0.08898337930440903\n",
            "Epoch 1858/2000, Learning Rate: 7.76589585732771e-11, Loss: 0.05405738949775696\n",
            "Epoch 1859/2000, Learning Rate: 7.688236898754433e-11, Loss: 0.05158405005931854\n",
            "Epoch 1860/2000, Learning Rate: 7.611354529766889e-11, Loss: 0.14550307393074036\n",
            "Epoch 1861/2000, Learning Rate: 7.53524098446922e-11, Loss: 0.1860761046409607\n",
            "Epoch 1862/2000, Learning Rate: 7.459888574624527e-11, Loss: 0.022122345864772797\n",
            "Epoch 1863/2000, Learning Rate: 7.385289688878282e-11, Loss: 0.021740034222602844\n",
            "Epoch 1864/2000, Learning Rate: 7.3114367919895e-11, Loss: 0.059128839522600174\n",
            "Epoch 1865/2000, Learning Rate: 7.238322424069605e-11, Loss: 0.024353187531232834\n",
            "Epoch 1866/2000, Learning Rate: 7.165939199828909e-11, Loss: 0.059132207185029984\n",
            "Epoch 1867/2000, Learning Rate: 7.09427980783062e-11, Loss: 0.0909198597073555\n",
            "Epoch 1868/2000, Learning Rate: 7.023337009752314e-11, Loss: 0.11879957467317581\n",
            "Epoch 1869/2000, Learning Rate: 6.953103639654791e-11, Loss: 0.060300908982753754\n",
            "Epoch 1870/2000, Learning Rate: 6.883572603258243e-11, Loss: 0.08791331201791763\n",
            "Epoch 1871/2000, Learning Rate: 6.81473687722566e-11, Loss: 0.0718296468257904\n",
            "Epoch 1872/2000, Learning Rate: 6.746589508453404e-11, Loss: 0.03049061819911003\n",
            "Epoch 1873/2000, Learning Rate: 6.67912361336887e-11, Loss: 0.18956345319747925\n",
            "Epoch 1874/2000, Learning Rate: 6.612332377235181e-11, Loss: 0.05079078674316406\n",
            "Epoch 1875/2000, Learning Rate: 6.546209053462829e-11, Loss: 0.016590185463428497\n",
            "Epoch 1876/2000, Learning Rate: 6.4807469629282e-11, Loss: 0.035230252891778946\n",
            "Epoch 1877/2000, Learning Rate: 6.415939493298919e-11, Loss: 0.023981835693120956\n",
            "Epoch 1878/2000, Learning Rate: 6.35178009836593e-11, Loss: 0.013806255534291267\n",
            "Epoch 1879/2000, Learning Rate: 6.288262297382271e-11, Loss: 0.1801118105649948\n",
            "Epoch 1880/2000, Learning Rate: 6.225379674408448e-11, Loss: 0.01119757816195488\n",
            "Epoch 1881/2000, Learning Rate: 6.163125877664363e-11, Loss: 0.09220385551452637\n",
            "Epoch 1882/2000, Learning Rate: 6.101494618887718e-11, Loss: 0.10787156969308853\n",
            "Epoch 1883/2000, Learning Rate: 6.040479672698841e-11, Loss: 0.049383778125047684\n",
            "Epoch 1884/2000, Learning Rate: 5.980074875971852e-11, Loss: 0.04832446947693825\n",
            "Epoch 1885/2000, Learning Rate: 5.920274127212134e-11, Loss: 0.0011979531263932586\n",
            "Epoch 1886/2000, Learning Rate: 5.861071385940012e-11, Loss: 0.0997159481048584\n",
            "Epoch 1887/2000, Learning Rate: 5.8024606720806116e-11, Loss: 0.15821754932403564\n",
            "Epoch 1888/2000, Learning Rate: 5.7444360653598054e-11, Loss: 0.052417293190956116\n",
            "Epoch 1889/2000, Learning Rate: 5.6869917047062076e-11, Loss: 0.068720743060112\n",
            "Epoch 1890/2000, Learning Rate: 5.630121787659145e-11, Loss: 0.005529316607862711\n",
            "Epoch 1891/2000, Learning Rate: 5.573820569782554e-11, Loss: 0.048619791865348816\n",
            "Epoch 1892/2000, Learning Rate: 5.518082364084728e-11, Loss: 0.023903407156467438\n",
            "Epoch 1893/2000, Learning Rate: 5.46290154044388e-11, Loss: 0.013575787656009197\n",
            "Epoch 1894/2000, Learning Rate: 5.4082725250394413e-11, Loss: 0.05315748229622841\n",
            "Epoch 1895/2000, Learning Rate: 5.354189799789047e-11, Loss: 0.04704860597848892\n",
            "Epoch 1896/2000, Learning Rate: 5.3006479017911565e-11, Loss: 0.14608559012413025\n",
            "Epoch 1897/2000, Learning Rate: 5.247641422773245e-11, Loss: 0.015225919894874096\n",
            "Epoch 1898/2000, Learning Rate: 5.1951650085455126e-11, Loss: 0.05811401084065437\n",
            "Epoch 1899/2000, Learning Rate: 5.1432133584600573e-11, Loss: 0.142912819981575\n",
            "Epoch 1900/2000, Learning Rate: 5.091781224875457e-11, Loss: 0.13776706159114838\n",
            "Epoch 1901/2000, Learning Rate: 5.0408634126267024e-11, Loss: 0.04339936003088951\n",
            "Epoch 1902/2000, Learning Rate: 4.990454778500435e-11, Loss: 0.0547042153775692\n",
            "Epoch 1903/2000, Learning Rate: 4.940550230715431e-11, Loss: 0.024515029042959213\n",
            "Epoch 1904/2000, Learning Rate: 4.8911447284082765e-11, Loss: 0.013169050216674805\n",
            "Epoch 1905/2000, Learning Rate: 4.8422332811241936e-11, Loss: 0.11069782078266144\n",
            "Epoch 1906/2000, Learning Rate: 4.793810948312952e-11, Loss: 0.17824864387512207\n",
            "Epoch 1907/2000, Learning Rate: 4.7458728388298224e-11, Loss: 0.04274213686585426\n",
            "Epoch 1908/2000, Learning Rate: 4.698414110441524e-11, Loss: 0.008045388385653496\n",
            "Epoch 1909/2000, Learning Rate: 4.651429969337109e-11, Loss: 0.18595269322395325\n",
            "Epoch 1910/2000, Learning Rate: 4.604915669643738e-11, Loss: 0.011651773937046528\n",
            "Epoch 1911/2000, Learning Rate: 4.5588665129473004e-11, Loss: 0.03269138187170029\n",
            "Epoch 1912/2000, Learning Rate: 4.513277847817827e-11, Loss: 0.16600777208805084\n",
            "Epoch 1913/2000, Learning Rate: 4.468145069339649e-11, Loss: 0.12009844183921814\n",
            "Epoch 1914/2000, Learning Rate: 4.4234636186462523e-11, Loss: 0.014624463394284248\n",
            "Epoch 1915/2000, Learning Rate: 4.37922898245979e-11, Loss: 0.061311930418014526\n",
            "Epoch 1916/2000, Learning Rate: 4.335436692635192e-11, Loss: 0.257727712392807\n",
            "Epoch 1917/2000, Learning Rate: 4.29208232570884e-11, Loss: 0.056942958384752274\n",
            "Epoch 1918/2000, Learning Rate: 4.249161502451752e-11, Loss: 0.0994173213839531\n",
            "Epoch 1919/2000, Learning Rate: 4.2066698874272344e-11, Loss: 0.040163423866033554\n",
            "Epoch 1920/2000, Learning Rate: 4.164603188552962e-11, Loss: 0.08743856847286224\n",
            "Epoch 1921/2000, Learning Rate: 4.122957156667432e-11, Loss: 0.0552542507648468\n",
            "Epoch 1922/2000, Learning Rate: 4.081727585100758e-11, Loss: 0.017778245732188225\n",
            "Epoch 1923/2000, Learning Rate: 4.0409103092497506e-11, Loss: 0.01581878960132599\n",
            "Epoch 1924/2000, Learning Rate: 4.000501206157253e-11, Loss: 0.006588045973330736\n",
            "Epoch 1925/2000, Learning Rate: 3.9604961940956804e-11, Loss: 0.07529076933860779\n",
            "Epoch 1926/2000, Learning Rate: 3.9208912321547236e-11, Loss: 0.006625001318752766\n",
            "Epoch 1927/2000, Learning Rate: 3.881682319833176e-11, Loss: 0.08384095132350922\n",
            "Epoch 1928/2000, Learning Rate: 3.842865496634844e-11, Loss: 0.04976719245314598\n",
            "Epoch 1929/2000, Learning Rate: 3.804436841668496e-11, Loss: 0.04711797460913658\n",
            "Epoch 1930/2000, Learning Rate: 3.766392473251811e-11, Loss: 0.09992258250713348\n",
            "Epoch 1931/2000, Learning Rate: 3.7287285485192926e-11, Loss: 0.3963286280632019\n",
            "Epoch 1932/2000, Learning Rate: 3.6914412630341e-11, Loss: 0.11927726119756699\n",
            "Epoch 1933/2000, Learning Rate: 3.654526850403759e-11, Loss: 0.06202758103609085\n",
            "Epoch 1934/2000, Learning Rate: 3.617981581899721e-11, Loss: 0.04195713251829147\n",
            "Epoch 1935/2000, Learning Rate: 3.581801766080724e-11, Loss: 0.16502521932125092\n",
            "Epoch 1936/2000, Learning Rate: 3.545983748419917e-11, Loss: 0.03576852008700371\n",
            "Epoch 1937/2000, Learning Rate: 3.510523910935718e-11, Loss: 0.018237292766571045\n",
            "Epoch 1938/2000, Learning Rate: 3.475418671826361e-11, Loss: 0.01802580989897251\n",
            "Epoch 1939/2000, Learning Rate: 3.440664485108098e-11, Loss: 0.025177061557769775\n",
            "Epoch 1940/2000, Learning Rate: 3.4062578402570165e-11, Loss: 0.20655211806297302\n",
            "Epoch 1941/2000, Learning Rate: 3.372195261854446e-11, Loss: 0.1461900919675827\n",
            "Epoch 1942/2000, Learning Rate: 3.3384733092359017e-11, Loss: 0.26279333233833313\n",
            "Epoch 1943/2000, Learning Rate: 3.3050885761435424e-11, Loss: 0.009395685978233814\n",
            "Epoch 1944/2000, Learning Rate: 3.272037690382107e-11, Loss: 0.047897905111312866\n",
            "Epoch 1945/2000, Learning Rate: 3.239317313478286e-11, Loss: 0.010046867653727531\n",
            "Epoch 1946/2000, Learning Rate: 3.206924140343503e-11, Loss: 0.23317351937294006\n",
            "Epoch 1947/2000, Learning Rate: 3.174854898940068e-11, Loss: 0.27987414598464966\n",
            "Epoch 1948/2000, Learning Rate: 3.143106349950667e-11, Loss: 0.07955023646354675\n",
            "Epoch 1949/2000, Learning Rate: 3.11167528645116e-11, Loss: 0.1178564503788948\n",
            "Epoch 1950/2000, Learning Rate: 3.080558533586648e-11, Loss: 0.02914200909435749\n",
            "Epoch 1951/2000, Learning Rate: 3.0497529482507815e-11, Loss: 0.036676350980997086\n",
            "Epoch 1952/2000, Learning Rate: 3.0192554187682734e-11, Loss: 0.10582216829061508\n",
            "Epoch 1953/2000, Learning Rate: 2.989062864580591e-11, Loss: 0.028791602700948715\n",
            "Epoch 1954/2000, Learning Rate: 2.959172235934785e-11, Loss: 0.23085060715675354\n",
            "Epoch 1955/2000, Learning Rate: 2.9295805135754375e-11, Loss: 0.039518147706985474\n",
            "Epoch 1956/2000, Learning Rate: 2.900284708439683e-11, Loss: 0.11120809614658356\n",
            "Epoch 1957/2000, Learning Rate: 2.8712818613552862e-11, Loss: 0.03141765296459198\n",
            "Epoch 1958/2000, Learning Rate: 2.8425690427417334e-11, Loss: 0.019304754212498665\n",
            "Epoch 1959/2000, Learning Rate: 2.814143352314316e-11, Loss: 0.02505418285727501\n",
            "Epoch 1960/2000, Learning Rate: 2.7860019187911727e-11, Loss: 0.050965867936611176\n",
            "Epoch 1961/2000, Learning Rate: 2.758141899603261e-11, Loss: 0.02759219892323017\n",
            "Epoch 1962/2000, Learning Rate: 2.7305604806072284e-11, Loss: 0.07604669779539108\n",
            "Epoch 1963/2000, Learning Rate: 2.703254875801156e-11, Loss: 0.031158989295363426\n",
            "Epoch 1964/2000, Learning Rate: 2.6762223270431444e-11, Loss: 0.05628214776515961\n",
            "Epoch 1965/2000, Learning Rate: 2.649460103772713e-11, Loss: 0.05388245731592178\n",
            "Epoch 1966/2000, Learning Rate: 2.622965502734986e-11, Loss: 0.11838878691196442\n",
            "Epoch 1967/2000, Learning Rate: 2.596735847707636e-11, Loss: 0.10564915835857391\n",
            "Epoch 1968/2000, Learning Rate: 2.5707684892305594e-11, Loss: 0.05821293592453003\n",
            "Epoch 1969/2000, Learning Rate: 2.545060804338254e-11, Loss: 0.00645937817171216\n",
            "Epoch 1970/2000, Learning Rate: 2.5196101962948712e-11, Loss: 0.13871800899505615\n",
            "Epoch 1971/2000, Learning Rate: 2.4944140943319225e-11, Loss: 0.052413273602724075\n",
            "Epoch 1972/2000, Learning Rate: 2.4694699533886032e-11, Loss: 0.09515102207660675\n",
            "Epoch 1973/2000, Learning Rate: 2.444775253854717e-11, Loss: 0.01750783436000347\n",
            "Epoch 1974/2000, Learning Rate: 2.42032750131617e-11, Loss: 0.019274286925792694\n",
            "Epoch 1975/2000, Learning Rate: 2.396124226303008e-11, Loss: 0.09466549754142761\n",
            "Epoch 1976/2000, Learning Rate: 2.372162984039978e-11, Loss: 0.05504258722066879\n",
            "Epoch 1977/2000, Learning Rate: 2.3484413541995783e-11, Loss: 0.055767375975847244\n",
            "Epoch 1978/2000, Learning Rate: 2.3249569406575825e-11, Loss: 0.020133938640356064\n",
            "Epoch 1979/2000, Learning Rate: 2.3017073712510065e-11, Loss: 0.048243675380945206\n",
            "Epoch 1980/2000, Learning Rate: 2.2786902975384965e-11, Loss: 0.1498376876115799\n",
            "Epoch 1981/2000, Learning Rate: 2.2559033945631114e-11, Loss: 0.15238861739635468\n",
            "Epoch 1982/2000, Learning Rate: 2.2333443606174803e-11, Loss: 0.05512034893035889\n",
            "Epoch 1983/2000, Learning Rate: 2.2110109170113053e-11, Loss: 0.014684135094285011\n",
            "Epoch 1984/2000, Learning Rate: 2.188900807841192e-11, Loss: 0.024878494441509247\n",
            "Epoch 1985/2000, Learning Rate: 2.1670117997627803e-11, Loss: 0.019758043810725212\n",
            "Epoch 1986/2000, Learning Rate: 2.1453416817651523e-11, Loss: 0.01302814669907093\n",
            "Epoch 1987/2000, Learning Rate: 2.1238882649475007e-11, Loss: 0.02162892371416092\n",
            "Epoch 1988/2000, Learning Rate: 2.1026493822980256e-11, Loss: 0.03762875124812126\n",
            "Epoch 1989/2000, Learning Rate: 2.0816228884750453e-11, Loss: 0.1569533348083496\n",
            "Epoch 1990/2000, Learning Rate: 2.0608066595902948e-11, Loss: 0.24957729876041412\n",
            "Epoch 1991/2000, Learning Rate: 2.0401985929943917e-11, Loss: 0.0038722846657037735\n",
            "Epoch 1992/2000, Learning Rate: 2.019796607064448e-11, Loss: 0.015009758993983269\n",
            "Epoch 1993/2000, Learning Rate: 1.9995986409938034e-11, Loss: 0.009553898125886917\n",
            "Epoch 1994/2000, Learning Rate: 1.9796026545838652e-11, Loss: 0.0324418768286705\n",
            "Epoch 1995/2000, Learning Rate: 1.9598066280380265e-11, Loss: 0.036595482379198074\n",
            "Epoch 1996/2000, Learning Rate: 1.940208561757646e-11, Loss: 0.21087108552455902\n",
            "Epoch 1997/2000, Learning Rate: 1.9208064761400698e-11, Loss: 0.0645546019077301\n",
            "Epoch 1998/2000, Learning Rate: 1.901598411378669e-11, Loss: 0.027387546375393867\n",
            "Epoch 1999/2000, Learning Rate: 1.8825824272648825e-11, Loss: 0.006833028979599476\n",
            "Epoch 2000/2000, Learning Rate: 1.8637566029922338e-11, Loss: 0.006489491555839777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def degree_distribution(data, N):\n",
        "    # Find all unique nodes present in the edge index tensor\n",
        "    all_nodes = torch.unique(torch.cat((data.edge_index[0], data.edge_index[1])))\n",
        "\n",
        "    # Calculate the number of nodes\n",
        "    num_nodes = int(all_nodes.max()) + 1 if all_nodes.numel() > 0 else 0  # If there are no nodes, set num_nodes to 0\n",
        "\n",
        "    # Calculate the degree of each node\n",
        "    degrees = torch.bincount(data.edge_index[0], minlength=num_nodes)\n",
        "\n",
        "    # Convert degrees to degree distribution\n",
        "    degree_values = torch.bincount(degrees)\n",
        "\n",
        "    # Make sure the tensor is of length N\n",
        "    degree_distribution = torch.cat((degree_values, torch.zeros(max(N - len(degree_values), 0)).to(degree_values.device)))\n",
        "\n",
        "    return degree_distribution\n",
        "\n",
        "\n",
        "def clustering_coefficient(data):\n",
        "    # Ensure the graph is undirected\n",
        "    #data = to_undirected(data)\n",
        "\n",
        "    # Calculate the number of nodes\n",
        "    num_nodes = data.x.shape[0]\n",
        "\n",
        "    # Calculate the number of triangles each node is involved in\n",
        "    num_triangles = torch.zeros(num_nodes, dtype=torch.float)\n",
        "\n",
        "    # Calculate the number of connected triples each node is involved in\n",
        "    num_connected_triples = torch.zeros(num_nodes, dtype=torch.float)\n",
        "\n",
        "    # Iterate over each edge in the graph\n",
        "    for i, j in data.edge_index.t().tolist():\n",
        "        # Get the neighbors of node i and node j\n",
        "        neighbors_i = set(data.edge_index[1][data.edge_index[0] == i].tolist())\n",
        "        neighbors_j = set(data.edge_index[1][data.edge_index[0] == j].tolist())\n",
        "\n",
        "        # Calculate the number of common neighbors between i and j\n",
        "        common_neighbors = neighbors_i.intersection(neighbors_j)\n",
        "\n",
        "        # Update the number of triangles and connected triples for nodes i and j\n",
        "        num_triangles[i] += len(common_neighbors)\n",
        "        num_triangles[j] += len(common_neighbors)\n",
        "        num_connected_triples[i] += len(neighbors_i) - 1\n",
        "        num_connected_triples[j] += len(neighbors_j) - 1\n",
        "\n",
        "    # Print intermediate results for debugging\n",
        "    #print(\"Number of triangles per node:\", num_triangles)\n",
        "    #print(\"Number of connected triples per node:\", num_connected_triples)\n",
        "\n",
        "    # Calculate the local clustering coefficient for each node\n",
        "    local_clustering_coefficient = (num_triangles / 2) / num_connected_triples\n",
        "    local_clustering_coefficient[torch.isnan(local_clustering_coefficient)] = 0  # Set NaN values to 0\n",
        "\n",
        "    # Print intermediate results for debugging\n",
        "    #print(\"Local clustering coefficient per node:\", local_clustering_coefficient)\n",
        "\n",
        "    # Calculate the average clustering coefficient\n",
        "    avg_clustering_coefficient = local_clustering_coefficient.mean()\n",
        "\n",
        "    return avg_clustering_coefficient\n",
        "import torch\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "\n",
        "def spectrum_distribution(data, k):\n",
        "    # Convert PyTorch Geometric Data object to a dense adjacency matrix\n",
        "    num_nodes = data.num_nodes\n",
        "    adj_matrix = torch_geometric.utils.to_dense_adj(data.edge_index).squeeze(0)\n",
        "\n",
        "    # Calculate degree matrix\n",
        "    degree = torch.sum(adj_matrix, dim=1)\n",
        "    degree_matrix = torch.diag(degree)\n",
        "\n",
        "    # Calculate Laplacian matrix\n",
        "    laplacian_matrix = degree_matrix - adj_matrix\n",
        "\n",
        "    # Convert Laplacian to dense matrix and numpy array\n",
        "    laplacian_dense = laplacian_matrix.numpy()\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = np.linalg.eig(laplacian_dense)\n",
        "\n",
        "    # Sort eigenvalues in ascending order\n",
        "    eigenvalues = np.sort(eigenvalues)\n",
        "\n",
        "    # If k is greater than the number of eigenvalues, pad with zeros\n",
        "    if k > len(eigenvalues):\n",
        "        padded_eigenvalues = np.pad(eigenvalues, (0, k - len(eigenvalues)), 'constant')\n",
        "    else:\n",
        "        padded_eigenvalues = eigenvalues[:k]\n",
        "\n",
        "    return padded_eigenvalues.astype(np.complex128)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CNtV3mcEzvIc"
      },
      "id": "CNtV3mcEzvIc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "\n",
        "def spectrum_distribution(data, k):\n",
        "    # Convert PyTorch Geometric Data object to a dense adjacency matrix\n",
        "    num_nodes = data.num_nodes\n",
        "    adj_matrix = torch_geometric.utils.to_dense_adj(data.edge_index).squeeze(0)\n",
        "\n",
        "    # Calculate degree matrix\n",
        "    degree = torch.sum(adj_matrix, dim=1)\n",
        "    degree_matrix = torch.diag(degree)\n",
        "\n",
        "    # Calculate Laplacian matrix\n",
        "    laplacian_matrix = degree_matrix - adj_matrix\n",
        "\n",
        "    # Convert Laplacian to dense matrix and numpy array\n",
        "    laplacian_dense = laplacian_matrix.numpy()\n",
        "\n",
        "    # Compute eigenvalues\n",
        "    eigenvalues, _ = np.linalg.eig(laplacian_dense)\n",
        "\n",
        "    # Sort eigenvalues in ascending order\n",
        "    eigenvalues = np.sort(eigenvalues)\n",
        "\n",
        "    # If k is greater than the number of eigenvalues, pad with zeros\n",
        "    if k > len(eigenvalues):\n",
        "        padded_eigenvalues = np.pad(eigenvalues, (0, k - len(eigenvalues)), 'constant')\n",
        "    else:\n",
        "        padded_eigenvalues = eigenvalues[:k]\n",
        "\n",
        "    return padded_eigenvalues\n",
        "\n",
        "# Example usage\n",
        "# Assuming 'data' is your PyTorch Geometric Data object and 'k' is the desired number of eigenvalues\n",
        "# eigenvalues = spectrum_distribution(data, k)\n",
        "\n"
      ],
      "metadata": {
        "id": "jhmriIr-IQA4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jhmriIr-IQA4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Calculate MMD using Gaussian Kernel"
      ],
      "metadata": {
        "id": "5Gyggs5foZ1D"
      },
      "id": "5Gyggs5foZ1D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ap2d94AGf1R"
      },
      "outputs": [],
      "source": [
        "def mmd_kernel(X, Y, kernel_func):\n",
        "    \"\"\"\n",
        "    Compute Maximum Mean Discrepancy (MMD) between two samples using a given kernel function.\n",
        "\n",
        "    Parameters:\n",
        "    - X: torch.Tensor, shape (n_samples, n_features)\n",
        "    - Y: torch.Tensor, shape (m_samples, n_features)\n",
        "    - kernel_func: function, kernel function to compute pairwise kernel values\n",
        "\n",
        "    Returns:\n",
        "    - mmd: float, Maximum Mean Discrepancy\n",
        "    \"\"\"\n",
        "    m, n = len(X), len(Y)\n",
        "\n",
        "    # Compute kernel matrices\n",
        "    K_xx = kernel_func(X, X)\n",
        "    K_yy = kernel_func(Y, Y)\n",
        "    K_xy = kernel_func(X, Y)\n",
        "\n",
        "    # Compute MMD statistic\n",
        "    mmd = 1.0 / (m * (m - 1)) * torch.sum(K_xx - torch.diag(torch.diagonal(K_xx))) + \\\n",
        "          1.0 / (n * (n - 1)) * torch.sum(K_yy - torch.diag(torch.diagonal(K_yy))) - \\\n",
        "          2.0 / (m * n) * torch.sum(K_xy)\n",
        "\n",
        "    return mmd.item()  # Convert the result to a Python float\n",
        "\n",
        "# Example of using a Gaussian (RBF) kernel function for PyTorch\n",
        "def gaussian_kernel(X, Y, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Gaussian (RBF) kernel function.\n",
        "\n",
        "    Parameters:\n",
        "    - X: torch.Tensor, shape (n_samples, n_features)\n",
        "    - Y: torch.Tensor, shape (m_samples, n_features)\n",
        "    - sigma: float, bandwidth parameter of the kernel\n",
        "\n",
        "    Returns:\n",
        "    - K: torch.Tensor, shape (n_samples, m_samples), kernel matrix\n",
        "    \"\"\"\n",
        "    pairwise_sq_dists = torch.sum(X**2, dim=1, keepdim=True) + torch.sum(Y**2, dim=1, keepdim=True).t() - 2 * torch.mm(X, Y.t())\n",
        "    K = torch.exp(-pairwise_sq_dists / (2.0 * sigma**2))\n",
        "    return K\n",
        "\n",
        "\n"
      ],
      "id": "9Ap2d94AGf1R"
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_nodes = max(data.x.size(0) if data.x is not None else data.edge_index.max().item() + 1 for data in dataset)\n",
        "print(max_num_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwGmgiRKIX7z",
        "outputId": "54e66343-ce22-4acc-b3c4-8e2a60b26f2b"
      },
      "id": "GwGmgiRKIX7z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_graphs_by_class(model, dataset, N):\n",
        "    \"\"\"\n",
        "    Separates graphs in the dataset into two classes based on the model's predictions.\n",
        "    Also computes latent embeddings, degree distribution, clustering coefficients,\n",
        "    and spectrum distributions for each class.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained GNN model for predictions.\n",
        "        dataset (list): A list of graph data objects to classify.\n",
        "        N (int): Number of nodes (used for computing degree distribution and spectrum).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing data, embeddings, and metrics for each class.\n",
        "    \"\"\"\n",
        "    # Initialize containers for class-separated data\n",
        "    results = {\n",
        "        0: {\n",
        "            \"data\": [],\n",
        "            \"latent_data\": [],\n",
        "            \"degreedist\": [],\n",
        "            \"cluscoeff\": [],\n",
        "            \"spec\": [],\n",
        "        },\n",
        "        1: {\n",
        "            \"data\": [],\n",
        "            \"latent_data\": [],\n",
        "            \"degreedist\": [],\n",
        "            \"cluscoeff\": [],\n",
        "            \"spec\": [],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    for data in dataset:\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        embedding, out = model(data.x, data.edge_index, data.batch)\n",
        "        pred = out.argmax(dim=1).item()  # Classify the graph\n",
        "\n",
        "        # Append data and metrics to the appropriate class\n",
        "        results[pred][\"data\"].append(data)\n",
        "        results[pred][\"latent_data\"].append(embedding)\n",
        "        results[pred][\"degreedist\"].append(degree_distribution(data, N))\n",
        "        results[pred][\"cluscoeff\"].append(clustering_coefficient(data))\n",
        "        results[pred][\"spec\"].append(spectrum_distribution(data, N))\n",
        "\n",
        "    print(f\"Class 0: {len(results[0]['data'])} graphs\")\n",
        "    print(f\"Class 1: {len(results[1]['data'])} graphs\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "DZY6UttMIC8g"
      },
      "id": "DZY6UttMIC8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming model, dataset, and N are already defined\n",
        "N=25\n",
        "results = separate_graphs_by_class(model, dataset, N)\n",
        "\n",
        "# Access class-specific data\n",
        "data1 = results[0][\"data\"]\n",
        "latent_data1 = results[0][\"latent_data\"]\n",
        "degreedist1 = results[0][\"degreedist\"]\n",
        "cluscoeff1 = results[0][\"cluscoeff\"]\n",
        "spec1 = results[0][\"spec\"]\n",
        "\n",
        "data2 = results[1][\"data\"]\n",
        "latent_data2 = results[1][\"latent_data\"]\n",
        "degreedist2 = results[1][\"degreedist\"]\n",
        "cluscoeff2 = results[1][\"cluscoeff\"]\n",
        "spec2 = results[1][\"spec\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwJ252swIFJi",
        "outputId": "d048ce4b-bab3-4756-ea96-29bf2edcbaae"
      },
      "id": "NwJ252swIFJi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 512 graphs\n",
            "Class 1: 488 graphs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Representation Model"
      ],
      "metadata": {
        "id": "WInlV_Wy3F0N"
      },
      "id": "WInlV_Wy3F0N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91d7fe7a",
      "metadata": {
        "id": "91d7fe7a"
      },
      "outputs": [],
      "source": [
        "def GraphRepModel(N,classdata):\n",
        "    B=np.zeros(N)# Node Type Matrix for nodes of 10 types\n",
        "    X=np.zeros((N,10))\n",
        "    numgraphs=len(classdata)\n",
        "    workingdata=classdata\n",
        "\n",
        "    #Learning the on/off bit and node representations\n",
        "    for i in range(numgraphs):#len(data1))\n",
        "        data=workingdata[i]\n",
        "        x=data.x\n",
        "        k=len(data.x) # keeping tab of the number of nodes in the ith graph\n",
        "        #print(k)\n",
        "        x=x.numpy()\n",
        "\n",
        "        B[0:k]+=1\n",
        "        #print(B)\n",
        "        X[:x.shape[0], :] += x\n",
        "    B1=B\n",
        "    B=np.reshape(B,(N,-1))\n",
        "    mean_estimate=X/B\n",
        "\n",
        "    #print(mean_estimate)\n",
        "    covarr=np.zeros((10,10,N))\n",
        "\n",
        "    for i in range(numgraphs):\n",
        "        data=workingdata[i]\n",
        "        x=data.x\n",
        "        #print(len(x))\n",
        "        x=x.numpy()\n",
        "        subtracted_array = x-mean_estimate[:x.shape[0], :]\n",
        "        result_matrices=[]\n",
        "        for row in subtracted_array:\n",
        "          element = row.reshape(10, 1)  # Reshape to a 2x1 element\n",
        "          result_matrix = np.dot(element, element.T)  # Multiply by its transpose\n",
        "          result_matrices.append(result_matrix)\n",
        "\n",
        "    # Concatenate the resulting 2x2 matrices along the third dimension to create a 3D array\n",
        "        result_array = np.stack(result_matrices, axis=2)\n",
        "        #print(result_array.shape)\n",
        "        covarr[:, :, :result_array.shape[2]] += result_array\n",
        "    print(covarr.shape)\n",
        "    covariance_estimate=covarr/B[:,None,None]\n",
        "    #print(covariance_estimate.shape)\n",
        "    print(covariance_estimate.shape)\n",
        "    result_list=[]\n",
        "    for i in range(N):\n",
        "        result_list.append(covarr[:, :, i] / B[i])\n",
        "\n",
        "    # Convert the list of results back to a NumPy array\n",
        "    result = np.stack(result_list, axis=2)\n",
        "    print(result.shape)\n",
        "    Bdist=B/numgraphs\n",
        "    Adj=np.zeros((N,N))# Edge type count for only two types edge present/edge absent\n",
        "    for i in range(len(workingdata)):\n",
        "        data=workingdata[i]\n",
        "        adj=data.edge_index\n",
        "        rowlen=len(adj[0][:])\n",
        "        #print(rowlen)\n",
        "        #print(adj[:][0])\n",
        "        #print(adj[:][1])\n",
        "        for j in range(rowlen):\n",
        "            k1=adj[0][j]\n",
        "            k2=adj[1][j]\n",
        "            Adj[k1][k2]+=1\n",
        "\n",
        "    #Learning the parameters for the distribution of nodes\n",
        "    #numgraphs=len(data1)\n",
        "    #X=X/numgraphs #converting X to the node distribution matrix\n",
        "    Adj=Adj/numgraphs\n",
        "    return Bdist, mean_estimate, result, Adj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling Graphs from the Generative Model"
      ],
      "metadata": {
        "id": "A-meRMwM3JZn"
      },
      "id": "A-meRMwM3JZn"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "def graphsampler(N,Bdist,mean_estimate, result, Adj,show=False):\n",
        "  Bdist=np.concatenate(Bdist)\n",
        "  #print(Bdist)\n",
        "  samples = [np.random.choice([0, 1], p=[1 - p, p]) for p in Bdist]\n",
        "  #print(samples[1])\n",
        "  nodefeat=[]\n",
        "  for i in range(N):\n",
        "    if(samples[i]!=0):\n",
        "      meanvec=mean_estimate[i,:]\n",
        "      covarvec=result[:,:,i]\n",
        "      normalsamples = np.random.multivariate_normal(meanvec, covarvec, 1)\n",
        "      nodefeat.append(normalsamples)\n",
        "  Adjacencymat=np.random.binomial(1,Adj)\n",
        "  #print(Adjacencymat.shape)\n",
        "  #print(Adjacencymat[3][5])\n",
        "  nodefeat=np.vstack(nodefeat)\n",
        "\n",
        "  nodefeat=torch.tensor(nodefeat)\n",
        "  #nodefeat=nodefeat.double()\n",
        "  #print(nodefeat)\n",
        "  merged_array = np.vstack(nodefeat)\n",
        "  #print(nodefeat.shape)\n",
        "  k=nodefeat.shape[0]\n",
        "  for i in range(N):\n",
        "    for j in range(N):\n",
        "      if (i>k-1 or j> k-1):\n",
        "        Adjacencymat[i][j]=0\n",
        "    # Convert the NumPy array to a PyTorch tensor\n",
        "  adjacency_matrix = torch.tensor(Adjacencymat)\n",
        "\n",
        "  # Find the non-zero indices in the adjacency matrix\n",
        "  edge_indices = torch.nonzero(adjacency_matrix, as_tuple=False).t()\n",
        "  edge_indices=edge_indices.long()\n",
        "  newdata=Data(x=nodefeat,edge_index=edge_indices)\n",
        "  newdata.x=newdata.x.float()\n",
        "  newdata.pos=newdata.x\n",
        "  #print(newdata.x.dtype)\n",
        "  #print(newdata.x)\n",
        "  #print(newdata.edge_index)\n",
        "  embedding, out=model(newdata.x,newdata.edge_index,newdata.batch)\n",
        "  soft=torch.nn.Softmax(dim=1)\n",
        "  explainergraph=to_networkx(newdata,to_undirected=True)\n",
        "  degreedist=degree_distribution(newdata,N)\n",
        "  cluscoeffexp=clustering_coefficient(newdata)\n",
        "  specdist=spectrum_distribution(newdata,N)\n",
        "  if show:\n",
        "    nx.draw_networkx(explainergraph, node_size=150, node_color='red',with_labels=False)\n",
        "\n",
        "  problities=soft(out)\n",
        "  explainernodes=nx.number_of_nodes(explainergraph)\n",
        "  explaineredges=nx.number_of_edges(explainergraph)\n",
        "  explainerdensity=(explaineredges)/(explainernodes*explainernodes)\n",
        "  return problities, explainerdensity, embedding, degreedist, cluscoeffexp, specdist,explainergraph\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C5RCPMVxRao7"
      },
      "id": "C5RCPMVxRao7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Metrics on the Target Class"
      ],
      "metadata": {
        "id": "XwOdrn773QdL"
      },
      "id": "XwOdrn773QdL"
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import torch\n",
        "\n",
        "def target_class_analysis( N,Bdist,mean_estimate,result,Adj,latent,numexplanations=50, numsample=10, label=0,show=False):\n",
        "    \"\"\"\n",
        "    Analyzes the robustness of a target class in graph data by sampling graphs\n",
        "    and evaluating accuracy, density, and other metrics.\n",
        "\n",
        "    Args:\n",
        "        X: Node feature matrix.\n",
        "        Adj: Adjacency matrix.\n",
        "        N: Number of nodes in the graph.\n",
        "        latent: Latent space embeddings of the original data.\n",
        "        numexplanations: Number of explanations to generate.\n",
        "        numsample: Number of samples to generate.\n",
        "        label: Target class label to analyze (default: 0).\n",
        "\n",
        "    Returns:\n",
        "        meanaccuracy: Mean accuracy.\n",
        "        stdaccuracy: Standard deviation of accuracies.\n",
        "        meandensity: Mean density.\n",
        "        stddensity: Standard deviation of densities.\n",
        "        MMD: Maximum Mean Discrepancy (MMD) distance.\n",
        "        degreedistlist: Degree distributions across samples.\n",
        "        cluslist: Clustering coefficients across samples.\n",
        "        speclist: Spectral metrics across samples.\n",
        "    \"\"\"\n",
        "    accuracy = []\n",
        "    density = []\n",
        "    embeddings = []\n",
        "    degreedistlist = []\n",
        "    cluslist = []\n",
        "    speclist = []\n",
        "\n",
        "\n",
        "    for _ in range(numexplanations):\n",
        "\n",
        "        max_prob = 0\n",
        "\n",
        "        for _ in range(numsample):\n",
        "            probabilities, explainerdensity, embedding, degreedistb, clus, spec,explainergraph =graphsampler(N, Bdist, mean_estimate, result, Adj)\n",
        "\n",
        "            if probabilities[0][label] > max_prob:\n",
        "                sampleaccuracy = probabilities[0][label]\n",
        "                sampledensity = explainerdensity\n",
        "                sampleembedding = embedding\n",
        "                sampledeg = degreedistb\n",
        "                sampleclus = clus\n",
        "                samplespec = spec\n",
        "                bestgraph=explainergraph\n",
        "                max_prob = probabilities[0][label]\n",
        "        if show:\n",
        "          nx.draw_networkx(bestgraph, node_size=150, node_color='red',with_labels=False)\n",
        "\n",
        "        accuracy.append(sampleaccuracy)\n",
        "        density.append(sampledensity)\n",
        "        embeddings.append(sampleembedding)\n",
        "        degreedistlist.append(sampledeg)\n",
        "        cluslist.append(sampleclus)\n",
        "        speclist.append(samplespec)\n",
        "\n",
        "    meanaccuracy = torch.mean(torch.tensor(accuracy))\n",
        "    stdaccuracy = torch.std(torch.tensor(accuracy))\n",
        "    meandensity = statistics.mean(density)\n",
        "    stddensity = statistics.stdev(density)\n",
        "\n",
        "    MMDemb = mmd_kernel(torch.cat(latent, dim=0), torch.cat(embeddings, dim=0), gaussian_kernel)\n",
        "\n",
        "    return meanaccuracy, stdaccuracy, meandensity, stddensity, MMDemb, degreedistlist, cluslist, speclist\n"
      ],
      "metadata": {
        "id": "qRllXeKVB_Ql"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qRllXeKVB_Ql"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for Aligning the Data"
      ],
      "metadata": {
        "id": "OTrwped13U8a"
      },
      "id": "OTrwped13U8a"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import degree\n",
        "import torch\n",
        "\n",
        "def align_data_degreewise(data_list):\n",
        "    \"\"\"\n",
        "    Aligns the node features (`data.x`) and edges (`data.edge_index`) of a list\n",
        "    of PyTorch Geometric `Data` objects degreewise in descending order.\n",
        "\n",
        "    Args:\n",
        "        data_list (list): A list of PyTorch Geometric `Data` objects.\n",
        "\n",
        "    Returns:\n",
        "        list: A new list of `Data` objects with aligned node features and edges.\n",
        "    \"\"\"\n",
        "    aligned_data_list = []\n",
        "\n",
        "    for data in data_list:\n",
        "        # Compute the degree of each node\n",
        "        node_degrees = degree(data.edge_index[0], num_nodes=data.x.size(0))\n",
        "\n",
        "        # Get the descending order of nodes by degree\n",
        "        sorted_indices = node_degrees.argsort(descending=True)\n",
        "\n",
        "        # Reorder the node features\n",
        "        aligned_x = data.x[sorted_indices]\n",
        "\n",
        "        # Create a mapping from old node indices to new indices\n",
        "        index_mapping = torch.zeros_like(sorted_indices)\n",
        "        index_mapping[sorted_indices] = torch.arange(len(sorted_indices))\n",
        "\n",
        "        # Update edge_index using the index_mapping\n",
        "        aligned_edge_index = index_mapping[data.edge_index]\n",
        "\n",
        "        # Create a new Data object with aligned features and edges\n",
        "        aligned_data = data.clone()\n",
        "        aligned_data.x = aligned_x\n",
        "        aligned_data.edge_index = aligned_edge_index\n",
        "\n",
        "        aligned_data_list.append(aligned_data)\n",
        "\n",
        "    return aligned_data_list\n"
      ],
      "metadata": {
        "id": "ftMqa05rxQB6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ftMqa05rxQB6"
    },
    {
      "cell_type": "code",
      "source": [
        "data1=align_data_degreewise(data1)\n",
        "data2=align_data_degreewise(data2)\n",
        "\n",
        "\n",
        "N=25\n",
        "Bdist, mean_estimate, result, Adj =GraphRepModel(N,data1)\n",
        "latent_data=latent_data1\n",
        "classlabel=0\n",
        "meanaccuracy, stdaccuracy, meandensity, stddensity, MMDemb, degreedistlist, cluslist, speclist=target_class_analysis(N, Bdist, mean_estimate, result, Adj,latent_data,numexplanations=2, numsample=70, label=classlabel,show=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "4N0tAXP2RoJT",
        "outputId": "0a4fead2-c930-42b6-8d1c-bfbe488fe869"
      },
      "id": "4N0tAXP2RoJT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(meanaccuracy, stdaccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygNf7-cGNfIR",
        "outputId": "7d38b5b7-ea45-4afd-bd7e-df6b919a6d5c"
      },
      "id": "ygNf7-cGNfIR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.) tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Dpc(dataset,threshold):\n",
        "  boundary1=[]\n",
        "  boundary2=[]\n",
        "  m=torch.nn.Softmax(dim=1)\n",
        "  for i in range(len(dataset)):\n",
        "      model.eval()\n",
        "      data=dataset[i]\n",
        "      embedding , out = model(data.x, data.edge_index, data.batch)\n",
        "      probs=m(out)\n",
        "      #out=torch.nn.Softmax(out,dim=0)\n",
        "      #pred = out.argmax(dim=1)\n",
        "      if(threshold<probs[0][0]):\n",
        "          boundary1.append(data)\n",
        "          #latent_data1.append(embedding)\n",
        "      if(threshold <probs[0][1]):\n",
        "          boundary2.append(data)\n",
        "  return boundary1 , boundary2\n"
      ],
      "metadata": {
        "id": "YOfX0kbgYtYi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YOfX0kbgYtYi"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sofX5x5f9tOV"
      },
      "id": "sofX5x5f9tOV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_mean_with_error(mean, std, threshold, label, numsample,title=None, ax=None):\n",
        "    \"\"\"\n",
        "    Plot mean with error bars.\n",
        "\n",
        "    Parameters:\n",
        "        mean (array_like): Array containing mean values.\n",
        "        std (array_like): Array containing standard deviation values.\n",
        "        threshold (array_like): Array containing threshold values.\n",
        "        label (str): Label for the data.\n",
        "        color (str): Color of the line.\n",
        "        numsample (int): Sample number.\n",
        "        ax (matplotlib.axes.Axes, optional): Axes object to plot on. If not provided, a new figure will be created.\n",
        "    \"\"\"\n",
        "    # Flatten the arrays\n",
        "    mean=torch.tensor(mean,dtype=torch.float32)\n",
        "    std=torch.tensor(std,dtype=torch.float32)\n",
        "    mean = np.array(mean).flatten()\n",
        "    std = np.array(std).flatten()\n",
        "    threshold = np.array(threshold).flatten()\n",
        "    # Select color automatically\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
        "    color = colors[numsample % 10]  # Cycle through colors\n",
        "\n",
        "    # Plotting\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    ax.errorbar(threshold, mean, yerr=std, fmt='-', color=color, label=f'NumSample={numsample}')  # '-' for line\n",
        "\n",
        "    # Adding labels and title\n",
        "    ax.set_xlabel('Threshold')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title(title)\n",
        "\n",
        "    ax.legend(loc='lower right',fontsize='small')  # Show legend\n",
        "    ax.grid(True)  # Add grid\n",
        "# Create a figure outside the function\n",
        "# fig, ax = plt.subplots()\n",
        "# plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=1,ax=ax)\n",
        "# plot_mean_with_error(Mean2,Std2,Threshold,label='class1',numsample=2,ax=ax)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "Gu-YYMNDIxVj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Gu-YYMNDIxVj"
    },
    {
      "cell_type": "code",
      "source": [
        "samplerange=6\n",
        "fig1, ax1 = plt.subplots()\n",
        "fig2, ax2 = plt.subplots()\n",
        "numsample1=20\n",
        "numsample2=70\n",
        "graphs=list(dataset)\n",
        "for numsample in range(1,samplerange):\n",
        "  threshold=0.5\n",
        "  Threshold=[]\n",
        "  Mean1=[]\n",
        "  Std1=[]\n",
        "  Mean2=[]\n",
        "  Std2=[]\n",
        "\n",
        "  while(threshold<1):\n",
        "    Threshold.append(threshold)\n",
        "    data1 , data2=Dpc(graphs,threshold)\n",
        "    print(len(data1))\n",
        "    print(len(data2))\n",
        "\n",
        "    Bdist1, mean_estimate1, result1, Adj1 =GraphRepModel(N,data1)\n",
        "    Bdist2, mean_estimate2, result2, Adj2 =GraphRepModel(N,data2)\n",
        "    mean1,std1,*_=target_class_analysis(N, Bdist1, mean_estimate1, result1, Adj1,latent_data1,numexplanations=10, numsample=numsample1, label=0)\n",
        "    Mean1.append(mean1)\n",
        "    Std1.append(std1)\n",
        "    mean2,std2,*_=target_class_analysis(N, Bdist2, mean_estimate2, result2, Adj2,latent_data2,numexplanations=10, numsample=numsample2, label=1)\n",
        "    Mean2.append(mean2)\n",
        "    Std2.append(std2)\n",
        "    threshold+=0.05\n",
        "  numsample1+=1\n",
        "  numsample2+=1\n",
        "  plot_mean_with_error(Mean1,Std1,Threshold,label='class1',numsample=numsample1,title='Sensitivity analysis on House Class',ax=ax1)\n",
        "  plot_mean_with_error(Mean2,Std2,Threshold,label='class2',numsample=numsample2,title='Sensitivity analysis on Cycle Class',ax=ax2)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rpyb55jstCru",
        "outputId": "32ba9558-34a9-4b07-804c-d9a3bda6d0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "488\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "510\n",
            "487\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "507\n",
            "485\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "503\n",
            "481\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "502\n",
            "477\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "500\n",
            "475\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "495\n",
            "473\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "489\n",
            "467\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "481\n",
            "457\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "462\n",
            "444\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "512\n",
            "488\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "510\n",
            "487\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "507\n",
            "485\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "503\n",
            "481\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "502\n",
            "477\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "500\n",
            "475\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "495\n",
            "473\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "489\n",
            "467\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "481\n",
            "457\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "462\n",
            "444\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "512\n",
            "488\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "510\n",
            "487\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "507\n",
            "485\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "503\n",
            "481\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "502\n",
            "477\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "500\n",
            "475\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "495\n",
            "473\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "489\n",
            "467\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "481\n",
            "457\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "462\n",
            "444\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "512\n",
            "488\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "510\n",
            "487\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "507\n",
            "485\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "503\n",
            "481\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "502\n",
            "477\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "500\n",
            "475\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "495\n",
            "473\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "489\n",
            "467\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "481\n",
            "457\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "462\n",
            "444\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "512\n",
            "488\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "510\n",
            "487\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "507\n",
            "485\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "503\n",
            "481\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "502\n",
            "477\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "500\n",
            "475\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "495\n",
            "473\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "489\n",
            "467\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "481\n",
            "457\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "462\n",
            "444\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "id": "Rpyb55jstCru"
    },
    {
      "cell_type": "code",
      "source": [
        "Bdist1, mean_estimate1, result1, Adj1 =GraphRepModel(N,data1)\n",
        "Bdist2, mean_estimate2, result2, Adj2 =GraphRepModel(N,data2)\n",
        "mean_estimates=[mean_estimate1,mean_estimate2]\n",
        "covar_estimates=[result1,result2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3myf5k9ioO3e",
        "outputId": "ed8d38ab-1801-4ee8-f71c-7c5deef27c23"
      },
      "id": "3myf5k9ioO3e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(10, 10, 25)\n",
            "(25, 10, 10, 25)\n",
            "(10, 10, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mixcoeff=0.5\n",
        "Bdistmix=mixcoeff*Bdist1+(1-mixcoeff)*Bdist2\n",
        "Adjmix=mixcoeff*Adj1+(1-mixcoeff)*Adj2\n"
      ],
      "metadata": {
        "id": "zuoCIWGDnbym"
      },
      "id": "zuoCIWGDnbym",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def boundary_analysis(N, Bdistmix, mean_estimates,covar_estimates, Adj,numboundary, numsample, label1, label2):\n",
        "    accuracies = []\n",
        "    embeddings = []\n",
        "\n",
        "    for _ in range(numboundary):\n",
        "        min_difference = float('inf')\n",
        "        sample_accuracy = None\n",
        "        sample_embedding = None\n",
        "        mean_estimate=random.choice(mean_estimates)\n",
        "        result=random.choice(covar_estimates)\n",
        "\n",
        "        for _ in range(numsample):\n",
        "            probabilities, _, embedding,*_ = graphsampler(N, Bdist, mean_estimate, result, Adj)\n",
        "\n",
        "            # Difference in class scores\n",
        "            score_diff = abs(probabilities[0][label1] - probabilities[0][label2])\n",
        "\n",
        "            if score_diff < min_difference:\n",
        "                min_difference = score_diff\n",
        "                sample_accuracy = probabilities[0][label1]\n",
        "                sample_embedding = embedding\n",
        "\n",
        "        accuracies.append(sample_accuracy)\n",
        "        embeddings.append(sample_embedding)\n",
        "\n",
        "    # Calculate mean and standard deviation of accuracies\n",
        "    mean_accuracy = torch.mean(torch.tensor(accuracies))\n",
        "    std_accuracy = torch.std(torch.tensor(accuracies))\n",
        "\n",
        "    return mean_accuracy, std_accuracy, embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "3N3b8878bPUj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3N3b8878bPUj"
    },
    {
      "cell_type": "code",
      "source": [
        "bdmean,bdstd,embeddings=boundary_analysis(N, Bdistmix, mean_estimates,covar_estimates, Adjmix,10, 2000, 0, 1)\n",
        "print(bdmean,bdstd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJx0yFQAe1Cy",
        "outputId": "f5817a7e-abc6-4403-e0a8-dcf7802b29bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4787) tensor(0.0650)\n"
          ]
        }
      ],
      "id": "bJx0yFQAe1Cy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing boundary metrics\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def boundary_margin(embeddings_c1, embeddings_c2):\n",
        "    \"\"\"\n",
        "    Compute the boundary margin.\n",
        "\n",
        "    Args:\n",
        "    - embeddings_c1 (torch.Tensor): Embeddings of class c1 graphs.\n",
        "    - embeddings_c2 (torch.Tensor): Embeddings of boundary graphs between class c1 and c2.\n",
        "\n",
        "    Returns:\n",
        "    - margin (float): The boundary margin.\n",
        "\n",
        "    \"\"\"\n",
        "    embeddings_c1=torch.cat(embeddings_c1,dim=0)\n",
        "    embeddings_c2=torch.cat(embeddings_c2,dim=0)\n",
        "    distances = torch.norm(embeddings_c1 - embeddings_c2, dim=1)\n",
        "    margin = torch.min(distances).item()\n",
        "    return margin\n",
        "\n",
        "def boundary_thickness(embeddings_c1, embeddings_c1_c2, model, c1, c2, gamma=0.75, num_points=100):\n",
        "    thickness_values = []\n",
        "\n",
        "    for emb_c1, emb_c1_c2 in zip(embeddings_c1, embeddings_c1_c2):\n",
        "        t_values = torch.linspace(0, 1, num_points)\n",
        "        h_t = (1 - t_values).unsqueeze(1) * emb_c1 + t_values.unsqueeze(1) * emb_c1_c2\n",
        "        #print(model(h_t).size())\n",
        "\n",
        "        # Compute the logits\n",
        "        logits_h_t = model(h_t)  # Assuming `model` is your classifier\n",
        "        probs_h_t = F.softmax(logits_h_t, dim=1)\n",
        "\n",
        "        # Compute the integrand\n",
        "        integrand = (gamma > (probs_h_t[:, c1] - probs_h_t[:, c2])).float()\n",
        "\n",
        "        # Approximate the integral using the trapezoidal rule\n",
        "        integral = torch.trapz(integrand, t_values)\n",
        "\n",
        "        # Compute the thickness value\n",
        "        thickness_value = (emb_c1 - emb_c1_c2).norm() * integral.mean()\n",
        "        thickness_values.append(thickness_value.item())\n",
        "\n",
        "    return sum(thickness_values) / len(thickness_values)\n",
        "\n",
        "# def boundary_complexity(embeddings, D):\n",
        "#     \"\"\"\n",
        "#     Compute the boundary complexity.\n",
        "\n",
        "#     Args:\n",
        "#     - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
        "#     - D (int): Dimensionality of the embeddings.\n",
        "\n",
        "#     Returns:\n",
        "#     - complexity (float): The boundary complexity.\n",
        "#     \"\"\"\n",
        "#     # Compute the covariance matrix of the embeddings\n",
        "#     embeddings=torch.cat(embeddings,dim=0)\n",
        "#     covariance_matrix = torch.cov(embeddings.T)\n",
        "\n",
        "#     # Compute the eigenvalues of the covariance matrix\n",
        "#     eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
        "#     print(eigenvalues)\n",
        "\n",
        "#     # Normalize the eigenvalues\n",
        "#     eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
        "#     print(eigenvalues_normalized)\n",
        "\n",
        "#     # Compute the entropy of the normalized eigenvalues\n",
        "#     entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + 1e-7))\n",
        "#     print(entropy)\n",
        "\n",
        "#     # Normalize the entropy by dividing it by log(D)\n",
        "#     complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
        "\n",
        "#     return complexity.item()\n",
        "def boundary_complexity(embeddings, D, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Compute the boundary complexity.\n",
        "\n",
        "    Args:\n",
        "    - embeddings (torch.Tensor): Embeddings of the boundary graphs with shape (num_graphs, embedding_dim).\n",
        "    - D (int): Dimensionality of the embeddings.\n",
        "    - epsilon (float): Small value added to eigenvalues to prevent log(0).\n",
        "\n",
        "    Returns:\n",
        "    - complexity (float): The boundary complexity.\n",
        "    \"\"\"\n",
        "    # Flatten and concatenate embeddings\n",
        "    embeddings = torch.cat(embeddings, dim=0)\n",
        "\n",
        "    # Compute the covariance matrix of the embeddings\n",
        "    covariance_matrix = torch.cov(embeddings.T)\n",
        "\n",
        "    # Add a small value to the diagonal for regularization\n",
        "    covariance_matrix += epsilon * torch.eye(covariance_matrix.size(0))\n",
        "\n",
        "    # Compute the eigenvalues of the covariance matrix\n",
        "    eigenvalues = torch.linalg.eigvalsh(covariance_matrix)\n",
        "\n",
        "    # Clamp eigenvalues to avoid very small negative values due to numerical errors\n",
        "    eigenvalues = torch.clamp(eigenvalues, min=epsilon)\n",
        "\n",
        "    # Normalize the eigenvalues\n",
        "    eigenvalues_normalized = eigenvalues / eigenvalues.sum()\n",
        "\n",
        "    # Compute the entropy of the normalized eigenvalues\n",
        "    entropy = -torch.sum(eigenvalues_normalized * torch.log(eigenvalues_normalized + epsilon))\n",
        "\n",
        "    # Normalize the entropy by dividing it by log(D)\n",
        "    complexity = entropy / torch.log(torch.tensor(D, dtype=torch.float32))\n",
        "\n",
        "    return complexity.item()"
      ],
      "metadata": {
        "id": "tHGTBOVJnC-n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tHGTBOVJnC-n"
    },
    {
      "cell_type": "code",
      "source": [
        "boundaryembeddings=embeddings\n",
        "latent_data=latent_data2\n",
        "margin=boundary_margin(boundaryembeddings[:len(latent_data)],latent_data)\n",
        "print(margin)\n",
        "thickness=boundary_thickness(boundaryembeddings[:len(latent_data)],latent_data,model.classifier,1,0)\n",
        "print(thickness)\n",
        "complexity=boundary_complexity(boundaryembeddings[:len(latent_data)],64)\n",
        "print(complexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "RMPPyubSnGC7",
        "outputId": "7c031f3b-1aa2-4345-c0a8-fb3a949eb178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (10) must match the size of tensor b (488) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-d28b50d41643>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mboundaryembeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlatent_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_data2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboundary_margin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboundaryembeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboundary_thickness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboundaryembeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-c87ba5b848f5>\u001b[0m in \u001b[0;36mboundary_margin\u001b[0;34m(embeddings_c1, embeddings_c2)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0membeddings_c1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0membeddings_c2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_c2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_c1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0membeddings_c2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (488) at non-singleton dimension 0"
          ]
        }
      ],
      "id": "RMPPyubSnGC7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the Confusion Matrix of the Classifier"
      ],
      "metadata": {
        "id": "y7odkU7CnAlk"
      },
      "id": "y7odkU7CnAlk"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(model, dataset, class_dict):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the provided dataset, compute the confusion matrix,\n",
        "    and plot it with class names.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained GNN model\n",
        "    - dataset: List of data objects\n",
        "    - class_dict: Dictionary mapping class labels to class names, e.g., {0: 'Class A', 1: 'Class B'}\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Evaluate the model and get predictions and true labels\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataset:\n",
        "            _, out = model(data.x, data.edge_index, data.batch)\n",
        "            pred = out.argmax(dim=1)\n",
        "            all_preds.append(pred.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Step 2: Compute the confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Step 3: Plot the confusion matrix\n",
        "    class_names = [class_dict[i] for i in range(len(class_dict))]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming the class labels are {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
        "#class_dict = {0: 'Mutagenic', 1: 'Non-Mutagenic'}\n",
        "\n",
        "# Example dataset (assuming it's a list of data objects)\n",
        "# dataset = [...]\n",
        "\n",
        "# Call the function with the model, dataset (as a list), and class dictionary\n",
        "#plot_confusion_matrix(model, dataset, class_dict)\n"
      ],
      "metadata": {
        "id": "Ub2ns9s8ScHP"
      },
      "id": "Ub2ns9s8ScHP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict={0:'Cycle',1:'House'}\n",
        "plot_confusion_matrix(model,dataset,class_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "t0LchX_6SfLR",
        "outputId": "72c3771a-595b-4fef-9c79-38ca32b242ec"
      },
      "id": "t0LchX_6SfLR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}